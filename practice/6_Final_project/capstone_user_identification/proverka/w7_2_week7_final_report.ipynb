{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача: научиться идентифицировать пользователей по статистике посещения сайтов. **\n",
    "    \n",
    "Есть данные по пользователям, разложены по папкам на разное количество пользователей. Так как модели тренировать - долгий процесс, вначале будем все рассматривать на небольшом количестве пользователеей (папки по 3, 10 и 150 пользователей). В самом конце пробуем обучить модель на 3000 пользователей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "#pip install tqdm\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import stats\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Предварительный анализ и преобразование данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вначале посмотрим, что за данные у нас в наличии, на примере одного пользователя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-11-15 08:12:07</td>\n",
       "      <td>fpdownload2.macromedia.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-11-15 08:12:17</td>\n",
       "      <td>laposte.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-11-15 08:12:17</td>\n",
       "      <td>www.laposte.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-11-15 08:12:17</td>\n",
       "      <td>www.google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-11-15 08:12:18</td>\n",
       "      <td>www.laposte.net</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                        site\n",
       "0  2013-11-15 08:12:07  fpdownload2.macromedia.com\n",
       "1  2013-11-15 08:12:17                 laposte.net\n",
       "2  2013-11-15 08:12:17             www.laposte.net\n",
       "3  2013-11-15 08:12:17              www.google.com\n",
       "4  2013-11-15 08:12:18             www.laposte.net"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "PATH_TO_DATA = 'datasets' \n",
    "\n",
    "user31_data = pd.read_csv(os.path.join(PATH_TO_DATA, \n",
    "                                       '10users/user0031.csv'))\n",
    "\n",
    "user31_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Видим, что по каждому пользователю у нас по файлу csv, в котором данные о просмотренных сайтах. Данные состоят из столбца с временем посещения сайта и адресом сайта.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7600 entries, 0 to 7599\n",
      "Data columns (total 2 columns):\n",
      "timestamp    7600 non-null object\n",
      "site         7600 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 118.8+ KB\n"
     ]
    }
   ],
   "source": [
    "user31_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По данному конкретному пользователю - 7600 посещенных им сайтов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                    7600\n",
       "unique                    962\n",
       "top       webmail.laposte.net\n",
       "freq                      399\n",
       "Name: site, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user31_data.site.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом уникальных сайтов меньше тысячи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2013-11-15 08:12:07', '2014-03-28 13:09:07')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user31_data.timestamp.min(), user31_data.timestamp.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные распространяются на несколько месяцев интернет-активности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем создать из этиъ файлов по пользователям один файл. В нем будет отражено, какие сайты подряд просматривал каждый из пользователей. Названия сайтов будут преобразованы в индексы, сами названия можно будет проверить по файлу-словарю (в котором названию сайта соответствует его индекс и сколько раз он встерчался в датасете). Данные о пользователях тогда будут выглядеть таким образом:\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-s6z2{text-align:center}\n",
    ".tg .tg-baqh{text-align:center;vertical-align:top}\n",
    ".tg .tg-hgcj{font-weight:bold;text-align:center}\n",
    ".tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-hgcj\">session_id</th>\n",
    "    <th class=\"tg-hgcj\">site1</th>\n",
    "    <th class=\"tg-hgcj\">site2</th>\n",
    "    <th class=\"tg-amwm\">user_id</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">1</td>\n",
    "    <td class=\"tg-s6z2\">1</td>\n",
    "    <td class=\"tg-s6z2\">2</td>\n",
    "    <td class=\"tg-baqh\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">2</td>\n",
    "    <td class=\"tg-s6z2\">1</td>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-baqh\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-s6z2\">2</td>\n",
    "    <td class=\"tg-baqh\">2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">4</td>\n",
    "    <td class=\"tg-s6z2\">4</td>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-baqh\">2</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "**Для начала зададим длину сессии 10 сайтов, то есть каждой строчке будут соответствовать 10 сайтов, просмотренных подряд каким-либо пользователем.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дополнительно убедимся, чтобы не множились одни и те же сайты, записанные по-разному (выше заметно, что сайт laposte был записан и как laposte.net, и как www.laposte.net). При этом специально оставляем значимые префиксы и доменные имена, потому что они могут быть полезны при идентификации пользователей.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_columns(session_length):\n",
    "    \n",
    "    columns = []\n",
    "    for index in range(session_length):\n",
    "        columns.append(''.join(['site', str(index+1)]))\n",
    "    columns.append('user_id')\n",
    "    return columns\n",
    "\n",
    "\n",
    "def prepare_train_set(path_to_csv_files, session_length=10):\n",
    "\n",
    "    files = glob(os.path.join(path_to_csv_files, \n",
    "                                       '*.csv'))\n",
    "    sites_freq = {}\n",
    "    current_id = 1\n",
    "    \n",
    "    columns = name_columns(session_length)\n",
    "    \n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    user_id = 1\n",
    "    \n",
    "    for file in tqdm(files):\n",
    "        user_df = pd.read_csv(file)\n",
    "        user_sites = user_df.site.values\n",
    "        user_sites_ids = []\n",
    "        \n",
    "        for site in user_sites:\n",
    "            # убираем лишние префиксы в названии сайта\n",
    "            correct_site = site.replace(\"www.\", \"\")\n",
    "            correct_site = correct_site.replace(\"http://\", \"\")\n",
    "            correct_site = correct_site.replace(\"https://\", \"\")\n",
    "\n",
    "            if correct_site in sites_freq:\n",
    "                site_id = sites_freq[correct_site][0]\n",
    "                count = sites_freq[correct_site][1] + 1\n",
    "                sites_freq[correct_site] = (site_id, count)\n",
    "                user_sites_ids.append(site_id)\n",
    "                \n",
    "            else:\n",
    "                sites_freq[correct_site] = (current_id, 1)\n",
    "                user_sites_ids.append(current_id)\n",
    "                current_id += 1\n",
    "              \n",
    "        if len(user_sites) % session_length != 0:\n",
    "            user_sites_ids = np.pad(user_sites_ids, (0, session_length - len(user_sites_ids) % session_length), 'constant')\n",
    "        user_sites_ids = np.reshape(user_sites_ids, [-1, session_length])\n",
    "        user_ids = np.array([user_id for i in range(user_sites_ids.shape[0])])\n",
    "        user_ids = np.reshape(user_ids, [user_sites_ids.shape[0], -1])\n",
    "        user_sites_ids = np.hstack([user_sites_ids, user_ids])\n",
    "        \n",
    "        df_new = pd.DataFrame(user_sites_ids, columns=columns)\n",
    "        df = df.append(df_new)\n",
    "\n",
    "        user_id += 1\n",
    "        \n",
    "    return df, sites_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определили нужные функции, теперь применим их к нашим датасетам, с 10 и с 150 пользователями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 746 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_10users, site_freq_10users = prepare_train_set(os.path.join(PATH_TO_DATA, '10users'), \n",
    "                                                     session_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 150/150 [00:07<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_150users, site_freq_150users = prepare_train_set(os.path.join(PATH_TO_DATA, '150users'), \n",
    "                                                     session_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проведем анализ полученного датасета со 150 пользователями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим количество уникальных сайтов, а также на самые популярные и самые редкие сайты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27287"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(site_freq_150users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sites = pd.DataFrame(site_freq_150users)\n",
    "sorted_sites = sites.sort_values(by=1, axis=1, ascending=False, kind='mergesort')\n",
    "top15_popular = list(sorted_sites.columns[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['google.fr',\n",
       " 'google.com',\n",
       " 'facebook.com',\n",
       " 'apis.google.com',\n",
       " 's.youtube.com',\n",
       " 'clients1.google.com',\n",
       " 'mail.google.com',\n",
       " 'plus.google.com',\n",
       " 'safebrowsing-cache.google.com',\n",
       " 'youtube.com',\n",
       " 'twitter.com',\n",
       " 'platform.twitter.com',\n",
       " 's-static.ak.facebook.com',\n",
       " 'accounts.google.com',\n",
       " 'bing.com']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top15_popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.static.wix.com',\n",
       " '00.art.skyrock.net',\n",
       " '02.img.v4.skyrock.net',\n",
       " '045-qrg-025.mktoresp.com',\n",
       " '05.wir.skyrock.net',\n",
       " '08.mgl.skyrock.net',\n",
       " '0f.img.v4.skyrock.net',\n",
       " '0img.imgo.tv',\n",
       " '0w2myakzob.s.ad6media.fr',\n",
       " '1.images.gametrailers.com',\n",
       " '1.viki.io',\n",
       " '1001pharmacies.com',\n",
       " '100p100arbitres.com',\n",
       " '101cookbooks.com',\n",
       " '10parjour.net']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_sites = sites.sort_values(by=1, axis=1, ascending=True, kind='mergesort')\n",
    "top15_unpopular = list(sorted_sites.columns[:15])\n",
    "top15_unpopular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Смотрим распределение целевого класса, на сколько уникальных сайтов заходят разные пользователи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103    4653\n",
       "21     3152\n",
       "104    2954\n",
       "29     2796\n",
       "102    2675\n",
       "93     2607\n",
       "8      2204\n",
       "1      2137\n",
       "4      2094\n",
       "97     2049\n",
       "60     2039\n",
       "59     1953\n",
       "37     1868\n",
       "23     1752\n",
       "146    1722\n",
       "28     1712\n",
       "5      1651\n",
       "88     1645\n",
       "42     1643\n",
       "130    1630\n",
       "62     1612\n",
       "100    1562\n",
       "138    1544\n",
       "41     1537\n",
       "56     1493\n",
       "80     1476\n",
       "125    1438\n",
       "64     1401\n",
       "51     1382\n",
       "24     1370\n",
       "       ... \n",
       "78      462\n",
       "69      461\n",
       "112     456\n",
       "40      445\n",
       "31      442\n",
       "128     442\n",
       "75      441\n",
       "119     441\n",
       "70      435\n",
       "137     434\n",
       "114     429\n",
       "127     423\n",
       "13      422\n",
       "126     415\n",
       "136     411\n",
       "87      409\n",
       "123     406\n",
       "108     405\n",
       "55      400\n",
       "33      400\n",
       "86      399\n",
       "67      396\n",
       "30      395\n",
       "48      394\n",
       "9       394\n",
       "84      390\n",
       "77      382\n",
       "110     377\n",
       "135     365\n",
       "145     351\n",
       "Name: user_id, Length: 150, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_150users['user_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохраним полученные датасеты и словари на диск в формате разреженных матриц**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_sparce(users_history):\n",
    "    indices = users_history.flatten()\n",
    "    indptr = [num for num in range(0, indices.shape[0]+1, users_history.shape[1])]\n",
    "    data = np.ones(indices.shape)\n",
    "    return csr_matrix((data, indices, indptr), (users_history.shape[0], len(np.unique(users_history))))[:, 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_10users, y_10users = train_data_10users.iloc[:, :-1].values, \\\n",
    "                       train_data_10users.iloc[:, -1].values\n",
    "X_150users, y_150users = train_data_150users.iloc[:, :-1].values, \\\n",
    "                         train_data_150users.iloc[:, -1].values\n",
    "    \n",
    "X_sparse_10users = convert_to_sparce(X_10users)\n",
    "X_sparse_150users = convert_to_sparce(X_150users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, \n",
    "                       'X_sparse_10users.pkl'), 'wb') as X10_pkl:\n",
    "    pickle.dump(X_sparse_10users, X10_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, \n",
    "                       'y_10users.pkl'), 'wb') as y10_pkl:\n",
    "    pickle.dump(y_10users, y10_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, \n",
    "                       'X_sparse_150users.pkl'), 'wb') as X150_pkl:\n",
    "    pickle.dump(X_sparse_150users, X150_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, \n",
    "                       'y_150users.pkl'), 'wb') as y150_pkl:\n",
    "    pickle.dump(y_150users, y150_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, \n",
    "                       'site_freq_10users.pkl'), 'wb') as site_freq_10users_pkl:\n",
    "    pickle.dump(site_freq_10users, site_freq_10users_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, \n",
    "                       'site_freq_150users.pkl'), 'wb') as site_freq_150users_pkl:\n",
    "    pickle.dump(site_freq_150users, site_freq_150users_pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Посчитаем распределение числа уникальных сайтов в каждой сессии из 10 посещенных подряд сайтов.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     2311\n",
       "6     2205\n",
       "8     2038\n",
       "5     1756\n",
       "9     1368\n",
       "2     1248\n",
       "4     1173\n",
       "3      897\n",
       "10     638\n",
       "1      427\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unique_sites = [np.unique(train_data_10users.values[i, :-1]).shape[0] \n",
    "                    for i in range(train_data_10users.shape[0])]\n",
    "pd.Series(num_unique_sites).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Проверим гипотезу о том, что пользователь хотя бы раз зайдет на сайт, который он уже ранее посетил в сессии из 10 сайтов. Проверим с помощью биномиального критерия для доли, что доля случаев, когда пользователь повторно посетил какой-то сайт (то есть число уникальных сайтов в сессии < 10) велика: больше 95%. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0057114607369772776"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_two_similar = (np.array(num_unique_sites) < 10).astype('int')\n",
    "pi_val = stats.binom_test(sum(has_two_similar), len(has_two_similar), p=0.95, alternative='greater') \n",
    "pi_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**И построим для этой доли 95% доверительный интервал Уилсона.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.95106032697778875, 0.95794387597978881)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "wilson_interval = proportion_confint(sum(has_two_similar), len(has_two_similar), alpha=0.05, method='wilson')\n",
    "wilson_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Замечательно, надежда на благоприятный исход и возможность угадывать пользователя по этим данным есть.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Подготовка данных для линейных моделей и тренировка простых линейных моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем данные в разреженные матрицы, чтобы было возможно попробовать на них простые модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_columns(session_length):\n",
    "    \n",
    "    columns = []\n",
    "    for index in range(session_length):\n",
    "        columns.append(''.join(['site', str(index+1)]))\n",
    "    columns.append('user_id')\n",
    "    return columns\n",
    "\n",
    "def prepare_train_set_window(path_to_csv_files, site_freq_path, \n",
    "                                    session_length=10, window_size=10):\n",
    "\n",
    "\n",
    "    sites_freq = pickle.load(open(site_freq_path, 'rb'))\n",
    "    files = glob(os.path.join(path_to_csv_files, '*.csv'))\n",
    "\n",
    "    user_id = 1\n",
    "    data_array = np.empty((0, session_length+1), int)\n",
    "    \n",
    "    for file in tqdm(files):\n",
    "        user_df = pd.read_csv(file)\n",
    "        user_sites = user_df.site.values\n",
    "        user_sites_ids = []\n",
    "        \n",
    "        for site in user_sites:\n",
    "            # убираем лишние префиксы в названии сайта\n",
    "            correct_site = site.replace(\"www.\", \"\")\n",
    "            correct_site = correct_site.replace(\"http://\", \"\")\n",
    "            correct_site = correct_site.replace(\"https://\", \"\")\n",
    "            \n",
    "            site_id = sites_freq[correct_site][0]\n",
    "            user_sites_ids.append(site_id)\n",
    "                        \n",
    "        if len(user_sites_ids) % window_size != 0:\n",
    "            user_sites_ids = np.pad(user_sites_ids, (0, session_length - len(user_sites_ids) % window_size), 'constant')\n",
    "        remainder = (len(user_sites_ids) - ((len(user_sites_ids) / window_size) - 1) * window_size) % session_length\n",
    "        if remainder != 0:\n",
    "            user_sites_ids = np.pad(user_sites_ids, (0, session_length - int(remainder)), 'constant')\n",
    "        \n",
    "        \n",
    "            \n",
    "        user_sites_ids = np.vstack([user_sites_ids[i:session_length+i] for i in range(0, len(user_sites), window_size)])\n",
    "            \n",
    "        user_ids = np.array([user_id for i in range(user_sites_ids.shape[0])])\n",
    "        user_ids = np.reshape(user_ids, [user_sites_ids.shape[0], -1])\n",
    "        user_sites_ids = np.hstack([user_sites_ids, user_ids])\n",
    "\n",
    "        data_array = np.vstack([data_array, user_sites_ids])\n",
    "\n",
    "        user_id += 1\n",
    "    \n",
    "    return data_array, sites_freq\n",
    "\n",
    "def prepare_sparse_train_set_window(path_to_csv_files, site_freq_path, \n",
    "                                    session_length=10, window_size=10):\n",
    "    \n",
    "    data_array, sites_freq = prepare_train_set_window(path_to_csv_files, site_freq_path, \n",
    "                                    session_length=session_length, window_size=window_size)\n",
    "    \n",
    "    X, y = data_array[:, :-1], data_array[:, -1]\n",
    "    indices = X.flatten()\n",
    "    indptr = [num for num in range(0, indices.shape[0]+1, X.shape[1])]\n",
    "    data = np.ones(indices.shape)\n",
    "    X = csr_matrix((data, indices, indptr), (X.shape[0], len(sites_freq) + 1))[:, 1:]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сразу же сохраним данные с разной длиной сессии и разной длиной окна**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 11.40it/s]\n",
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 25.05it/s]\n",
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 24.33it/s]\n",
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 22.30it/s]\n",
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 23.10it/s]\n",
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 22.99it/s]\n",
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 20.84it/s]\n",
      "100%|████████████████████████████████████████| 150/150 [00:04<00:00, 31.35it/s]\n",
      "100%|████████████████████████████████████████| 150/150 [00:04<00:00, 30.58it/s]\n",
      "100%|████████████████████████████████████████| 150/150 [00:04<00:00, 33.23it/s]\n",
      "100%|████████████████████████████████████████| 150/150 [00:05<00:00, 32.16it/s]\n",
      "100%|████████████████████████████████████████| 150/150 [00:05<00:00, 26.97it/s]\n",
      "100%|████████████████████████████████████████| 150/150 [00:04<00:00, 30.40it/s]\n",
      "100%|████████████████████████████████████████| 150/150 [00:05<00:00, 28.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_lengths = []\n",
    "\n",
    "for num_users in [10, 150]:\n",
    "    \n",
    "    for window_size, session_length in itertools.product([15, 10, 5], [15, 10, 7, 5]):\n",
    "        \n",
    "        if window_size <= session_length:\n",
    "            X_sparse, y = prepare_sparse_train_set_window(os.path.join(PATH_TO_DATA, str(num_users)+'users'), \n",
    "                                                       os.path.join(PATH_TO_DATA, 'site_freq_'+str(num_users)+'users.pkl'),\n",
    "                                       session_length=session_length, window_size=window_size)\n",
    "            \n",
    "            with open(os.path.join(PATH_TO_DATA, \n",
    "                                   'X_sparse_'+str(num_users)+'users_s'+str(session_length)+'_w'+str(window_size)+'.pkl'), \n",
    "                      'wb') as X_pkl:\n",
    "                pickle.dump(X_sparse, X_pkl, protocol=2)\n",
    "                \n",
    "            with open(os.path.join(PATH_TO_DATA, \n",
    "                       'y_'+str(num_users)+'users_s'+str(session_length)+'_w'+str(window_size)+'.pkl'), \n",
    "                      'wb') as y_pkl:\n",
    "                pickle.dump(y, y_pkl, protocol=2)\n",
    "            data_lengths.append(X_sparse.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Начнем подбирать модель и ее параметры, используя базовый датасет из 10 пользователей, сесссии в 10 сайтов и стандартной ширине окна**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# функция для построения графиков по результатам поиска параметров grid search\n",
    "def plot_validation_curves(param_values, grid_cv_results_):\n",
    "    train_mu, train_std = grid_cv_results_['mean_train_score'], grid_cv_results_['std_train_score']\n",
    "    valid_mu, valid_std = grid_cv_results_['mean_test_score'], grid_cv_results_['std_test_score']\n",
    "    train_line = plt.plot(param_values, train_mu, '-', label='train', color='green')\n",
    "    valid_line = plt.plot(param_values, valid_mu, '-', label='test', color='red')\n",
    "    plt.fill_between(param_values, train_mu - train_std, train_mu + train_std, edgecolor='none',\n",
    "                     facecolor=train_line[0].get_color(), alpha=0.2)\n",
    "    plt.fill_between(param_values, valid_mu - valid_std, valid_mu + valid_std, edgecolor='none',\n",
    "                     facecolor=valid_line[0].get_color(), alpha=0.2)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, \n",
    "         'X_sparse_10users.pkl'), 'rb') as X_sparse_10users_pkl:\n",
    "    X_sparse_10users = pickle.load(X_sparse_10users_pkl)\n",
    "with open(os.path.join(PATH_TO_DATA, \n",
    "                       'y_10users.pkl'), 'rb') as y_10users_pkl:\n",
    "    y_10users = pickle.load(y_10users_pkl)\n",
    "\n",
    "y_10users = y_10users.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_sparse_10users, y_10users, \n",
    "                                                      test_size=0.3, \n",
    "                                                     random_state=17, stratify=y_10users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Мы знаем, что в таких случаях лучше всего работают линейные модели, поэтому поработаем с ними**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76234560498500425"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "logit = LogisticRegression(random_state=17, n_jobs=-1)\n",
    "cross_val_score(logit, X_train, y_train, cv=skf).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кросс-валидация обычно бывает достаточно точным индикатором, но проверим и на отложенной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78075373311211183"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit(X_train, y_train)\n",
    "accuracy_score(y_valid, logit.predict(X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проведем поиск лучшего параметра C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit_c_values1 = np.linspace(3, 7, 10)\n",
    "\n",
    "logit_grid_searcher1 = LogisticRegressionCV(multi_class='multinomial', random_state=17, cv=skf, n_jobs=-1)\n",
    "logit_grid_searcher1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.31740828,  0.49523171,  0.61867757,  0.71601482,  0.75422041,\n",
       "        0.76295394,  0.75543426,  0.74273654,  0.7409103 ,  0.73918259])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_for_all_classes = np.sum(np.array([value for key, value in logit_grid_searcher1.scores_.items()]), axis=0) / 10\n",
    "logit_mean_cv_scores1 = np.sum(scores_for_all_classes, axis=0) / 3  \n",
    "logit_mean_cv_scores1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Покажем визуально, как меняется аккуратность при разных значениях C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEkL/HztzJmqMQZ1oAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6bd31dd940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(logit_c_values1, logit_mean_cv_scores1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажем лучшее значение C и соответствующую аккуратность на кросс-валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.2222222222222223, 0.76295393759368579)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_c = logit_c_values1[logit_mean_cv_scores1.argmax()]\n",
    "best_c, logit_mean_cv_scores1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77909457217350087"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_cv_acc = accuracy_score(y_valid, logit_grid_searcher1.predict(X_valid))\n",
    "logit_cv_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Теперь попробуем линейный SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC(C=1, random_state=17)\n",
    "\n",
    "svm_params = {'C': np.linspace(1e-3, 1, 30)}\n",
    "\n",
    "svm_grid_searcher = GridSearchCV(svm, svm_params, cv=skf, n_jobs=-1)\n",
    "svm_grid_searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.76437715911400117, {'C': 0.10434482758620689})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_searcher.best_score_, svm_grid_searcher.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6bd3284748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_validation_curves(svm_params['C'], svm_grid_searcher.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77909457217350087"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_cv_acc = accuracy_score(y_valid, svm_grid_searcher.predict(X_valid))\n",
    "svm_cv_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результаты всех опробованных нами моделей приблизительно одинаковы (в рамках погрешностей). Посмотрим, какое значение от ширины окна и количества сайтов в сессии.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_assessment(estimator, path_to_X_pickle, path_to_y_pickle, cv, random_state=17, test_size=0.3):\n",
    "    '''\n",
    "    Estimates CV-accuracy for (1 - test_size) share of (X_sparse, y) \n",
    "    loaded from path_to_X_pickle and path_to_y_pickle and holdout accuracy for (test_size) share of (X_sparse, y).\n",
    "    The split is made with stratified train_test_split with params random_state and test_size.\n",
    "    \n",
    "    :param estimator – Scikit-learn estimator (classifier or regressor)\n",
    "    :param path_to_X_pickle – path to pickled sparse X (instances and their features)\n",
    "    :param path_to_y_pickle – path to pickled y (responses)\n",
    "    :param cv – cross-validation as in cross_val_score (use StratifiedKFold here)\n",
    "    :param random_state –  for train_test_split\n",
    "    :param test_size –  for train_test_split\n",
    "    \n",
    "    :returns mean CV-accuracy for (X_train, y_train) and accuracy for (X_valid, y_valid) where (X_train, y_train)\n",
    "    and (X_valid, y_valid) are (1 - test_size) and (testsize) shares of (X_sparse, y).\n",
    "    '''\n",
    "    with open(path_to_X_pickle, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    with open(path_to_y_pickle, 'rb') as f:\n",
    "        y = pickle.load(f)\n",
    "    y = y.astype(np.int)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      test_size=test_size, \n",
    "                                                     random_state=random_state, stratify=y)\n",
    "    \n",
    "    \n",
    "    cv_mean = cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1).mean()\n",
    "    \n",
    "    estimator.fit(X_train, y_train)\n",
    "    \n",
    "    accuracy = accuracy_score(y_valid, estimator.predict(X_valid))\n",
    "    \n",
    "    return cv_mean, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = svm_grid_searcher.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s15_w15: CV: 0.7755197785275921, accuracy: 0.7987913259864913\n",
      "s15_w10: CV: 0.8222940711593122, accuracy: 0.8392984119459588\n",
      "s15_w7.: CV: 0.8477203248099773, accuracy: 0.852165256346441\n",
      "s15_w5.: CV: 0.8671896112653901, accuracy: 0.8749407302038881\n",
      "s10_w10: CV: 0.7643770124685939, accuracy: 0.7790945721735009\n",
      "s10_w7.: CV: 0.797862799639289, accuracy: 0.8055417288866766\n",
      "s10_w5.: CV: 0.8143487040996397, accuracy: 0.8227833096254149\n",
      "s5_w5.p: CV: 0.7214200131138755, accuracy: 0.735538169748696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for session_length, window_size in itertools.product([15, 10, 5], [15, 10, 7, 5]):\n",
    "    if window_size <= session_length:\n",
    "        path_to_X_pkl = \"./datasets/X_sparse_10users_s\" + str(session_length) + \"_w\" + str(window_size) + \".pkl\"\n",
    "        path_to_y_pkl = \"./datasets/y_10users_s\" + str(session_length) + \"_w\" + str(window_size) + \".pkl\"\n",
    "        CV, accuracy = model_assessment(estimator, path_to_X_pkl, path_to_y_pkl, cv=skf)\n",
    "        print(\"{}: CV: {}, accuracy: {}\".format(path_to_X_pkl[28:35], CV, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Итак, получается, что длина сессии в 15 сайтов и ширина окна в 5 сайтов - лучшие параметры для данного датасета**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Рискнем попробовать повторить последний шаг уже для данных по 150 пользователям**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s15_w15: CV: 0.4837601093917822, accuracy: 0.5102517329441809\n",
      "s15_w10: CV: 0.5460882450796428, accuracy: 0.572081934510777\n",
      "s15_w5.: CV: 0.6104008703556459, accuracy: 0.6326106000876041\n",
      "s10_w10: CV: 0.45998807394111346, accuracy: 0.48265460030165913\n",
      "s10_w5.: CV: 0.523446905495096, accuracy: 0.5440818610989439\n",
      "s5_w5.p: CV: 0.4077351546308323, accuracy: 0.4207426874969582\n"
     ]
    }
   ],
   "source": [
    "for session_length, window_size in itertools.product([15, 10, 5], [15, 10, 5]):\n",
    "    if window_size <= session_length:\n",
    "        path_to_X_pkl = \"./datasets/X_sparse_150users_s\" + str(session_length) + \"_w\" + str(window_size) + \".pkl\"\n",
    "        path_to_y_pkl = \"./datasets/y_150users_s\" + str(session_length) + \"_w\" + str(window_size) + \".pkl\"\n",
    "        CV, accuracy = model_assessment(estimator, path_to_X_pkl, path_to_y_pkl, cv=skf)\n",
    "        print(\"{}: CV: {}, accuracy: {}\".format(path_to_X_pkl[29:36], CV, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Да, результат получается не таким хорошим, но при длине сессии в 15 сайтов и ширине окна в 5 получается не такой уж и плохой результат. Тем не менее надо будет попробовать воспользоваться Vowpal Wabbit и попробовать добавить дополнительные признаки в датасет. Но пока посмотрим, какая вероятность с нашими текущими моделями идентифицировать одного конкретного пользователя: определять по сессии, он это или нет.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_150users.pkl'), 'rb') as X_sparse_150users_pkl:\n",
    "     X_sparse_150users = pickle.load(X_sparse_150users_pkl)\n",
    "with open(os.path.join(PATH_TO_DATA, 'y_150users.pkl'), 'rb') as y_150users_pkl:\n",
    "    y_150users = pickle.load(y_150users_pkl)\n",
    "    \n",
    "X_train_150, X_valid_150, y_train_150, y_valid_150 = train_test_split(X_sparse_150users, \n",
    "                                                                      y_150users, test_size=0.3, \n",
    "                                                     random_state=17, stratify=y_150users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.22222222222\n"
     ]
    }
   ],
   "source": [
    "print(best_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit_cv_150users = LogisticRegressionCV(Cs=[best_c], multi_class='ovr', n_jobs=-1, random_state=17)\n",
    "logit_cv_150users.fit(X_train_150, y_train_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99605858279971338"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_cv_150users.scores_[1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Красивый результат, но не реалистичный. С таким количеством пользователей константный прогноз \"не он\" позволяет получить видимость хорошего результата. Посчитаем для каждого пользователя разницу между долей правильных ответов на кросс-валидации  и долей меток, отличных от ID \n",
    " этого пользователя.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78000000000000003"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_150 = y_train_150.astype('int')\n",
    "\n",
    "class_distr = np.bincount(y_train_150.astype('int'))\n",
    "\n",
    "acc_diff_vs_constant = []\n",
    "\n",
    "for user_id in np.unique(y_train_150):\n",
    "    diff = logit_cv_150users.scores_[user_id].mean() - (sum(class_distr) - class_distr[user_id]) / sum(class_distr)\n",
    "    acc_diff_vs_constant.append(diff)\n",
    "acc_diff_vs_constant\n",
    "\n",
    "num_better_than_default = (np.array(list(acc_diff_vs_constant)) > 0).sum()\n",
    "\n",
    "prop_better_than_default = num_better_than_default / len(np.unique(y_train_150))\n",
    "prop_better_than_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Как видим, это уже намного реалистичней**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Vowpal Wabbit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переходим к Vowpal Wabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_150users_s15_w5.pkl'), 'rb') as X_train_sparse_pkl:\n",
    "    X_train_sparse = pickle.load(X_train_sparse_pkl)\n",
    "with open(os.path.join(PATH_TO_DATA, 'y_150users_s15_w5.pkl'), 'rb') as X_test_sparse_pkl:\n",
    "    y = pickle.load(X_test_sparse_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vowpal Wabbit требует, чтоб метки классов были распределены от 1 до K, где K – число классов в задаче классификации (в нашем случае – 550). Поэтому придется применить LabelEncoder и добавить +1 к получившимся меткам (LabelEncoder переводит метки в диапозон от 0 до K-1).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class_encoder = LabelEncoder().fit(y)\n",
    "y_for_vw = class_encoder.transform(y) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_sparse, y_for_vw, test_size=0.3, \n",
    "                                                     random_state=17, stratify=y_for_vw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Делаем функцию, переводящие данные в текстовые данные формата, который распознает VW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse_matrix_to_vw(X_sparse, y=1, out_file='tmp.vw'):\n",
    "    sessions = X_sparse.nonzero()[0]\n",
    "    sites = X_sparse.nonzero()[1]\n",
    "    vw_train_data = ''\n",
    "    if np.sum(y) == 1:\n",
    "        label = 1\n",
    "        for session in range(len(np.unique(sessions))):\n",
    "            sites_current_session = [sessions == session]\n",
    "            try:\n",
    "                counts = X_sparse[sessions[sites_current_session], sites[sites_current_session]].getA1().astype(np.int)\n",
    "                current_session = [str(site) + ':' + str(count) for site, count \n",
    "                                    in zip(sites[sites_current_session], counts)]\n",
    "                current_session = ' '.join(str(site) for site in current_session)\n",
    "                vowpal_line = str(label) + ' |sites ' + current_session + '\\n'\n",
    "                vw_train_data += vowpal_line\n",
    "            except AttributeError:\n",
    "                vowpal_line = str(label) + ' |sites ' + '0:0' + '\\n'\n",
    "                vw_train_data += vowpal_line\n",
    "                \n",
    "    else:\n",
    "        for session in range(len(np.unique(sessions))):\n",
    "            label = y[session]  \n",
    "            sites_current_session = [sessions == session]\n",
    "            counts = X_sparse[sessions[sites_current_session], sites[sites_current_session]].getA1().astype(np.int)\n",
    "            current_session = [str(site) + ':' + str(count) for site, count \n",
    "                                in zip(sites[sites_current_session], counts)]\n",
    "            current_session = ' '.join(str(site) for site in current_session)\n",
    "            vowpal_line = str(label) + ' |sites ' + current_session + '\\n'\n",
    "            vw_train_data += vowpal_line\n",
    "                \n",
    "\n",
    "    with open(out_file, 'w') as fout:\n",
    "        fout.write(vw_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sparse_matrix_to_vw(X_train, y_train, os.path.join(PATH_TO_DATA, 'train_part.vw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sparse_matrix_to_vw(X_valid, y_valid, os.path.join(PATH_TO_DATA, 'valid.vw'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vw = './datasets/train_part.vw'\n",
    "valid_vw = './datasets/valid.vw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренируем простую линейную модель VW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "final_regressor = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_model.vw\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/train_part.vw.cache\n",
      "Reading datafile = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/train_part.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0       88        1        7\n",
      "1.000000 1.000000            2            2.0       64       88        9\n",
      "1.000000 1.000000            4            4.0       91       88        7\n",
      "1.000000 1.000000            8            8.0       17       88       10\n",
      "1.000000 1.000000           16           16.0       41       64       13\n",
      "1.000000 1.000000           32           32.0      138      119        6\n",
      "1.000000 1.000000           64           64.0       28       45        8\n",
      "0.968750 0.937500          128          128.0       76       59       13\n",
      "0.949219 0.929688          256          256.0       16       14       11\n",
      "0.908203 0.867188          512          512.0      124      146       12\n",
      "0.860352 0.812500         1024         1024.0       42       42        7\n",
      "0.814453 0.768555         2048         2048.0       78       93       13\n",
      "0.771484 0.728516         4096         4096.0      125      125       12\n",
      "0.709839 0.648193         8192         8192.0      102      102       10\n",
      "0.657288 0.604736        16384        16384.0      109      109       14\n",
      "0.600647 0.544006        32768        32768.0      141       56        9\n",
      "0.544983 0.489319        65536        65536.0       99       29       13\n",
      "0.492027 0.439072       131072       131072.0       57       13        9\n",
      "0.447264 0.447264       262144       262144.0       28       28        6 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 172593\n",
      "passes used = 3\n",
      "weighted example sum = 517779.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.379693 h\n",
      "total feature number = 4834725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw --oaa 400 -d C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/train_part.vw \\\n",
    "-f C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_model.vw   \\\n",
    "--passes 3 -b 26 -c --random_seed 17 -k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring duplicate option: '--bit_precision 26'\n",
      "only testing\n",
      "predictions = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_pred.vw\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/valid.vw.cache\n",
      "Reading datafile = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0      129      129       13\n",
      "0.000000 0.000000            2            2.0      124      124       12\n",
      "0.250000 0.500000            4            4.0       63       73       10\n",
      "0.375000 0.500000            8            8.0      131      131       12\n",
      "0.437500 0.500000           16           16.0       30      113       13\n",
      "0.343750 0.250000           32           32.0       59       42       12\n",
      "0.359375 0.375000           64           64.0       29       29        3\n",
      "0.343750 0.328125          128          128.0        8        8       10\n",
      "0.308594 0.273438          256          256.0      105      102       11\n",
      "0.357422 0.406250          512          512.0      138      138       10\n",
      "0.368164 0.378906         1024         1024.0        1        1        8\n",
      "0.375000 0.381836         2048         2048.0       29       29       13\n",
      "0.382568 0.390137         4096         4096.0       59       59       11\n",
      "0.381470 0.380371         8192         8192.0       29       29        4\n",
      "0.373718 0.365967        16384        16384.0      107      107       13\n",
      "0.374359 0.375000        32768        32768.0       33       33       14\n",
      "0.373108 0.371857        65536        65536.0       29       29        4\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 82188\n",
      "passes used = 1\n",
      "weighted example sum = 82188.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.372049\n",
      "total feature number = 766829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -i C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_model.vw \\\n",
    "-t -d C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/valid.vw \\\n",
    "-p C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_pred.vw \\\n",
    " -c -b 26 --random_seed 17 -k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62795055239207676"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vw_valid_pred = pd.read_csv(\"./datasets/vw_pred.vw\", header=None)\n",
    "accuracy_score(y_valid, vw_valid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, простая модель в VW дала тот же результат, что и линейная модель в ScikitLearn. Попробуем улучшить ее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим N-Gram, который может помочь в нашем случае"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 100-grams for all namespaces.\n",
      "final_regressor = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_model.vw\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/train_part.vw.cache\n",
      "Reading datafile = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/train_part.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0       88        1       22\n",
      "1.000000 1.000000            2            2.0       64       88       37\n",
      "1.000000 1.000000            4            4.0       91       88       22\n",
      "1.000000 1.000000            8            8.0       17       88       46\n",
      "1.000000 1.000000           16           16.0       41      100       79\n",
      "1.000000 1.000000           32           32.0      138       95       16\n",
      "0.984375 0.968750           64           64.0       28      109       29\n",
      "0.953125 0.921875          128          128.0       76        1       79\n",
      "0.941406 0.929688          256          256.0       16       42       56\n",
      "0.898438 0.855469          512          512.0      124       21       67\n",
      "0.870117 0.841797         1024         1024.0       42       42       22\n",
      "0.832031 0.793945         2048         2048.0       78       49       79\n",
      "0.795898 0.759766         4096         4096.0      125      125       67\n",
      "0.756714 0.717529         8192         8192.0      102      102       46\n",
      "0.704102 0.651489        16384        16384.0      109      109       92\n",
      "0.645172 0.586243        32768        32768.0      141       56       37\n",
      "0.577423 0.509674        65536        65536.0       99      123       79\n",
      "0.501297 0.425171       131072       131072.0       57       56       37\n",
      "0.422440 0.422440       262144       262144.0       28       28       16 h\n",
      "0.379238 0.336034       524288       524288.0        1        1       29 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 172593\n",
      "passes used = 5\n",
      "weighted example sum = 862965.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.336879 h\n",
      "total feature number = 38827975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw --oaa 400 -d C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/train_part.vw \\\n",
    "-f C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_model.vw   \\\n",
    "--passes 5 -b 26 -c --random_seed 17 -k --ngram 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring duplicate option: '--bit_precision 26'\n",
      "Generating 100-grams for all namespaces.\n",
      "only testing\n",
      "predictions = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_pred.vw\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/valid.vw.cache\n",
      "Reading datafile = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0      129      129       79\n",
      "0.000000 0.000000            2            2.0      124      124       67\n",
      "0.000000 0.000000            4            4.0       63       63       46\n",
      "0.250000 0.500000            8            8.0      131      131       67\n",
      "0.312500 0.375000           16           16.0       30       30       79\n",
      "0.250000 0.187500           32           32.0       59       59       67\n",
      "0.312500 0.375000           64           64.0       29       29        4\n",
      "0.320313 0.328125          128          128.0        8      104       46\n",
      "0.304688 0.289063          256          256.0      105      104       56\n",
      "0.308594 0.312500          512          512.0      138      138       46\n",
      "0.326172 0.343750         1024         1024.0        1        1       29\n",
      "0.340820 0.355469         2048         2048.0       29       62       79\n",
      "0.343750 0.346680         4096         4096.0       59       11       56\n",
      "0.340576 0.337402         8192         8192.0       29       12        7\n",
      "0.340698 0.340820        16384        16384.0      107      107       79\n",
      "0.338745 0.336792        32768        32768.0       33       33       92\n",
      "0.333939 0.329132        65536        65536.0       29       29        7\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 82188\n",
      "passes used = 1\n",
      "weighted example sum = 82188.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.331922\n",
      "total feature number = 3693183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -i C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_model.vw \\\n",
    "-t -d C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/valid.vw \\\n",
    "-p C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_pred.vw \\\n",
    " -c -b 26 --random_seed 17 -k  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66807806492431987"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vw_valid_pred = pd.read_csv(\"./datasets/vw_pred.vw\", header=None)\n",
    "accuracy_score(y_valid, vw_valid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Как видим, добавление ngram привело к дальнейшему улучшению модели**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Генерация признаков и их анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь немного поработаем с новыми дополнительными признаками, добавление которых может улучшить нашу модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_n_unique(array):\n",
    "    unique_sites = np.unique(array)\n",
    "    if 0 in unique_sites:\n",
    "        return len(unique_sites) - 1\n",
    "    else:\n",
    "        return len(unique_sites)\n",
    "\n",
    "def get_session_timespan(timestamps, session_length, window_size):\n",
    "    session_timespan = []\n",
    "    for i in range(0, len(timestamps), window_size):\n",
    "        try:\n",
    "            session_timespan.append(int((timestamps[session_length - 1 + i] - timestamps[i]).total_seconds()))\n",
    "        except KeyError:\n",
    "            session_timespan.append(int((timestamps.iloc[-1] - timestamps[i]).total_seconds()))\n",
    "    return np.array(session_timespan)\n",
    "    \n",
    "\n",
    "def prepare_train_set_with_fe(path_to_csv_files, site_freq_path, feature_names,\n",
    "                                    session_length=10, window_size=10):\n",
    "    \n",
    "    sites_freq = pickle.load(open(site_freq_path, 'rb'))\n",
    "    files = glob(os.path.join(path_to_csv_files, '*.csv'))\n",
    "\n",
    "    data_array = np.empty((0, len(feature_names)), int)\n",
    "    \n",
    "    for file in tqdm(files):\n",
    "        user_df = pd.read_csv(file)\n",
    "        user_sites = user_df.site.values\n",
    "        user_sites_ids = []\n",
    "        \n",
    "        # переводим данные о времени посещения сайтов в формат datetime\n",
    "        timestamps = pd.to_datetime(user_df.timestamp)\n",
    "        \n",
    "        for site in user_sites:\n",
    "            # убираем лишние префиксы в названии сайта\n",
    "            correct_site = site.replace(\"www.\", \"\")\n",
    "            correct_site = correct_site.replace(\"http://\", \"\")\n",
    "            correct_site = correct_site.replace(\"https://\", \"\")\n",
    "            \n",
    "            site_id = sites_freq[correct_site][0]\n",
    "            user_sites_ids.append(site_id)\n",
    "        \n",
    "        # добавляем нули в конце сессии при необходимости\n",
    "        if len(user_sites_ids) % window_size != 0:\n",
    "            user_sites_ids = np.pad(user_sites_ids, (0, session_length - len(user_sites_ids) % window_size), 'constant')\n",
    "        remainder = (len(user_sites_ids) - ((len(user_sites_ids) / window_size) - 1) * window_size) % session_length\n",
    "        if remainder != 0:\n",
    "            user_sites_ids = np.pad(user_sites_ids, (0, session_length - int(remainder)), 'constant')\n",
    "                              \n",
    "        user_sites_ids = np.vstack([user_sites_ids[i:session_length+i] for i in range(0, len(user_sites), window_size)])\n",
    "        \n",
    "        # создаем признак session_timespan для сессий пользователя\n",
    "        session_timespan = get_session_timespan(timestamps, session_length, window_size)\n",
    "        session_timespan = np.reshape(session_timespan, [user_sites_ids.shape[0], -1])\n",
    "\n",
    "        # создаем признак n_unique для сессий пользователя      \n",
    "        n_unique = np.apply_along_axis(count_n_unique, axis=1, arr=user_sites_ids)\n",
    "        n_unique = np.reshape(n_unique, [user_sites_ids.shape[0], -1])\n",
    "        \n",
    "        # создаем признак start_hour для сессий пользователя      \n",
    "        start_hour = np.array([timestamps[i].hour for i in range(0, len(user_sites), window_size)])\n",
    "        start_hour = np.reshape(start_hour, [user_sites_ids.shape[0], -1])\n",
    "        \n",
    "        # создаем признак day_of_week для сессий пользователя      \n",
    "        day_of_week = np.array([timestamps[i].dayofweek for i in range(0, len(user_sites), window_size)])\n",
    "        day_of_week = np.reshape(day_of_week, [user_sites_ids.shape[0], -1])\n",
    "\n",
    "        # создаем столбец с id пользователя \n",
    "        user_id = int(os.path.splitext(file)[0][-4:])\n",
    "        user_ids = np.array([user_id for i in range(user_sites_ids.shape[0])])\n",
    "        user_ids = np.reshape(user_ids, [user_sites_ids.shape[0], -1])\n",
    "        \n",
    "        #объединяем все столбцы по пользователю\n",
    "        user_data_array = np.hstack([user_sites_ids, session_timespan, n_unique, start_hour, day_of_week, user_ids])\n",
    "        \n",
    "        # объединяем матрицу с признаками по текущему пользователю с общей матрицей по всем пользователям\n",
    "        data_array = np.vstack([data_array, user_data_array])    \n",
    "    \n",
    "    df = pd.DataFrame(data_array, columns=feature_names)\n",
    "    \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = ['site' + str(i) for i in range(1,11)] + \\\n",
    "                ['session_timespan', '#unique_sites', 'start_hour', \n",
    "                 'day_of_week', 'user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [00:08<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_10users = prepare_train_set_with_fe(os.path.join(PATH_TO_DATA, \n",
    "                                                         '10users'), \n",
    "                  site_freq_path=os.path.join(PATH_TO_DATA, \n",
    "                                              'site_freq_10users.pkl'),\n",
    "                  feature_names=feature_names, session_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 150/150 [00:36<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_150users = prepare_train_set_with_fe(os.path.join(PATH_TO_DATA, \n",
    "                                                         '150users'), \n",
    "                  site_freq_path=os.path.join(PATH_TO_DATA, \n",
    "                                              'site_freq_150users.pkl'),\n",
    "                  feature_names=feature_names, session_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top10(site_freq_path):\n",
    "    sites_freq = pickle.load(open(site_freq_path, 'rb'))\n",
    "    sites_freq = pd.DataFrame.from_dict(sites_freq, orient='index')\n",
    "    sites_freq = sites_freq.sort_values(by=1, axis=0, ascending=False, kind='mergesort')\n",
    "    sites_freq.columns = ['id', 'views']\n",
    "    top10_sites = np.array(sites_freq.index[:10])\n",
    "    return top10_sites\n",
    "\n",
    "\n",
    "def feature_engineering(path_to_csv_files, site_freq_path, features, session_length=10):\n",
    "    \n",
    "    top10_sites = top10(site_freq_path)\n",
    "    files = glob(os.path.join(path_to_csv_files, '*.csv'))\n",
    "    \n",
    "    data_array = np.empty((0, len(features)), int)\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        user_df = pd.read_csv(file)\n",
    "        user_sites = user_df.site.values\n",
    "        \n",
    "        timestamps = pd.to_datetime(user_df.timestamp)\n",
    "        \n",
    "        is_weekend = np.array([0 if timestamps[i].dayofweek < 5 else 1 for i in range(0, len(user_sites), session_length)])\n",
    "        is_weekend = np.reshape(is_weekend, [len(is_weekend), -1])\n",
    "        \n",
    "        time_of_day = []\n",
    "        for i in range(0, len(user_sites), session_length):\n",
    "            if timestamps[i].hour < 5:\n",
    "                time_of_day.append(0)\n",
    "            if timestamps[i].hour < 10:\n",
    "                time_of_day.append(1)\n",
    "            elif timestamps[i].hour < 19:\n",
    "                time_of_day.append(2)\n",
    "            else:\n",
    "                time_of_day.append(3)\n",
    "        time_of_day = np.array(time_of_day)\n",
    "        time_of_day = np.reshape(time_of_day, [len(time_of_day), -1])\n",
    "        \n",
    "        visited_top10 = []\n",
    "        for site in user_sites:\n",
    "            if site in top10_sites:\n",
    "                visited_top10.append(1)\n",
    "            else: \n",
    "                visited_top10.append(-1)\n",
    "                \n",
    "        if len(user_sites) % session_length != 0:\n",
    "            visited_top10 = np.pad(visited_top10, (0, session_length - len(visited_top10) % session_length), 'constant')\n",
    "        visited_top10 = np.reshape(visited_top10, [-1, session_length])\n",
    "\n",
    "        proportion_top10 = np.apply_along_axis(sum, axis=1, arr=visited_top10)\n",
    "        proportion_top10 = np.reshape(proportion_top10, [len(proportion_top10), -1])\n",
    "        \n",
    "        new_features = np.hstack([is_weekend, time_of_day, proportion_top10])\n",
    "        \n",
    "        data_array = np.vstack([data_array, new_features])  \n",
    "\n",
    "    return pd.DataFrame(data_array, columns=features)\n",
    "\n",
    "features = ['is_weekend', 'time_of_day', 'proportion_top10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [00:03<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_features_10users = feature_engineering(os.path.join(PATH_TO_DATA, '10users'), \n",
    "                  site_freq_path=os.path.join(PATH_TO_DATA, 'site_freq_10users.pkl'),\n",
    "                                           features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 150/150 [00:28<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_features_150users = feature_engineering(os.path.join(PATH_TO_DATA, '150users'), \n",
    "                  site_freq_path=os.path.join(PATH_TO_DATA, 'site_freq_150users.pkl'),\n",
    "                                           features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_10users[features] = new_features_10users\n",
    "train_data_150users[features] = new_features_150users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_features_10users = train_data_10users[['session_timespan', '#unique_sites', 'start_hour',\n",
    "       'day_of_week', 'is_weekend', 'time_of_day',\n",
    "       'proportion_top10']]\n",
    "\n",
    "selected_features_150users = train_data_150users[['session_timespan', '#unique_sites', 'start_hour',\n",
    "       'day_of_week', 'is_weekend', 'time_of_day',\n",
    "       'proportion_top10']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, \n",
    "                       'selected_features_10users.pkl'), 'wb') as selected_features_10users_pkl:\n",
    "    pickle.dump(selected_features_10users, selected_features_10users_pkl, \n",
    "                protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, \n",
    "                       'selected_features_150users.pkl'), 'wb') as selected_features_150users_pkl:\n",
    "    pickle.dump(selected_features_150users, selected_features_150users_pkl, \n",
    "                protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы сгенерировали следующие признаки:\n",
    "- session_timespan – продолжительность сессии (разница между максимальным и минимальным временем посещения сайтов в сессии, в секундах)\n",
    "- #unique_sites – число уникальных сайтов в сессии\n",
    "- start_hour – час начала сессии (то есть час в записи минимального timestamp среди десяти)\n",
    "- day_of_week – день недели (то есть день недели в записи минимального timestamp среди десяти)\n",
    "- is_weekend - бинарный признак, является ли день начала сессии выходным, \n",
    "- time_of_day - категориальный признак, указывающий время дня (ночь/утро, вечер или день), \n",
    "- proportion_top10 - признак, указывающий разницу между количеством сайтов в сессии, входящих в топ10 и не входящих"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Давайте посмотрим на несколько визуализаций этих признаков (я не стал делать много, остальные в уже проверенном задании недели 3, не хочется делать блокнот еще более длинным, чем он уже есть)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на гистограмму распределения длины сессии в секундах (*session_timespan*).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(10, 6))\n",
    "\n",
    "# увеличил параметр bins для большей точности графика\n",
    "plt.hist(x=train_data_10users['session_timespan'].values, range=(0,200), bins=50, color='darkviolet')\n",
    "plt.xlabel('Длина сессии')\n",
    "plt.ylabel('Количество сессий')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на гистограмму распределения числа уникальных сайтов в сессии (*#unique_sites*). **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(10, 6))\n",
    "\n",
    "train_data_10users['#unique_sites'].plot(kind='hist', color='aqua')\n",
    "plt.xlabel('Количество уникальных сайтов в сессии')\n",
    "plt.ylabel('Количество сессий')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сравним час начала сессии в будние дни и в выходные.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(10, 6))\n",
    "\n",
    "sns.countplot(x='start_hour', hue='is_weekend',\n",
    "                            data=train_data_10users)\n",
    "\n",
    "plt.xlabel('Час начала сессии')\n",
    "plt.ylabel('Количество сессий')\n",
    "plt.legend(labels=['Не выходной', 'Выходной'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделали новые признаки, немного посмотрели на них, теперь можно добавить их в данные для VW и посмотреть, улучшится или ухудшится результат**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_matrix_and_features_to_vw(X_sparse, y, features, out_file='tmp.vw'):\n",
    "    sessions = X_sparse.nonzero()[0]\n",
    "    sites = X_sparse.nonzero()[1]\n",
    "    vw_train_data = ''\n",
    "    if np.sum(y) == 1:\n",
    "        label = 1\n",
    "        for session in range(len(np.unique(sessions))):\n",
    "            sites_current_session = [sessions == session]\n",
    "            try:\n",
    "                counts = X_sparse[sessions[sites_current_session], sites[sites_current_session]].getA1().astype(np.int)\n",
    "                current_session = [str(site) + ':' + str(count) for site, count \n",
    "                                    in zip(sites[sites_current_session], counts)]\n",
    "                current_session = ' '.join(str(site) for site in current_session)\n",
    "                \n",
    "                string_features = \"\"\n",
    "                for index in range(0, features.shape[1]):\n",
    "                    string_features += (\" |\" + features.columns[index] +\n",
    "                                    \" \" + str(features.iloc[1, index]))              \n",
    "                \n",
    "                vowpal_line = str(label) + string_features + ' |sites ' + current_session + '\\n'\n",
    "                vw_train_data += vowpal_line\n",
    "            except AttributeError:\n",
    "                vowpal_line = str(label) + ' |sites ' + '0:0' + '\\n'\n",
    "                vw_train_data += vowpal_line\n",
    "                \n",
    "    else:\n",
    "        for session in range(len(np.unique(sessions))):\n",
    "            label = y[session]  \n",
    "            sites_current_session = [sessions == session]\n",
    "            counts = X_sparse[sessions[sites_current_session], sites[sites_current_session]].getA1().astype(np.int)\n",
    "            current_session = [str(site) + ':' + str(count) for site, count \n",
    "                                in zip(sites[sites_current_session], counts)]\n",
    "            current_session = ' '.join(str(site) for site in current_session)\n",
    "                \n",
    "            string_features = \"\"\n",
    "            for index in range(0, features.shape[1]):\n",
    "                string_features += (\" |\" + features.columns[index] +\n",
    "                                    \" \" + str(features.iloc[1, index]))            \n",
    "                \n",
    "            vowpal_line = str(label) + string_features + ' |sites ' + current_session + '\\n'\n",
    "            vw_train_data += vowpal_line\n",
    "                \n",
    "\n",
    "    with open(out_file, 'w') as fout:\n",
    "        fout.write(vw_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_150users_s15_w5.pkl'), 'rb') as X_train_sparse_pkl:\n",
    "    X_train_sparse = pickle.load(X_train_sparse_pkl)\n",
    "with open(os.path.join(PATH_TO_DATA, 'y_150users_s15_w5.pkl'), 'rb') as X_test_sparse_pkl:\n",
    "    y = pickle.load(X_test_sparse_pkl)\n",
    "    \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_sparse, y_for_vw, test_size=0.3, \n",
    "                                                     random_state=17, stratify=y_for_vw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sparse_matrix_and_features_to_vw(X_train, y_train, selected_features_150users, \n",
    "                                 os.path.join(PATH_TO_DATA, 'train_with_features.vw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sparse_matrix_and_features_to_vw(X_valid, y_valid, selected_features_150users, \n",
    "                                 os.path.join(PATH_TO_DATA, 'valid_with_features.vw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "final_regressor = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_model_with_features.vw\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/train_with_features.vw.cache\n",
      "Reading datafile = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/train_with_features.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0       88        1       14\n",
      "1.000000 1.000000            2            2.0       64       88       16\n",
      "1.000000 1.000000            4            4.0       91      103       14\n",
      "1.000000 1.000000            8            8.0       17       46       17\n",
      "1.000000 1.000000           16           16.0       41      100       20\n",
      "1.000000 1.000000           32           32.0      138       24       13\n",
      "1.000000 1.000000           64           64.0       28       75       15\n",
      "0.976563 0.953125          128          128.0       76       82       20\n",
      "0.960938 0.945313          256          256.0       16       97       18\n",
      "0.933594 0.906250          512          512.0      124       97       19\n",
      "0.899414 0.865234         1024         1024.0       42      117       14\n",
      "0.861816 0.824219         2048         2048.0       78       69       20\n",
      "0.808838 0.755859         4096         4096.0      125      125       19\n",
      "0.743286 0.677734         8192         8192.0      102       20       17\n",
      "0.684265 0.625244        16384        16384.0      109      109       21\n",
      "0.624451 0.564636        32768        32768.0      141       56       16\n",
      "0.564331 0.504211        65536        65536.0       99       21       20\n",
      "0.509026 0.453720       131072       131072.0       57       66       16\n",
      "0.461924 0.461924       262144       262144.0       28       28       13 h\n",
      "0.425778 0.389631       524288       524288.0        1        7       15 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 172593\n",
      "passes used = 5\n",
      "weighted example sum = 862965.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.378390 h\n",
      "total feature number = 14098630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw --oaa 400 -d C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/train_with_features.vw \\\n",
    "-f C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_model_with_features.vw   \\\n",
    "--passes 5 -b 26 -c --random_seed 17 -k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring duplicate option: '--bit_precision 26'\n",
      "only testing\n",
      "predictions = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_pred_with_features.vw\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/valid_with_features.vw.cache\n",
      "Reading datafile = C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/valid_with_features.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0      129      129       20\n",
      "0.000000 0.000000            2            2.0      124      124       19\n",
      "0.250000 0.500000            4            4.0       63       73       17\n",
      "0.375000 0.500000            8            8.0      131      131       19\n",
      "0.500000 0.625000           16           16.0       30      113       20\n",
      "0.375000 0.250000           32           32.0       59       42       19\n",
      "0.375000 0.375000           64           64.0       29       29       10\n",
      "0.343750 0.312500          128          128.0        8        8       17\n",
      "0.308594 0.273438          256          256.0      105      102       18\n",
      "0.355469 0.402344          512          512.0      138      138       17\n",
      "0.365234 0.375000         1024         1024.0        1        1       15\n",
      "0.372070 0.378906         2048         2048.0       29       29       20\n",
      "0.382813 0.393555         4096         4096.0       59       59       18\n",
      "0.380981 0.379150         8192         8192.0       29       29       11\n",
      "0.374634 0.368286        16384        16384.0      107      107       20\n",
      "0.374634 0.374634        32768        32768.0       33       33       21\n",
      "0.373047 0.371460        65536        65536.0       29       29       11\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 82188\n",
      "passes used = 1\n",
      "weighted example sum = 82188.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.371818\n",
      "total feature number = 1342145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -i C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_model_with_features.vw \\\n",
    "-t -d C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/valid_with_features.vw \\\n",
    "-p C:/Users/Polly/Documents/Roma/Python/Coursera/MO-Phistekch/06/Identification/datasets/vw_pred_with_features.vw \\\n",
    " -c -b 26 --random_seed 17 -k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6281817296928992"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vw_valid_pred = pd.read_csv(\"./datasets/vw_pred_with_features.vw\", header=None)\n",
    "accuracy_score(y_valid, vw_valid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Получается, что для простую линейную модель такие дополнительные признаки не улучшают. Тем не менее их изучение, как мы уже убедились на графиках, может помочь лучше понять привычки пользователей и различия между пользователями. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы изучили данные, построили на их основе датасет для простой линейной модели. Лучшая из получившихся моделей обеспечивает аккуратность, приближающуюся к 70%. Вычислительных мощностей ноутбука не хватает на то, чтобы провести дополнительный отбор и генерацию признаков в отношении нашего датасета. При наличии доступа к большим вычислительным мощностям есть реальная возможность сильно улучшить модель и сделать ее более эффективной. Но уже и в такой форме ее использование позволит идентифицировать пользователей с намного большей вероятностью, чем случайной."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
