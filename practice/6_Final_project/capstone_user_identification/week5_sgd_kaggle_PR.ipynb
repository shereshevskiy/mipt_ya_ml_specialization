{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://habrastorage.org/web/677/8e1/337/6778e1337c3d4b159d7e99df94227cb2.jpg\"/>\n",
    "## Специализация \"Машинное обучение и анализ данных\"\n",
    "</center>\n",
    "<center>Автор материала: программист-исследователь Mail.ru Group, старший преподаватель Факультета Компьютерных Наук ВШЭ Юрий Кашницкий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Capstone проект №1. Идентификация пользователей по посещенным веб-страницам\n",
    "<img src='http://i.istockimg.com/file_thumbview_approve/21546327/5/stock-illustration-21546327-identification-de-l-utilisateur.jpg'>\n",
    "\n",
    "# <center>Неделя 5.  Соревнование Kaggle \"Catch Me If You Can\"\n",
    "\n",
    "На этой неделе мы вспомним про концепцию стохастического градиентного спуска и опробуем классификатор Scikit-learn SGDClassifier, который работает намного быстрее на больших выборках, чем алгоритмы, которые мы тестировали на 4 неделе. Также мы познакомимся с данными [соревнования](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) Kaggle по идентификации пользователей и сделаем в нем первые посылки. По итогам этой недели дополнительные баллы получат те, кто попадет в топ-30 публичного лидерборда соревнования.\n",
    "\n",
    "**В этой части проекта Вам могут быть полезны видеозаписи следующих лекций курса \"Обучение на размеченных данных\":**\n",
    "   - [Стохатический градиентный спуск](https://www.coursera.org/learn/supervised-learning/lecture/xRY50/stokhastichieskii-ghradiientnyi-spusk)\n",
    "   - [Линейные модели. Sklearn.linear_model. Классификация](https://www.coursera.org/learn/supervised-learning/lecture/EBg9t/linieinyie-modieli-sklearn-linear-model-klassifikatsiia)\n",
    "   \n",
    "**Также рекомендуется вернуться и просмотреть [задание](https://www.coursera.org/learn/supervised-learning/programming/t2Idc/linieinaia-rieghriessiia-i-stokhastichieskii-ghradiientnyi-spusk) \"Линейная регрессия и стохастический градиентный спуск\" 1 недели 2 курса специализации.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считаем данные [соревнования](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) в DataFrame train_df и test_df (обучающая и тестовая выборки).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Поменяйте на свой путь к данным\n",
    "PATH_TO_DATA = ('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n",
    "                       index_col='session_id')\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n",
    "                      index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>2014-02-20 10:02:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>942.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>2014-02-22 11:20:15</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>2014-02-22 11:20:16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>2013-12-16 16:40:17</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013-12-16 16:40:18</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14769.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:20</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:21</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:22</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:52:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:52:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:54:12</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-28 10:54:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>2014-02-28 10:53:05</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:57:06</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:57:11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1  site2                time2    site3  \\\n",
       "session_id                                                                    \n",
       "1             718  2014-02-20 10:02:45    NaN                  NaN      NaN   \n",
       "2             890  2014-02-22 11:19:50  941.0  2014-02-22 11:19:50   3847.0   \n",
       "3           14769  2013-12-16 16:40:17   39.0  2013-12-16 16:40:18  14768.0   \n",
       "4             782  2014-03-28 10:52:12  782.0  2014-03-28 10:52:42    782.0   \n",
       "5              22  2014-02-28 10:53:05  177.0  2014-02-28 10:55:22    175.0   \n",
       "\n",
       "                          time3    site4                time4  site5  \\\n",
       "session_id                                                             \n",
       "1                           NaN      NaN                  NaN    NaN   \n",
       "2           2014-02-22 11:19:51    941.0  2014-02-22 11:19:51  942.0   \n",
       "3           2013-12-16 16:40:19  14769.0  2013-12-16 16:40:19   37.0   \n",
       "4           2014-03-28 10:53:12    782.0  2014-03-28 10:53:42  782.0   \n",
       "5           2014-02-28 10:55:22    178.0  2014-02-28 10:55:23  177.0   \n",
       "\n",
       "                          time5  ...                  time6    site7  \\\n",
       "session_id                       ...                                   \n",
       "1                           NaN  ...                    NaN      NaN   \n",
       "2           2014-02-22 11:19:51  ...    2014-02-22 11:19:51   3847.0   \n",
       "3           2013-12-16 16:40:19  ...    2013-12-16 16:40:19  14768.0   \n",
       "4           2014-03-28 10:54:12  ...    2014-03-28 10:54:42    782.0   \n",
       "5           2014-02-28 10:55:23  ...    2014-02-28 10:55:59    175.0   \n",
       "\n",
       "                          time7    site8                time8    site9  \\\n",
       "session_id                                                               \n",
       "1                           NaN      NaN                  NaN      NaN   \n",
       "2           2014-02-22 11:19:52   3846.0  2014-02-22 11:19:52   1516.0   \n",
       "3           2013-12-16 16:40:20  14768.0  2013-12-16 16:40:21  14768.0   \n",
       "4           2014-03-28 10:55:12    782.0  2014-03-28 10:55:42    782.0   \n",
       "5           2014-02-28 10:55:59    177.0  2014-02-28 10:55:59    177.0   \n",
       "\n",
       "                          time9   site10               time10 target  \n",
       "session_id                                                            \n",
       "1                           NaN      NaN                  NaN      0  \n",
       "2           2014-02-22 11:20:15   1518.0  2014-02-22 11:20:16      0  \n",
       "3           2013-12-16 16:40:22  14768.0  2013-12-16 16:40:24      0  \n",
       "4           2014-03-28 10:56:12    782.0  2014-03-28 10:56:42      0  \n",
       "5           2014-02-28 10:57:06    178.0  2014-02-28 10:57:11      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Объединим обучающую и тестовую выборки – это понадобится, чтоб вместе потом привести их к разреженному формату.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающей выборке видим следующие признаки:\n",
    "    - site1 – индекс первого посещенного сайта в сессии\n",
    "    - time1 – время посещения первого сайта в сессии\n",
    "    - ...\n",
    "    - site10 – индекс 10-го посещенного сайта в сессии\n",
    "    - time10 – время посещения 10-го сайта в сессии\n",
    "    - user_id – ID пользователя\n",
    "    \n",
    "Сессии пользователей выделены таким образом, что они не могут быть длинее получаса или 10 сайтов. То есть сессия считается оконченной либо когда пользователь посетил 10 сайтов подряд, либо когда сессия заняла по времени более 30 минут. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на статистику признаков.**\n",
    "\n",
    "Пропуски возникают там, где сессии короткие (менее 10 сайтов). Скажем, если человек 1 января 2015 года посетил *vk.com* в 20:01, потом *yandex.ru* в 20:29, затем *google.com* в 20:33, то первая его сессия будет состоять только из двух сайтов (site1 – ID сайта *vk.com*, time1 – 2015-01-01 20:01:00, site2 – ID сайта  *yandex.ru*, time2 – 2015-01-01 20:29:00, остальные признаки – NaN), а начиная с *google.com* пойдет новая сессия, потому что уже прошло более 30 минут с момента посещения *vk.com*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 253561 entries, 1 to 253561\n",
      "Data columns (total 21 columns):\n",
      "site1     253561 non-null int64\n",
      "time1     253561 non-null object\n",
      "site2     250098 non-null float64\n",
      "time2     250098 non-null object\n",
      "site3     246919 non-null float64\n",
      "time3     246919 non-null object\n",
      "site4     244321 non-null float64\n",
      "time4     244321 non-null object\n",
      "site5     241829 non-null float64\n",
      "time5     241829 non-null object\n",
      "site6     239495 non-null float64\n",
      "time6     239495 non-null object\n",
      "site7     237297 non-null float64\n",
      "time7     237297 non-null object\n",
      "site8     235224 non-null float64\n",
      "time8     235224 non-null object\n",
      "site9     233084 non-null float64\n",
      "time9     233084 non-null object\n",
      "site10    231052 non-null float64\n",
      "time10    231052 non-null object\n",
      "target    253561 non-null int64\n",
      "dtypes: float64(9), int64(2), object(10)\n",
      "memory usage: 42.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>site6</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>2014-10-04 11:19:53</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2014-10-04 11:19:53</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>321.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>2211.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>6730.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>44582.0</td>\n",
       "      <td>2014-10-04 11:20:00</td>\n",
       "      <td>15336.0</td>\n",
       "      <td>2014-10-04 11:20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>782</td>\n",
       "      <td>2014-07-03 11:00:28</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:00:53</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:00:58</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:06</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:09</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:10</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:23</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:29</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:30</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>2014-12-05 15:55:12</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:55:13</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:55:14</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:15</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:16</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:17</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:18</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:19</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>2014-12-05 15:56:33</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>2014-12-05 15:56:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1023</td>\n",
       "      <td>2014-11-04 10:03:19</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>2014-11-04 10:03:19</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2014-11-04 10:03:20</td>\n",
       "      <td>222.0</td>\n",
       "      <td>2014-11-04 10:03:21</td>\n",
       "      <td>202.0</td>\n",
       "      <td>2014-11-04 10:03:21</td>\n",
       "      <td>3374.0</td>\n",
       "      <td>2014-11-04 10:03:22</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2014-11-04 10:03:22</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2014-11-04 10:03:22</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2014-11-04 10:03:23</td>\n",
       "      <td>3374.0</td>\n",
       "      <td>2014-11-04 10:03:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>301</td>\n",
       "      <td>2014-05-16 15:05:31</td>\n",
       "      <td>301.0</td>\n",
       "      <td>2014-05-16 15:05:32</td>\n",
       "      <td>301.0</td>\n",
       "      <td>2014-05-16 15:05:33</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2014-05-16 15:05:39</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2014-05-16 15:05:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1   site2                time2  site3  \\\n",
       "session_id                                                                   \n",
       "1              29  2014-10-04 11:19:53    35.0  2014-10-04 11:19:53   22.0   \n",
       "2             782  2014-07-03 11:00:28   782.0  2014-07-03 11:00:53  782.0   \n",
       "3              55  2014-12-05 15:55:12    55.0  2014-12-05 15:55:13   55.0   \n",
       "4            1023  2014-11-04 10:03:19  1022.0  2014-11-04 10:03:19   50.0   \n",
       "5             301  2014-05-16 15:05:31   301.0  2014-05-16 15:05:32  301.0   \n",
       "\n",
       "                          time3  site4                time4  site5  \\\n",
       "session_id                                                           \n",
       "1           2014-10-04 11:19:54  321.0  2014-10-04 11:19:54   23.0   \n",
       "2           2014-07-03 11:00:58  782.0  2014-07-03 11:01:06  782.0   \n",
       "3           2014-12-05 15:55:14   55.0  2014-12-05 15:56:15   55.0   \n",
       "4           2014-11-04 10:03:20  222.0  2014-11-04 10:03:21  202.0   \n",
       "5           2014-05-16 15:05:33   66.0  2014-05-16 15:05:39   67.0   \n",
       "\n",
       "                          time5   site6                time6   site7  \\\n",
       "session_id                                                             \n",
       "1           2014-10-04 11:19:54  2211.0  2014-10-04 11:19:54  6730.0   \n",
       "2           2014-07-03 11:01:09   782.0  2014-07-03 11:01:10   782.0   \n",
       "3           2014-12-05 15:56:16    55.0  2014-12-05 15:56:17    55.0   \n",
       "4           2014-11-04 10:03:21  3374.0  2014-11-04 10:03:22    50.0   \n",
       "5           2014-05-16 15:05:40    69.0  2014-05-16 15:05:40    70.0   \n",
       "\n",
       "                          time7  site8                time8    site9  \\\n",
       "session_id                                                             \n",
       "1           2014-10-04 11:19:54   21.0  2014-10-04 11:19:54  44582.0   \n",
       "2           2014-07-03 11:01:23  782.0  2014-07-03 11:01:29    782.0   \n",
       "3           2014-12-05 15:56:18   55.0  2014-12-05 15:56:19   1445.0   \n",
       "4           2014-11-04 10:03:22   48.0  2014-11-04 10:03:22     48.0   \n",
       "5           2014-05-16 15:05:40   68.0  2014-05-16 15:05:40     71.0   \n",
       "\n",
       "                          time9   site10               time10  \n",
       "session_id                                                     \n",
       "1           2014-10-04 11:20:00  15336.0  2014-10-04 11:20:00  \n",
       "2           2014-07-03 11:01:30    782.0  2014-07-03 11:01:53  \n",
       "3           2014-12-05 15:56:33   1445.0  2014-12-05 15:56:36  \n",
       "4           2014-11-04 10:03:23   3374.0  2014-11-04 10:03:23  \n",
       "5           2014-05-16 15:05:40    167.0  2014-05-16 15:05:44  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82797 entries, 1 to 82797\n",
      "Data columns (total 20 columns):\n",
      "site1     82797 non-null int64\n",
      "time1     82797 non-null object\n",
      "site2     81308 non-null float64\n",
      "time2     81308 non-null object\n",
      "site3     80075 non-null float64\n",
      "time3     80075 non-null object\n",
      "site4     79182 non-null float64\n",
      "time4     79182 non-null object\n",
      "site5     78341 non-null float64\n",
      "time5     78341 non-null object\n",
      "site6     77566 non-null float64\n",
      "time6     77566 non-null object\n",
      "site7     76840 non-null float64\n",
      "time7     76840 non-null object\n",
      "site8     76151 non-null float64\n",
      "time8     76151 non-null object\n",
      "site9     75484 non-null float64\n",
      "time9     75484 non-null object\n",
      "site10    74806 non-null float64\n",
      "time10    74806 non-null object\n",
      "dtypes: float64(9), int64(1), object(10)\n",
      "memory usage: 13.3+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В обучающей выборке – 2297 сессий одного пользователя (Alice) и 251264 сессий – других пользователей, не Элис. Дисбаланс классов очень сильный, и смотреть на долю верных ответов (accuracy) непоказательно.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    251264\n",
       "1      2297\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пока для прогноза будем использовать только индексы посещенных сайтов. Индексы нумеровались с 1, так что заменим пропуски на нули.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df_sites = train_test_df[['site%d' % i for i in range(1, 11)]].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>941</td>\n",
       "      <td>3847</td>\n",
       "      <td>941</td>\n",
       "      <td>942</td>\n",
       "      <td>3846</td>\n",
       "      <td>3847</td>\n",
       "      <td>3846</td>\n",
       "      <td>1516</td>\n",
       "      <td>1518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>39</td>\n",
       "      <td>14768</td>\n",
       "      <td>14769</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>177</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>175</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>570</td>\n",
       "      <td>21</td>\n",
       "      <td>570</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>803</td>\n",
       "      <td>23</td>\n",
       "      <td>5956</td>\n",
       "      <td>17513</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>803</td>\n",
       "      <td>17514</td>\n",
       "      <td>17514</td>\n",
       "      <td>17514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>5041</td>\n",
       "      <td>14422</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>5041</td>\n",
       "      <td>14421</td>\n",
       "      <td>14421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>668</td>\n",
       "      <td>940</td>\n",
       "      <td>942</td>\n",
       "      <td>941</td>\n",
       "      <td>941</td>\n",
       "      <td>942</td>\n",
       "      <td>940</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3700</td>\n",
       "      <td>229</td>\n",
       "      <td>570</td>\n",
       "      <td>21</td>\n",
       "      <td>229</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>2336</td>\n",
       "      <td>2044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "1             718      0      0      0      0      0      0      0      0   \n",
       "2             890    941   3847    941    942   3846   3847   3846   1516   \n",
       "3           14769     39  14768  14769     37     39  14768  14768  14768   \n",
       "4             782    782    782    782    782    782    782    782    782   \n",
       "5              22    177    175    178    177    178    175    177    177   \n",
       "6             570     21    570     21     21      0      0      0      0   \n",
       "7             803     23   5956  17513     37     21    803  17514  17514   \n",
       "8              22     21     29   5041  14422     23     21   5041  14421   \n",
       "9             668    940    942    941    941    942    940     23     21   \n",
       "10           3700    229    570     21    229     21     21     21   2336   \n",
       "\n",
       "            site10  \n",
       "session_id          \n",
       "1                0  \n",
       "2             1518  \n",
       "3            14768  \n",
       "4              782  \n",
       "5              178  \n",
       "6                0  \n",
       "7            17514  \n",
       "8            14421  \n",
       "9               22  \n",
       "10            2044  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df_sites.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создайте разреженные матрицы *X_train_sparse* и *X_test_sparse* аналогично тому, как мы это делали ранее. Используйте объединенную матрицу *train_test_df_sites*, потом разделите обратно на обучающую и тестовую части.**\n",
    "\n",
    "Обратите внимание на то, что в  сессиях меньше 10 сайтов  у нас остались нули, так что первый признак (сколько раз попался 0) по смыслу отличен от остальных (сколько раз попался сайт с индексом $i$). Поэтому первый столбец разреженной матрицы надо будет удалить.\n",
    "\n",
    "**Выделите в отдельный вектор *y* ответы на обучающей выборке.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_sparse = csr_matrix((np.ones(train_test_df_sites.values.size), \n",
    "                                train_test_df_sites.values.ravel(), \n",
    "                                np.arange(train_test_df_sites.values.shape[0] + 1) * \n",
    "                                train_test_df_sites.values.shape[1]), dtype=int)[:, 1:]\n",
    "\n",
    "X_train_sparse = train_test_sparse[:train_df.shape[0]]\n",
    "X_test_sparse = train_test_sparse[train_df.shape[0]:]\n",
    "y = train_df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Выведите размерности матриц *X_train_sparse* и *X_test_sparse* – 4 числа на одной строке через пробел: число строк и столбцов матрицы *X_train_sparse*, затем число строк и столбцов матрицы *X_test_sparse*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 48371), (82797, 48371))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохраним в pickle-файлы объекты *X_train_sparse*, *X_test_sparse* и *y* (последний – в файл *kaggle_data/train_target.pkl*).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'X_train_sparse.pkl'), 'wb') as X_train_sparse_pkl:\n",
    "    pickle.dump(X_train_sparse, X_train_sparse_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'X_test_sparse.pkl'), 'wb') as X_test_sparse_pkl:\n",
    "    pickle.dump(X_test_sparse, X_test_sparse_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'train_target.pkl'), 'wb') as train_target_pkl:\n",
    "    pickle.dump(y, train_target_pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разобьем обучающую выборку на 2 части в пропорции 7/3, причем не перемешивая. Исходные данные упорядочены по времени, тестовая выборка по времени четко отделена от обучающей, это же соблюдем и здесь.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_share = int(.7 * X_train_sparse.shape[0])\n",
    "X_train, y_train = X_train_sparse[:train_share, :], y[:train_share]\n",
    "X_valid, y_valid  = X_train_sparse[train_share:, :], y[train_share:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Создайте объект `sklearn.linear_model.SGDClassifier` с логистической функцией потерь и параметром *random_state*=17. Остальные параметры оставьте по умолчанию, разве что *n_jobs*=-1 никогда не помешает. Обучите  модель на выборке `(X_train, y_train)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=None, n_iter=None,\n",
       "       n_jobs=-1, penalty='l2', power_t=0.5, random_state=17, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "sgd_logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделайте прогноз в виде предсказанных вероятностей того, что это сессия Элис, на отложенной выборке *(X_valid, y_valid)*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Напечатайте ROC AUC логистической регрессии, обученной с помощью стохастического градиентного спуска, на отложенной выборке. Округлите до 3 знаков после разделителя.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC = 0.934\n"
     ]
    }
   ],
   "source": [
    "print('ROC AUC =', round(roc_auc_score(y_valid, logit_valid_pred_proba[:, 1]), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделайте прогноз в виде предсказанных вероятностей отнесения к классу 1 для тестовой выборки с помощью той же *sgd_logit*, обученной уже на всей обучающей выборке (а не на 70%).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 704 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "logit_test_pred_proba = sgd_logit.fit(X_train_sparse, y).predict_proba(X_test_sparse)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишите ответы в файл и сделайте посылку на Kaggle. Дайте своей команде (из одного человека) на Kaggle говорящее название – по шаблону \"[YDF & MIPT] Coursera_Username\", чтоб можно было легко идентифицировать Вашу посылку на [лидерборде](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/leaderboard/public).**\n",
    "\n",
    "**Результат, который мы только что получили, соответствует бейзлайну \"SGDCLassifer\" на лидерборде, задача на эту неделю – как минимум его побить.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(logit_test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_logit.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Команда на Kaggle - **[YDF & MIPT] Dmitry Shereshevskiy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Критерии оценки работы:\n",
    "- Правильные ли получились размерности матриц в п. 1? (max. 2 балла)\n",
    "- Правильным ли получилось значения ROC AUC в п. 2? (max. 4 балла)\n",
    "- Побит ли бенчмарк \"sgd_logit_benchmark.csv\" на публичной части рейтинга в соревновании Kaggle? (max. 2 балла)\n",
    "- Побит ли бенчмарк \"Logit +3 features\" на публичной части рейтинга в соревновании Kaggle? (max. 2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Пути улучшения\n",
    "На этой неделе дается много времени на соревнование. Не забывайте вносить хорошие идеи, к которым Вы пришли по ходу соревнования, в описание финального проекта (`html`, `pdf` или `ipynb`).\n",
    "Что можно попробовать:\n",
    " - Использовать ранее построенные признаки для улучшения модели (проверить их можно на меньшей выборке по 150 пользователям, отделив одного из пользователей от остальных – это быстрее)\n",
    " - Настроить параметры моделей (например, коэффициенты регуляризации)\n",
    " - Если позволяют мощности (или хватает терпения), можно попробовать смешивание (блендинг) ответов бустинга и линейной модели. [Вот](http://mlwave.com/kaggle-ensembling-guide/) один из самых известных тьюториалов по смешиванию ответов алгоритмов, также хороша [статья](https://alexanderdyakonov.wordpress.com/2017/03/10/cтекинг-stacking-и-блендинг-blending) Александра Дьяконова\n",
    " - Обратите внимание, что в соревновании также даны исходные данные о посещенных веб-страницах Элис и остальными 1557 пользователями (*train.zip*). По этим данным можно сформировать свою обучающую выборку. \n",
    "\n",
    "На 6 неделе мы пройдем большой тьюториал по Vowpal Wabbit и попробуем его в деле, на данных соревнования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа по побитию бенчмарков на Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала посмотрим на словарь, который имеется на Kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site_freq_path=os.path.join(PATH_TO_DATA, 'site_dic.pkl')\n",
    "with open(site_freq_path, 'rb') as pkl_file:\n",
    "            freq_vocabulary = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cdt46.media.tourinsoft.eu': 30911,\n",
       " 'groups.live.com': 13997,\n",
       " 'majeureliguefootball.wordpress.com': 42436,\n",
       " 'www.abmecatronique.com': 25075,\n",
       " 'www.hdwallpapers.eu': 8104}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(freq_vocabulary.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что формат словаря **немного другой** (без частот), примем к сведению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим код для рассчета частотного словаря.    \n",
    "ПРИМЕЧАНИЕ: запустил было этот код, но неожидано о-о-о-очень долго считает (срубил через 20 мин., прогнозно - еще 20 мин., но скорость вычислений падает, похоже - мало памяти)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "freq_vocabulary = defaultdict(int)\n",
    "for row in tqdm_notebook(train_df.values):\n",
    "    for site in row:\n",
    "        freq_vocabulary[site] += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dict(list(freq_vocabulary.items())[:5])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# сохранение этого словаря\n",
    "with open(os.path.join(PATH_TO_DATA, 'site_freq_5week.pkl'), 'wb') as site_freq_5week_pkl:\n",
    "    pickle.dump(freq_vocabulary, site_freq_5week_pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем функцию, которая принимает на вход **train_df**, а выдает **X_train_sparse** с признаками-**сайтами** в виде \"мешка слов\" и новыми признаками (см. коммент в теле функции),    \n",
    "ну и плюс целевой признак **y**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_df(train_df, features, train=True):\n",
    "    '''\n",
    "    features = ['#unique_si tes', 'session_timespan', 'start_hour', 'day_of_week', 'day_of_week', 'weekend']\n",
    "    '''\n",
    "    # повторим, что было выше (частично)\n",
    "    train_df_sites = train_df[['site%d' % i for i in range(1, 11)]] # не забыть заполнить нули\n",
    "    \n",
    "    # подготовим временнЫе метки\n",
    "    train_df[['time%d' % i for i in range(1, 11)]] = train_df[['time%d' % i for i in range(1, 11)]].applymap(pd.to_datetime)\n",
    "    timestamp_df = train_df[['time%d' % i for i in range(1, 11)]].applymap(lambda x: pd.datetime.timestamp(x) \n",
    "                                                                  if type(x) == pd._libs.tslib.Timestamp else np.nan)\n",
    "    # добавляем признаки\n",
    "    if '#unique_sites' in features:\n",
    "        train_df['#unique_sites'] = train_df[['site' + str(i) for i in range(1, 11)]].nunique(axis=1, dropna=True)\n",
    "    if 'session_timespan' in features:\n",
    "        train_df['session_timespan'] = timestamp_df.max(axis=1, skipna=True) - timestamp_df.min(axis=1, skipna=True)\n",
    "    if 'start_hour' in features:    \n",
    "        train_df['start_hour'] = train_df['time1'].apply(lambda x: x.hour)\n",
    "    if 'day_of_week' in features:    \n",
    "        train_df['day_of_week'] = train_df['time1'].apply(lambda x: x.weekday())\n",
    "    if 'time_of_day' in features:    \n",
    "        # время суток (06-12 - утро(1), 12-19 - день(2), 19-24 - вечер(3), 00-06 - ночь(4))\n",
    "        train_df['time_of_day'] = train_df['time1'].apply(lambda x:  1 if x.hour >  6 and x.hour <= 12 else \n",
    "                                                                     2 if x.hour > 12 and x.hour <= 19 else\n",
    "                                                                     3 if x.hour > 19 and x.hour <= 24 else \n",
    "                                                                     4)\n",
    "    if 'weekend' in features:\n",
    "        # индикатор уикэнда в день начала сессии\n",
    "        train_df['weekend'] = train_df['time1'].apply(lambda x: 1 if x.weekday() in [5, 6] else 0)\n",
    "   \n",
    "    train_df_sites = train_df_sites.fillna(0).astype('int')\n",
    "    train_sparse = csr_matrix((np.ones(train_df_sites.values.size), \n",
    "                                train_df_sites.values.ravel(), \n",
    "                                np.arange(train_df_sites.values.shape[0] + 1) * \n",
    "                                train_df_sites.values.shape[1]), dtype=int)[:, 1:]\n",
    "    if train:\n",
    "        y = train_df['target'].values\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    return train_sparse, train_df[features], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logit_roc_auc_score(X_train_sparse, y):\n",
    "    '''\n",
    "    обучение и расчет метрики на отложенной выборке\n",
    "    '''\n",
    "\n",
    "    train_share = int(.7 * X_train_sparse.shape[0])\n",
    "    X_train, y_train = X_train_sparse[:train_share, :], y[:train_share]\n",
    "    X_valid, y_valid  = X_train_sparse[train_share:, :], y[train_share:]\n",
    "\n",
    "    sgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "    sgd_logit.fit(X_train, y_train)\n",
    "\n",
    "    logit_valid_pred_proba = sgd_logit.predict_proba(X_valid)\n",
    "\n",
    "    return roc_auc_score(y_valid, logit_valid_pred_proba[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохраним в pickle-файлы объекты X_train_sparse.**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'X_train_new_sparse.pkl'), 'wb') as X_train_sparse_pkl:\n",
    "    pickle.dump(X_train_new_sparse, X_train_sparse_pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим **все** комбинации добавления признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 лучших метрик с фичами\n",
      "roc_auc_score: 0.934381940306 features: ['weekend']\n",
      "roc_auc_score: 0.933847286791 features: []\n",
      "roc_auc_score: 0.924980978083 features: ['day_of_week', 'weekend']\n",
      "roc_auc_score: 0.924717063844 features: ['day_of_week']\n",
      "roc_auc_score: 0.923196690838 features: ['#unique_sites', 'weekend']\n",
      "\n",
      "5 худших метрик с фичами\n",
      "roc_auc_score: 0.674627312681 features: ['session_timespan', 'day_of_week', 'weekend']\n",
      "roc_auc_score: 0.668984875716 features: ['session_timespan', 'weekend']\n",
      "roc_auc_score: 0.66837137001 features: ['session_timespan', 'time_of_day', 'weekend']\n",
      "roc_auc_score: 0.666589603811 features: ['session_timespan', 'time_of_day']\n",
      "roc_auc_score: 0.665770081657 features: ['session_timespan']\n"
     ]
    }
   ],
   "source": [
    "features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "train_sites_sparse, features_df, y_values = feature_engineering_df(train_df, features, train=True)\n",
    "\n",
    "feature_scores = []\n",
    "for r in range(len(features) + 1):\n",
    "    for fe in itertools.combinations(features, r):\n",
    "        fe_ = list(fe)\n",
    "        features_sparse = csr_matrix(features_df[fe_].values)\n",
    "  \n",
    "        X_train_sparse = sparse.hstack([train_sites_sparse , features_sparse]).tocsr()\n",
    "        \n",
    "        logit_roc_auc = logit_roc_auc_score(X_train_sparse, y_values)\n",
    "        feature_scores.append((logit_roc_auc, fe_))\n",
    "\n",
    "sorted_features = sorted(feature_scores, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print('5 лучших метрик с фичами')\n",
    "for pair in sorted_features[:5]:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])\n",
    "\n",
    "print('\\n5 худших метрик с фичами')\n",
    "for pair in sorted_features[-5:]:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫВОД:**    \n",
    "Результат немного странный (если, конечно, не закралась системная ошибка), но тем не менее -   \n",
    "только добавление **одного** из всех признаков - а именно **'weekend'** - дало **увеличение** метрики **ROC AUC**.\n",
    "Остальные признаки только лишь **уменьшили** метрику **ROC AUC**.   \n",
    "Причем особенно **негативно** влияет присутствие признака **'session_timespan'**.   \n",
    "#### ИТАК, добавляем только **'weekend'**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим  **'weekend'**, подготовим данные и сформируем посылку на **Kaggle**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_sites_sparse, features_df, y_train = feature_engineering_df(train_test_df, features=features, train=True)\n",
    "features_sparse = csr_matrix(features_df[['weekend']].values)\n",
    "X_train_test_sparse_newfe = sparse.hstack([train_sites_sparse , features_sparse]).tocsr()\n",
    "\n",
    "# делим на трейн и тест\n",
    "X_train_sparse_newfe = X_train_test_sparse_newfe[:train_df.shape[0]]\n",
    "X_test_sparse_newfe = X_train_test_sparse_newfe[train_df.shape[0]:]\n",
    "y_train = y_train[:train_df.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "logit_test_pred_proba = sgd_logit.fit(X_train_sparse_newfe, y_train).predict_proba(X_test_sparse_newfe)[:, 1]\n",
    "    \n",
    "write_to_submission_file(logit_test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_logit_newfe.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем соответствующую **посылку на Kaggle**.   \n",
    "Её результат:   \n",
    "- **+46** позиций вверх    \n",
    "- Score Public Leaderboard = **0.92600**   \n",
    "- Бенчмарк \"sgd_logit_benchmark.csv\" на публичной части рейтинга в соревновании Kaggle - **побит**, программа *минимум* - **выполнена**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что далее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Длиной сессии** и **шириной окна** и  поиграть, я так понимаю, **не получится**, поскольку в **тестовой** выборке **жестко задано** (10, 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поработаем с настройками SGDClassifier:\n",
    "- альфа\n",
    "- балансировка классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации временн**ы**х затрат поработаем с выборкой на 150 пользователей.\n",
    "\n",
    "Загрузим сохраненные ранее данные и сформируем выборку для работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'selected_features_150users.pkl'), 'rb') as selected_features_150users_pkl:\n",
    "    selected_features_150users = pickle.load(selected_features_150users_pkl)\n",
    "    \n",
    "with open(os.path.join(PATH_TO_DATA, 'new_features_150users.pkl'), 'rb') as new_features_150users_pkl:\n",
    "    new_features_150users = pickle.load(new_features_150users_pkl)\n",
    "    \n",
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_150users.pkl'), 'rb') as X_sparse_150users_pkl:\n",
    "    X_sparse_150users = pickle.load(X_sparse_150users_pkl)\n",
    "    \n",
    "with open(os.path.join(PATH_TO_DATA, 'y_150users.pkl'), 'rb') as y_150users_pkl:\n",
    "    y_150users = pickle.load(y_150users_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_year</th>\n",
       "      <th>start_month</th>\n",
       "      <th>start_day</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>freq_top30_0_1</th>\n",
       "      <th>freq_top30_1_3</th>\n",
       "      <th>freq_top30_2_41</th>\n",
       "      <th>freq_top30_3_2</th>\n",
       "      <th>freq_top30_4_181</th>\n",
       "      <th>freq_top30_5_214</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_top30_21_1201</th>\n",
       "      <th>freq_top30_22_6</th>\n",
       "      <th>freq_top30_23_2165</th>\n",
       "      <th>freq_top30_24_244</th>\n",
       "      <th>freq_top30_25_180</th>\n",
       "      <th>freq_top30_26_1463</th>\n",
       "      <th>freq_top30_27_106</th>\n",
       "      <th>freq_top30_28_253</th>\n",
       "      <th>freq_top30_29_2707</th>\n",
       "      <th>weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_year  start_month  start_day  time_of_day  freq_top30_0_1  \\\n",
       "0        2013           11         15            1               4   \n",
       "1        2013           11         15            1               0   \n",
       "2        2013           11         15            1               5   \n",
       "3        2013           11         15            1               4   \n",
       "4        2013           11         15            1               3   \n",
       "\n",
       "   freq_top30_1_3  freq_top30_2_41  freq_top30_3_2  freq_top30_4_181  \\\n",
       "0               2                0               2                 0   \n",
       "1               1                0               0                 0   \n",
       "2               1                0               0                 0   \n",
       "3               0                0               0                 0   \n",
       "4               0                0               0                 0   \n",
       "\n",
       "   freq_top30_5_214   ...     freq_top30_21_1201  freq_top30_22_6  \\\n",
       "0                 0   ...                      0                0   \n",
       "1                 0   ...                      0                1   \n",
       "2                 0   ...                      0                0   \n",
       "3                 0   ...                      0                0   \n",
       "4                 0   ...                      0                0   \n",
       "\n",
       "   freq_top30_23_2165  freq_top30_24_244  freq_top30_25_180  \\\n",
       "0                   0                  0                  0   \n",
       "1                   0                  0                  0   \n",
       "2                   0                  0                  0   \n",
       "3                   0                  0                  0   \n",
       "4                   0                  0                  0   \n",
       "\n",
       "   freq_top30_26_1463  freq_top30_27_106  freq_top30_28_253  \\\n",
       "0                   0                  0                  0   \n",
       "1                   0                  0                  0   \n",
       "2                   0                  0                  0   \n",
       "3                   0                  0                  0   \n",
       "4                   0                  0                  0   \n",
       "\n",
       "   freq_top30_29_2707  weekend  \n",
       "0                   0        0  \n",
       "1                   0        0  \n",
       "2                   0        0  \n",
       "3                   0        0  \n",
       "4                   0        0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_150users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((137019, 35), (137019,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_150users.shape, y_150users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<137019x27797 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 1369510 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sparse_150users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вырежем нужные новые признаки ('weekend') и добавим к \"мешку слов\" с сайтами. Скорректируем также целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_150_newfe_sparse = csr_matrix(selected_features_150users[['weekend']].values)\n",
    "  \n",
    "X_150_train_newfe_sparse = sparse.hstack([X_sparse_150users, X_150_newfe_sparse]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_150_train = (y_150users == 128).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подбора параметров воспльзуемся **GridSearchCV**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 58.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "\n",
    "sgd_params = {'alpha': np.logspace(-8, 1, 40)}\n",
    "\n",
    "sgd_grid_searcher = GridSearchCV(sgd_logit, param_grid=sgd_params, scoring='roc_auc', n_jobs=-1, cv=skf)\n",
    "sgd_grid_searcher.fit(X_150_train_newfe_sparse, y_150_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 4.9238826317067415e-05} 0.971957399925\n"
     ]
    }
   ],
   "source": [
    "print(sgd_grid_searcher.best_params_, sgd_grid_searcher.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем то же, но **поменяем балансировку классов** в sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 38.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "\n",
    "sgd_params = {'alpha': np.logspace(-8, 1, 40)}\n",
    "\n",
    "sgd_grid_searcher = GridSearchCV(sgd_logit, param_grid=sgd_params, scoring='roc_auc', n_jobs=-1, cv=skf)\n",
    "sgd_grid_searcher.fit(X_150_train_newfe_sparse, y_150_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.00041246263829013564} 0.977036274013\n"
     ]
    }
   ],
   "source": [
    "print(sgd_grid_searcher.best_params_, sgd_grid_searcher.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВЫВОД:    \n",
    "результат **улучшился**.    \n",
    "Обучим на этих параметрах классификатор и сформируем посылку на **Kaggle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train_sites_sparse, features_df, y_train = feature_engineering_df(train_test_df, features=features, train=True)\n",
    "# features_sparse = csr_matrix(features_df[['weekend']].values)\n",
    "# X_train_test_sparse_newfe = sparse.hstack([train_sites_sparse , features_sparse]).tocsr()\n",
    "\n",
    "# делим на трейн и тест\n",
    "X_train_sparse_newfe = X_train_test_sparse_newfe[:train_df.shape[0]]\n",
    "X_test_sparse_newfe = X_train_test_sparse_newfe[train_df.shape[0]:]\n",
    "y_train = y_train[:train_df.shape[0]]\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, alpha=sgd_grid_searcher.best_params_['alpha'], \n",
    "                          class_weight='balanced', random_state=17)\n",
    "logit_test_pred_proba = sgd_logit.fit(X_train_sparse_newfe, y_train).predict_proba(X_test_sparse_newfe)[:, 1]\n",
    "    \n",
    "write_to_submission_file(logit_test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_logit_newfe_alpha_bal.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При таких параметрах **на Leader Board** auc_roc больше не стал, наоборот - **уменьшился**: **0.92275** (было 0.92600).    \n",
    "Будем подбирать дальше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, \n",
    "                          class_weight='balanced', random_state=17)\n",
    "logit_test_pred_proba = sgd_logit.fit(X_train_sparse_newfe, y_train).predict_proba(X_test_sparse_newfe)[:, 1]\n",
    "    \n",
    "write_to_submission_file(logit_test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_logit_newfe_bal.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score on Leader Board: **0.91911** - **уменьшился**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Поработаем еще с фичами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CV_score(estimator, X_train_sparse, y, cv=None, random_state=17, test_size=0.3):\n",
    "    '''\n",
    "    обучение и расчет метрики на отложенной выборке\n",
    "    '''\n",
    "    # Разобьем выборку на 2 части. Учтем, что разбиение выборки с train_test_split должно быть стратифицированным\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_sparse, y, \n",
    "                                                          test_size=test_size, \n",
    "                                                          random_state=random_state, stratify=y)\n",
    "    mean_CV_score = cross_val_score(estimator, X_train, y_train, cv=cv, scoring='roc_auc', n_jobs=-1).mean()\n",
    " \n",
    "    return mean_CV_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 лучших метрик с фичами\n",
      "roc_auc_score: 0.975041438505 features: ['#unique_sites', 'time_of_day', 'weekend']\n",
      "roc_auc_score: 0.974795606899 features: ['#unique_sites', 'day_of_week', 'time_of_day', 'weekend']\n",
      "roc_auc_score: 0.974109871176 features: ['day_of_week', 'time_of_day', 'weekend']\n",
      "roc_auc_score: 0.973203642856 features: ['#unique_sites', 'weekend']\n",
      "roc_auc_score: 0.972939022961 features: ['day_of_week', 'weekend']\n",
      "/n5 худших метрик с фичами\n",
      "roc_auc_score: 0.595232749661 features: ['#unique_sites', 'session_timespan', 'time_of_day', 'weekend']\n",
      "roc_auc_score: 0.592947564512 features: ['#unique_sites', 'session_timespan', 'time_of_day']\n",
      "roc_auc_score: 0.59078323166 features: ['#unique_sites', 'session_timespan', 'weekend']\n",
      "roc_auc_score: 0.588683480401 features: ['#unique_sites', 'session_timespan']\n",
      "roc_auc_score: 0.515455859856 features: ['session_timespan', 'weekend']\n",
      "Wall time: 13min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "feature_scores = []\n",
    "features_150users = pd.concat([new_features_150users, selected_features_150users], axis=1)\n",
    "for r in range(len(features) + 1):\n",
    "    for fe in itertools.combinations(features, r):\n",
    "        fe_ = list(fe)\n",
    "        X_150_new_sparse = csr_matrix(features_150users[fe_].values)\n",
    "  \n",
    "        X_150_train_new_sparse = sparse.hstack([X_sparse_150users, X_150_new_sparse]).tocsr()\n",
    "        \n",
    "        estimator = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "        roc_auc_score = CV_score(estimator, X_150_train_new_sparse, y_150_train, cv=skf, \n",
    "                                 random_state=17, test_size=0.3)\n",
    "        feature_scores.append((roc_auc_score, fe_))\n",
    "\n",
    "sorted_features = sorted(feature_scores, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print('5 лучших метрик с фичами')\n",
    "for pair in sorted_features[:5]:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])\n",
    "\n",
    "print('\\n5 худших метрик с фичами')\n",
    "for pair in sorted_features[-5:]:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Далее будем учитывать обстоятельство, что большинство дополнительных фич у нас по сути КАТЕГОРИАЛЬНЫЕ, а учитываются они пока КАК КОЛИЧЕСТВЕННЫЕ (то есть с отношением порядка)\n",
    "\n",
    "В частности, **становится понятным**, почему ранее \"выстрелила\" только **одна** из всех фич - **'weekend'**. Эта фича имеет **только два** значения (т.е. *бинарна*), что позволяет корректно учитывать ее в регрессии.   \n",
    "Для корректности остальные категориальные фичи надо тоже **бинаризовать**. Более правильно - это сделать **'one hot coding'**, или (что пока проще и позволяет природа этих признаков) - **бинаризовать по некоему порогу**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Порог** можно определить \"на глаз\", или взять как среднее арифметическое (для бинарной классификации) медиан при группировке по целевому признаку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В соответствии с этими соображениями модифицируем функцию для подготовки признаков для бинаризации **по порогу**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_df_bin(train_df, features=None, train=True):\n",
    "    '''\n",
    "    features = ['#unique_sites_bin', 'session_timespan_bin', 'time_of_day', 'weekend']\n",
    "    '''\n",
    "    \n",
    "    if features == None:\n",
    "        features_ = ['#unique_sites_bin', 'session_timespan_bin', 'time_of_day', 'weekend']\n",
    "    else:\n",
    "        features_ = features\n",
    "    \n",
    "    # готовим сайты\n",
    "    train_df_sites = train_df[['site%d' % i for i in range(1, 11)]] # наны заполним нулями потом\n",
    "    \n",
    "    # подготовим временнЫе метки\n",
    "    train_df[['time%d' % i for i in range(1, 11)]] = train_df[['time%d' % i for i in range(1, 11)]].applymap(pd.to_datetime)\n",
    "    timestamp_df = train_df[['time%d' % i for i in range(1, 11)]].applymap(lambda x: pd.datetime.timestamp(x) \n",
    "                                                                  if type(x) == pd._libs.tslib.Timestamp else np.nan)\n",
    "    # добавляем признаки\n",
    "    if '#unique_sites_bin' in features_:\n",
    "        train_df['#unique_sites_bin'] = (train_df[['site' + str(i) for i in range(1, 11)]].nunique(axis=1, dropna=True)). \\\n",
    "                                         apply(lambda x: 1 if x < 6 else 0)\n",
    "    if 'session_timespan_bin' in features_:\n",
    "        train_df['session_timespan_bin'] = (timestamp_df.max(axis=1, skipna=True) - timestamp_df.min(axis=1, skipna=True)). \\\n",
    "                                            apply(lambda x: 1 if x < 50 else 0)\n",
    "#     if 'start_hour' in features:    \n",
    "#         train_df['start_hour'] = train_df['time1'].apply(lambda x: x.hour)\n",
    "#     if 'day_of_week' in features:    \n",
    "#         train_df['day_of_week'] = train_df['time1'].apply(lambda x: x.weekday())\n",
    "    if 'time_of_day' in features_:    \n",
    "        # до обеда - после обеда\n",
    "        train_df['time_of_day'] = train_df['time1'].apply(lambda x:  1 if x.hour <  14 else 0)\n",
    "    if 'weekend' in features_:\n",
    "        # индикатор уикэнда в день начала сессии\n",
    "        train_df['weekend'] = train_df['time1'].apply(lambda x: 1 if x.weekday() in [5, 6] else 0)\n",
    " \n",
    "    train_df_sites = train_df_sites.fillna(0).astype('int')\n",
    "    train_sparse = csr_matrix((np.ones(train_df_sites.values.size), \n",
    "                                train_df_sites.values.ravel(), \n",
    "                                np.arange(train_df_sites.values.shape[0] + 1) * \n",
    "                                train_df_sites.values.shape[1]), dtype=int)[:, 1:]\n",
    "    if train:\n",
    "        y = train_df['target'].values\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    return train_sparse, train_df[features_], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_sites_sparse, selected_features_df, y_values = feature_engineering_df_bin(train_df, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Без баланса\n",
      "\n",
      "5 лучших метрик с фичами\n",
      "roc_auc_score: 0.934218622552 features: ['weekend']\n",
      "roc_auc_score: 0.933635284594 features: ['time_of_day', 'weekend']\n",
      "roc_auc_score: 0.932279637877 features: []\n",
      "roc_auc_score: 0.930773100919 features: ['time_of_day']\n",
      "roc_auc_score: 0.929718057263 features: ['#unique_sites_bin', 'time_of_day', 'weekend']\n",
      "\n",
      "5 худших метрик с фичами\n",
      "roc_auc_score: 0.923128645975 features: ['session_timespan_bin', 'weekend']\n",
      "roc_auc_score: 0.920592024569 features: ['#unique_sites_bin', 'session_timespan_bin', 'time_of_day']\n",
      "roc_auc_score: 0.920483235092 features: ['session_timespan_bin']\n",
      "roc_auc_score: 0.915035943593 features: ['#unique_sites_bin', 'session_timespan_bin', 'weekend']\n",
      "roc_auc_score: 0.910999149669 features: ['#unique_sites_bin', 'session_timespan_bin']\n",
      "Wall time: 3min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "features = selected_features_df.columns\n",
    "feature_scores = []\n",
    "for r in range(len(features) + 1):\n",
    "    for fe in itertools.combinations(features, r):\n",
    "        fe_ = list(fe)\n",
    "        selected_features_sparse = csr_matrix(selected_features_df[fe_].values)\n",
    "  \n",
    "        X_train_new_sparse = sparse.hstack([train_sites_sparse , selected_features_sparse]).tocsr()\n",
    "        \n",
    "        estimator = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "        roc_auc_score = CV_score(estimator, X_train_new_sparse, y_values, cv=skf, \n",
    "                                 random_state=17, test_size=0.3)\n",
    "        feature_scores.append((roc_auc_score, fe_))\n",
    "\n",
    "sorted_features = sorted(feature_scores, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print('Без баланса')\n",
    "print('\\n5 лучших метрик с фичами')\n",
    "for pair in sorted_features[:5]:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])\n",
    "\n",
    "print('\\n5 худших метрик с фичами')\n",
    "for pair in sorted_features[-5:]:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проделаем то же самое, делая баланс в обучающей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "С балансом\n",
      "\n",
      "5 лучших метрик с фичами\n",
      "roc_auc_score: 0.938555892249 features: ['#unique_sites_bin', 'session_timespan_bin', 'time_of_day', 'weekend']\n",
      "roc_auc_score: 0.936702678996 features: ['#unique_sites_bin', 'session_timespan_bin', 'time_of_day']\n",
      "roc_auc_score: 0.936023752468 features: ['#unique_sites_bin', 'session_timespan_bin', 'weekend']\n",
      "roc_auc_score: 0.935668706936 features: ['session_timespan_bin', 'time_of_day', 'weekend']\n",
      "roc_auc_score: 0.934530145929 features: ['#unique_sites_bin', 'session_timespan_bin']\n",
      "\n",
      "5 худших метрик с фичами\n",
      "roc_auc_score: 0.930224205183 features: ['weekend']\n",
      "roc_auc_score: 0.930118730518 features: ['session_timespan_bin']\n",
      "roc_auc_score: 0.929995398298 features: ['#unique_sites_bin']\n",
      "roc_auc_score: 0.929944599468 features: ['time_of_day']\n",
      "roc_auc_score: 0.926259578693 features: []\n",
      "Wall time: 3min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "features = selected_features_df.columns\n",
    "feature_scores = []\n",
    "for r in range(len(features) + 1):\n",
    "    for fe in itertools.combinations(features, r):\n",
    "        fe_ = list(fe)\n",
    "        selected_features_sparse = csr_matrix(selected_features_df[fe_].values)\n",
    "  \n",
    "        X_train_new_sparse = sparse.hstack([train_sites_sparse , selected_features_sparse]).tocsr()\n",
    "        \n",
    "        estimator = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "        roc_auc_score = CV_score(estimator, X_train_new_sparse, y_values, cv=skf, \n",
    "                                 random_state=17, test_size=0.3)\n",
    "        feature_scores.append((roc_auc_score, fe_))\n",
    "\n",
    "sorted_features = sorted(feature_scores, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print('С балансом')\n",
    "print('\\n5 лучших метрик с фичами')\n",
    "for pair in sorted_features[:5]:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])\n",
    "\n",
    "print('\\n5 худших метрик с фичами')\n",
    "for pair in sorted_features[-5:]:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем комбинацию лучших фич и на ее основе сформируем посылку на **Kaggle**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#unique_sites_bin', 'session_timespan_bin', 'time_of_day', 'weekend']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_features[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = ['#unique_sites_bin', 'session_timespan_bin', 'time_of_day', 'weekend']\n",
    "train_sites_sparse, selected_features_df, y_values\n",
    "X_train_test_sites_sparse, selected_train_test_features_df, y_train_none = \\\n",
    "                                                        feature_engineering_df_bin(train_test_df, features=features, train=True)\n",
    "# делим на трейн и тест\n",
    "selected_train_test_features_sparse = csr_matrix(selected_train_test_features_df[features].values)\n",
    "  \n",
    "X_train_test_newfe_sparse = sparse.hstack([X_train_test_sites_sparse , selected_train_test_features_sparse]).tocsr()\n",
    "X_train_newfe_sparse = X_train_test_newfe_sparse[:train_df.shape[0]]\n",
    "X_test_newfe_sparse = X_train_test_newfe_sparse[train_df.shape[0]:]\n",
    "y_train = y_train_none[:train_df.shape[0]]\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "logit_test_pred_proba = sgd_logit.fit(X_train_newfe_sparse, y_train).predict_proba(X_test_newfe_sparse)[:, 1]\n",
    "    \n",
    "write_to_submission_file(logit_test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_logit_4features_bal.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат \"так себе\" - **0.90482**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = ['time_of_day', 'weekend']\n",
    "train_sites_sparse, selected_features_df, y_values\n",
    "X_train_test_sites_sparse, selected_train_test_features_df, y_train_none = \\\n",
    "                                                        feature_engineering_df_bin(train_test_df, features=features, train=True)\n",
    "# делим на трейн и тест\n",
    "selected_train_test_features_sparse = csr_matrix(selected_train_test_features_df[features].values)\n",
    "  \n",
    "X_train_test_newfe_sparse = sparse.hstack([X_train_test_sites_sparse , selected_train_test_features_sparse]).tocsr()\n",
    "X_train_newfe_sparse = X_train_test_newfe_sparse[:train_df.shape[0]]\n",
    "X_test_newfe_sparse = X_train_test_newfe_sparse[train_df.shape[0]:]\n",
    "y_train = y_train_none[:train_df.shape[0]]\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "logit_test_pred_proba = sgd_logit.fit(X_train_newfe_sparse, y_train).predict_proba(X_test_newfe_sparse)[:, 1]\n",
    "    \n",
    "write_to_submission_file(logit_test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_logit_2features.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще хуже - **0.87852**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: \n",
      "params = {'alpha': 1.7012542798525893e-05}   roc_auc = 0.951589553787\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# готовим признаки\n",
    "features = ['#unique_sites_bin', 'session_timespan_bin', 'time_of_day', 'weekend']\n",
    "train_sites_sparse, selected_features_df, y_values\n",
    "X_train_test_sites_sparse, selected_train_test_features_df, y_train_none = \\\n",
    "                                                        feature_engineering_df_bin(train_test_df, features=features, train=True)\n",
    "###### делим на трейн и тест\n",
    "selected_train_test_features_sparse = csr_matrix(selected_train_test_features_df[features].values)\n",
    "  \n",
    "X_train_test_newfe_sparse = sparse.hstack([X_train_test_sites_sparse , selected_train_test_features_sparse]).tocsr()\n",
    "X_train_newfe_sparse = X_train_test_newfe_sparse[:train_df.shape[0]]\n",
    "X_test_newfe_sparse = X_train_test_newfe_sparse[train_df.shape[0]:]\n",
    "y_train = y_train_none[:train_df.shape[0]]\n",
    "\n",
    "# подбор альфа\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "\n",
    "sgd_params = {'alpha': np.logspace(-8, 1, 40)}\n",
    "\n",
    "sgd_grid_searcher = GridSearchCV(sgd_logit, param_grid=sgd_params, scoring='roc_auc', n_jobs=-1, cv=skf)\n",
    "sgd_grid_searcher.fit(X_train_newfe_sparse, y_train)\n",
    "\n",
    "print('best:','\\nparams =', sgd_grid_searcher.best_params_, '  roc_auc =', sgd_grid_searcher.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: \n",
      "params = {'alpha': 0.00041246263829013564}   roc_auc = 0.952625453824\n",
      "Wall time: 49.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# подбор альфа (balanced)\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "\n",
    "sgd_params = {'alpha': np.logspace(-8, 1, 40)}\n",
    "\n",
    "sgd_grid_searcher = GridSearchCV(sgd_logit, param_grid=sgd_params, scoring='roc_auc', n_jobs=-1, cv=skf)\n",
    "sgd_grid_searcher.fit(X_train_newfe_sparse, y_train)\n",
    "\n",
    "print('best:','\\nparams =', sgd_grid_searcher.best_params_, '  roc_auc =', sgd_grid_searcher.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# features = ['#unique_sites_bin', 'session_timespan_bin', 'time_of_day', 'weekend']\n",
    "# train_sites_sparse, selected_features_df, y_values\n",
    "# X_train_test_sites_sparse, selected_train_test_features_df, y_train_none = \\\n",
    "#                                                         feature_engineering_df_bin(train_test_df, features=features, train=True)\n",
    "# # делим на трейн и тест\n",
    "# selected_train_test_features_sparse = csr_matrix(selected_train_test_features_df[features].values)\n",
    "  \n",
    "# X_train_test_newfe_sparse = sparse.hstack([X_train_test_sites_sparse , selected_train_test_features_sparse]).tocsr()\n",
    "# X_train_newfe_sparse = X_train_test_newfe_sparse[:train_df.shape[0]]\n",
    "# X_test_newfe_sparse = X_train_test_newfe_sparse[train_df.shape[0]:]\n",
    "# y_train = y_train_none[:train_df.shape[0]]\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, alpha=sgd_grid_searcher.best_params_['alpha'], \n",
    "                          class_weight='balanced', random_state=17)\n",
    "logit_test_pred_proba = sgd_logit.fit(X_train_newfe_sparse, y_train).predict_proba(X_test_newfe_sparse)[:, 1]\n",
    "    \n",
    "write_to_submission_file(logit_test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_logit_4features_bal_alpha.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.90482**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### пробуем CatBoostClassifier\n",
    "```python\n",
    "%%time\n",
    "# пробуем CatBoostClassifier\n",
    "import catboost\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "features = ['time_of_day', 'weekend']\n",
    "feature_scores = []\n",
    "for r in range(len(features) + 1):\n",
    "    for fe in itertools.combinations(features, r):\n",
    "        fe_ = list(fe)\n",
    "        selected_features_sparse = csr_matrix(selected_features_df[fe_].values)\n",
    "  \n",
    "        X_train_new_sparse = sparse.hstack([train_sites_sparse , selected_features_sparse]).tocsr()\n",
    "        \n",
    "        estimator = catboost.CatBoostClassifier()\n",
    "        roc_auc_score = CV_score(estimator, X_train_new_sparse.toarray(), y_values, cv=skf, \n",
    "                                 score='roc_auc', random_state=17, test_size=0.3)\n",
    "        feature_scores.append((roc_auc_score, fe_))\n",
    "\n",
    "sorted_features = sorted(feature_scores, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# print('С балансом')\n",
    "# print('\\n5 лучших метрик с фичами')\n",
    "# for pair in sorted_features[:5]:\n",
    "#     print('roc_auc_score:', pair[0], 'features:', pair[1])\n",
    "\n",
    "# print('\\n5 худших метрик с фичами')\n",
    "# for pair in sorted_features[-5:]:\n",
    "#     print('roc_auc_score:', pair[0], 'features:', pair[1])\n",
    "for pair in sorted_features:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])\n",
    "    \n",
    "```\n",
    "**РЕЗЮМЕ:**    \n",
    "Catboost **не поддерживае**т работу с **sparse**-форматом, поэтому пока не подходит для этой задачи (а перевод в np.array приводит к **ошибке памяти**, скорее всего - памяти для этого просто не хватает)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пробуем XGBOOST, поскольку он умеет работать со sparse-форматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.904199430799 features: ['time_of_day', 'weekend']\n",
      "roc_auc_score: 0.899112212739 features: ['time_of_day']\n",
      "roc_auc_score: 0.890882452164 features: ['weekend']\n",
      "roc_auc_score: 0.884568244521 features: []\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# пробуем xgboost\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "features = ['time_of_day', 'weekend']\n",
    "feature_scores = []\n",
    "for r in range(len(features) + 1):\n",
    "    for fe in itertools.combinations(features, r):\n",
    "        fe_ = list(fe)\n",
    "        selected_features_sparse = csr_matrix(selected_features_df[fe_].values)\n",
    "  \n",
    "        X_train_new_sparse = sparse.hstack([train_sites_sparse , selected_features_sparse]).tocsr()\n",
    "        \n",
    "        estimator = xgb.XGBClassifier(n_jobs=-1, random_state=17)\n",
    "        roc_auc_score = CV_score(estimator, X_train_new_sparse, y_values, cv=skf, \n",
    "                                 random_state=17, test_size=0.3)\n",
    "        feature_scores.append((roc_auc_score, fe_))\n",
    "\n",
    "sorted_features = sorted(feature_scores, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# print('С балансом')\n",
    "# print('\\n5 лучших метрик с фичами')\n",
    "# for pair in sorted_features[:5]:\n",
    "#     print('roc_auc_score:', pair[0], 'features:', pair[1])\n",
    "\n",
    "# print('\\n5 худших метрик с фичами')\n",
    "# for pair in sorted_features[-5:]:\n",
    "#     print('roc_auc_score:', pair[0], 'features:', pair[1])\n",
    "for pair in sorted_features:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модифицируем **feature_engineering_df**, \"причесав\" **session_timespan**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_df_mod(train_df, test_df=None, features=None):\n",
    "    '''\n",
    "    features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    '''\n",
    "    if features is None:\n",
    "        features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    \n",
    "    if test_df is None:\n",
    "        df = train_df\n",
    "    else:\n",
    "        df = pd.concat([train_df, test_df])\n",
    "    \n",
    "    # выделим отдельно сайты\n",
    "    df_sites = df[['site%d' % i for i in range(1, 11)]] # не забыть заполнить нули\n",
    "    \n",
    "    # подготовим временнЫе метки\n",
    "    df[['time%d' % i for i in range(1, 11)]] = df[['time%d' % i for i in range(1, 11)]].applymap(pd.to_datetime)\n",
    "    timestamp_df = df[['time%d' % i for i in range(1, 11)]].applymap(lambda x: pd.datetime.timestamp(x) \n",
    "                                                                  if type(x) == pd._libs.tslib.Timestamp else np.nan)\n",
    "    # добавляем признаки\n",
    "    if '#unique_sites' in features:\n",
    "        df['#unique_sites'] = df[['site' + str(i) for i in range(1, 11)]].nunique(axis=1, dropna=True)\n",
    "    if 'session_timespan' in features:\n",
    "        df['session_timespan'] = timestamp_df.max(axis=1, skipna=True) - timestamp_df.min(axis=1, skipna=True)\n",
    "        df['session_timespan'] = df['session_timespan'].apply(lambda x: x // 30 if x < 210 else 7)\n",
    "    if 'start_hour' in features:    \n",
    "        df['start_hour'] = df['time1'].apply(lambda x: x.hour)\n",
    "    if 'day_of_week' in features:    \n",
    "        df['day_of_week'] = df['time1'].apply(lambda x: x.weekday())\n",
    "    if 'time_of_day' in features:    \n",
    "        # время суток (06-12 - утро(1), 12-19 - день(2), 19-24 - вечер(3), 00-06 - ночь(4))\n",
    "        df['time_of_day'] = df['time1'].apply(lambda x:  1 if x.hour >  6 and x.hour <= 12 else \n",
    "                                                                     2 if x.hour > 12 and x.hour <= 19 else\n",
    "                                                                     3 if x.hour > 19 and x.hour <= 24 else \n",
    "                                                                     4)\n",
    "    if 'weekend' in features:\n",
    "        # индикатор уикэнда в день начала сессии\n",
    "        df['weekend'] = df['time1'].apply(lambda x: 1 if x.weekday() in [5, 6] else 0)\n",
    "    \n",
    "    X_new_sparse = csr_matrix(df[features].values)\n",
    "    \n",
    "    df_sites = df_sites.fillna(0).astype('int')\n",
    "    train_sparse = csr_matrix((np.ones(df_sites.values.size), \n",
    "                                df_sites.values.ravel(), \n",
    "                                np.arange(df_sites.values.shape[0] + 1) * \n",
    "                                df_sites.values.shape[1]), dtype=int)[:, 1:]\n",
    "    \n",
    "    \n",
    "    y_train = df['target'].values[:train_df.shape[0]]\n",
    "        \n",
    "    X_sparse = sparse.hstack([train_sparse, X_new_sparse]).tocsr()\n",
    "    X_train_sparse = X_sparse[:train_df.shape[0]]\n",
    "    X_test_sparse = X_sparse[train_df.shape[0]:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (X_train_sparse, y_train) if test_df is None else (X_train_sparse, y_train, X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# готовим данные\n",
    "X_train_new_sparse, y_train = feature_engineering_df_mod(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# инициализируем параметры\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.5,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 170\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98481937043309076"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = xgb.XGBClassifier(n_jobs=-1, random_state=17, **params)\n",
    "roc_auc_score = CV_score(estimator, X_train_new_sparse, y_train, \n",
    "                         random_state=17, test_size=0.3)\n",
    "roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**roc_auc** получился довольно приличный, подготовим и сделаем на этих параметрах посылку на **Kaggle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_sparse, y_train, X_test_sparse = feature_engineering_df_mod(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.5,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 170\n",
    "}\n",
    "\n",
    "\n",
    "estimator = xgb.XGBClassifier(n_jobs=-1, random_state=17, **params)\n",
    "test_pred_proba = estimator.fit(X_train_sparse, y_train).predict_proba(X_test_sparse)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_xgb_allfeatures_1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.90088**     \n",
    ":("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.5,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 1000\n",
    "}\n",
    "\n",
    "\n",
    "estimator = xgb.XGBClassifier(n_jobs=-1, random_state=17, **params)\n",
    "test_pred_proba = estimator.fit(X_train_sparse, y_train).predict_proba(X_test_sparse)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_xgb_allfeatures_n_est1000.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.85717**     \n",
    ":((("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.984819370433\n",
      "Wall time: 49.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# инициализируем параметры\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.5,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 170,\n",
    "    'eval_metric': 'auc'\n",
    "}\n",
    "\n",
    "estimator = xgb.XGBClassifier(n_jobs=-1, cv=skf, random_state=17, **params)\n",
    "roc_auc_score = CV_score(estimator, X_train_new_sparse, y_train, \n",
    "                         random_state=17, test_size=0.3)\n",
    "\n",
    "print(roc_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточное РЕЗЮМЕ:**   \n",
    "Видим, что **xgboost с логисической регрессией**, несмотря на **высокие результаты на кроссвалидации**, на **Kaggle** хорошего результата **не дает**. Поэтому пока откажемся от него. (Теоретически его можно было бы поднастроить и использовать **для смешивания** (блендинга) ответов бустинга и линейной модели.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вернемся к **SGDClassifier**\n",
    "пропустим категориальные признаки через **OHE** и \"скормим\" SGDClassifier, где вначале получился высокий AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модифицируем подготовку фич, добавив OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_df_OHE(train_df, test_df=None, features=None):\n",
    "    '''\n",
    "    features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    '''\n",
    "    if features is None:\n",
    "        features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    \n",
    "    if test_df is None:\n",
    "        df = train_df\n",
    "    else:\n",
    "        df = pd.concat([train_df, test_df])\n",
    "    \n",
    "    # выделим отдельно сайты\n",
    "    df_sites = df[['site%d' % i for i in range(1, 11)]] # не забыть заполнить нули\n",
    "    \n",
    "    # подготовим временнЫе метки\n",
    "    df[['time%d' % i for i in range(1, 11)]] = df[['time%d' % i for i in range(1, 11)]].applymap(pd.to_datetime)\n",
    "    timestamp_df = df[['time%d' % i for i in range(1, 11)]].applymap(lambda x: pd.datetime.timestamp(x) \n",
    "                                                                  if type(x) == pd._libs.tslib.Timestamp else np.nan)\n",
    "    # добавляем признаки\n",
    "    if '#unique_sites' in features:\n",
    "        df['#unique_sites'] = df[['site' + str(i) for i in range(1, 11)]].nunique(axis=1, dropna=True)\n",
    "    if 'session_timespan' in features:\n",
    "        df['session_timespan'] = timestamp_df.max(axis=1, skipna=True) - timestamp_df.min(axis=1, skipna=True)\n",
    "        df['session_timespan'] = df['session_timespan'].apply(lambda x: x // 30 if x < 210 else 7)\n",
    "    if 'start_hour' in features:    \n",
    "        df['start_hour'] = df['time1'].apply(lambda x: x.hour)\n",
    "    if 'day_of_week' in features:    \n",
    "        df['day_of_week'] = df['time1'].apply(lambda x: x.weekday())\n",
    "    if 'time_of_day' in features:    \n",
    "        # время суток (06-12 - утро(1), 12-19 - день(2), 19-24 - вечер(3), 00-06 - ночь(4))\n",
    "        df['time_of_day'] = df['time1'].apply(lambda x:  1 if x.hour >  6 and x.hour <= 12 else \n",
    "                                                                     2 if x.hour > 12 and x.hour <= 19 else\n",
    "                                                                     3 if x.hour > 19 and x.hour <= 24 else \n",
    "                                                                     4)\n",
    "    if 'weekend' in features:\n",
    "        # индикатор уикэнда в день начала сессии\n",
    "        df['weekend'] = df['time1'].apply(lambda x: 1 if x.weekday() in [5, 6] else 0)\n",
    "    \n",
    "    encoder = OneHotEncoder()\n",
    "    #X_new_sparse = csr_matrix(df[features].values)\n",
    "    X_new_sparse = encoder.fit_transform(df[features].values)\n",
    "    \n",
    "    df_sites = df_sites.fillna(0).astype('int')\n",
    "    train_sparse = csr_matrix((np.ones(df_sites.values.size), \n",
    "                                df_sites.values.ravel(), \n",
    "                                np.arange(df_sites.values.shape[0] + 1) * \n",
    "                                df_sites.values.shape[1]), dtype=int)[:, 1:]\n",
    "    \n",
    "    \n",
    "    y_train = df['target'].values[:train_df.shape[0]]\n",
    "        \n",
    "    X_sparse = sparse.hstack([train_sparse, X_new_sparse]).tocsr()\n",
    "    X_train_sparse = X_sparse[:train_df.shape[0]]\n",
    "    X_test_sparse = X_sparse[train_df.shape[0]:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (X_train_sparse, y_train) if test_df is None else (X_train_sparse, y_train, X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_sparse_ohe, y_train_ohe, X_test_sparse_ohe = feature_engineering_df_OHE(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.974101262722\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "auc = CV_score(estimator, X_train_sparse_ohe, y_train_ohe, cv=skf)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат впечатляет, попробуем на **Kaggle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "test_pred_proba = estimator.fit(X_train_sparse_ohe, y_train_ohe).predict_proba(X_test_sparse_ohe)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_allfeatures_OHE.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.93742**    \n",
    "Наконец то!   \n",
    ":))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем с балансировкой классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.977356917229\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "auc = CV_score(estimator, X_train_sparse_ohe, y_train_ohe, cv=skf)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат улучшился. Сделаем посылку на Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "test_pred_proba = estimator.fit(X_train_sparse_ohe, y_train_ohe).predict_proba(X_test_sparse_ohe)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_allfeatures_OHE_bal.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.94607** - результат **еще более улучшился!**   \n",
    ":)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем подбор **alpha**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.00041246263829013564} 0.982048226764\n",
      "Wall time: 55.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "\n",
    "sgd_params = {'alpha': np.logspace(-8, 1, 40)}\n",
    "\n",
    "sgd_grid_searcher = GridSearchCV(sgd_logit, param_grid=sgd_params, scoring='roc_auc', n_jobs=-1, cv=skf)\n",
    "sgd_grid_searcher.fit(X_train_sparse_ohe, y_train_ohe)\n",
    "\n",
    "print(sgd_grid_searcher.best_params_, sgd_grid_searcher.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, alpha=sgd_grid_searcher.best_params_['alpha'], \n",
    "                          class_weight='balanced', random_state=17)\n",
    "test_pred_proba = estimator.fit(X_train_sparse_ohe, y_train_ohe).predict_proba(X_test_sparse_ohe)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_allfeatures_OHE_bal_alpha.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.94478 - немного уменьшился**.   \n",
    "\n",
    "Видим, что подбор **alpha** пока результата на **Kaggle** не дал: несмотря на **увеличение метрики на кроссвалидации**, на Leader Board **результат ухудшился**.  \n",
    "\n",
    "Попробуем еще одну вещь. Попробуем еще немного **модифицировать признак 'session_timespan'**, а именно: прологарифмируем его по основанию **2** и отбросим дробную часть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_df_OHE_log2(train_df, test_df=None, features=None):\n",
    "    '''\n",
    "    features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    '''\n",
    "    if features is None:\n",
    "        features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    \n",
    "    if test_df is None:\n",
    "        df = train_df\n",
    "    else:\n",
    "        df = pd.concat([train_df, test_df])\n",
    "    \n",
    "    # выделим отдельно сайты\n",
    "    df_sites = df[['site%d' % i for i in range(1, 11)]] # не забыть заполнить нули\n",
    "    \n",
    "    # подготовим временнЫе метки\n",
    "    df[['time%d' % i for i in range(1, 11)]] = df[['time%d' % i for i in range(1, 11)]].applymap(pd.to_datetime)\n",
    "    timestamp_df = df[['time%d' % i for i in range(1, 11)]].applymap(lambda x: pd.datetime.timestamp(x) \n",
    "                                                                  if type(x) == pd._libs.tslib.Timestamp else np.nan)\n",
    "    # добавляем признаки\n",
    "    if '#unique_sites' in features:\n",
    "        df['#unique_sites'] = df[['site' + str(i) for i in range(1, 11)]].nunique(axis=1, dropna=True)\n",
    "    if 'session_timespan' in features:\n",
    "        df['session_timespan'] = timestamp_df.max(axis=1, skipna=True) - timestamp_df.min(axis=1, skipna=True)\n",
    "#         df['session_timespan'] = df['session_timespan'].apply(lambda x: x // 30 if x < 210 else 7)\n",
    "        df['session_timespan'] = df['session_timespan'].apply(lambda x: int(np.log2(x)) if x > 1 else 0)\n",
    "    if 'start_hour' in features:    \n",
    "        df['start_hour'] = df['time1'].apply(lambda x: x.hour)\n",
    "    if 'day_of_week' in features:    \n",
    "        df['day_of_week'] = df['time1'].apply(lambda x: x.weekday())\n",
    "    if 'time_of_day' in features:    \n",
    "        # время суток (06-12 - утро(1), 12-19 - день(2), 19-24 - вечер(3), 00-06 - ночь(4))\n",
    "        df['time_of_day'] = df['time1'].apply(lambda x:  1 if x.hour >  6 and x.hour <= 12 else \n",
    "                                                                     2 if x.hour > 12 and x.hour <= 19 else\n",
    "                                                                     3 if x.hour > 19 and x.hour <= 24 else \n",
    "                                                                     4)\n",
    "    if 'weekend' in features:\n",
    "        # индикатор уикэнда в день начала сессии\n",
    "        df['weekend'] = df['time1'].apply(lambda x: 1 if x.weekday() in [5, 6] else 0)\n",
    "    \n",
    "    encoder = OneHotEncoder()\n",
    "    #X_new_sparse = csr_matrix(df[features].values)\n",
    "    X_new_sparse = encoder.fit_transform(df[features].values)\n",
    "    \n",
    "    df_sites = df_sites.fillna(0).astype('int')\n",
    "    train_sparse = csr_matrix((np.ones(df_sites.values.size), \n",
    "                                df_sites.values.ravel(), \n",
    "                                np.arange(df_sites.values.shape[0] + 1) * \n",
    "                                df_sites.values.shape[1]), dtype=int)[:, 1:]\n",
    "    \n",
    "    \n",
    "    y_train = df['target'].values[:train_df.shape[0]]\n",
    "        \n",
    "    X_sparse = sparse.hstack([train_sparse, X_new_sparse]).tocsr()\n",
    "    X_train_sparse = X_sparse[:train_df.shape[0]]\n",
    "    X_test_sparse = X_sparse[train_df.shape[0]:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (X_train_sparse, y_train) if test_df is None else (X_train_sparse, y_train, X_test_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверим** гипотезу на ранее подготовленной функции с кроссвалидацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976601765937\n",
      "Wall time: 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_sparse_ohe_l2, y_train_ohe, X_test_sparse_ohe_l2 = feature_engineering_df_OHE_log2(train_df, test_df)\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "auc = CV_score(estimator, X_train_sparse_ohe_l2, y_train_ohe, cv=skf)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что **auc** немного увеличилась (в 4-м знаке, было 0.977325731478). Лимит посылок на **Kaggle** позволяет, поэтому проверим на Leader Board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "test_pred_proba = estimator.fit(X_train_sparse_ohe_l2, y_train_ohe).predict_proba(X_test_sparse_ohe_l2)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_allfeatures_OHE_bal_log2.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На **Kaggle** - 0.94549. Довольно высокий, но все-таки **немного ниже максимального**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее, можно еще попробовать \n",
    "- **TfidfVectorizer()**\n",
    "- подбор параметра **alpha** более продвинутым методом, например - с помощью библиотеки **hyperopt** (хотя на это я мало надеюсь, поскольку ранее вариант с более высоким **AUC** со CV давал **AUC** на Kaggle меньше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_df_OHE_Tfidf(train_df, test_df=None, features=None):\n",
    "    '''\n",
    "    features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    '''\n",
    "    if features is None:\n",
    "        features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    \n",
    "    if test_df is None:\n",
    "        df = train_df\n",
    "    else:\n",
    "        df = pd.concat([train_df, test_df])\n",
    "    \n",
    "    # выделим отдельно сайты\n",
    "    df_sites = df[['site%d' % i for i in range(1, 11)]] # не забыть заполнить нули\n",
    "    \n",
    "    # подготовим временнЫе метки\n",
    "    df[['time%d' % i for i in range(1, 11)]] = df[['time%d' % i for i in range(1, 11)]].applymap(pd.to_datetime)\n",
    "    timestamp_df = df[['time%d' % i for i in range(1, 11)]].applymap(lambda x: pd.datetime.timestamp(x) \n",
    "                                                                  if type(x) == pd._libs.tslib.Timestamp else np.nan)\n",
    "    # добавляем признаки\n",
    "    if '#unique_sites' in features:\n",
    "        df['#unique_sites'] = df[['site' + str(i) for i in range(1, 11)]].nunique(axis=1, dropna=True)\n",
    "    if 'session_timespan' in features:\n",
    "        df['session_timespan'] = timestamp_df.max(axis=1, skipna=True) - timestamp_df.min(axis=1, skipna=True)\n",
    "        df['session_timespan'] = df['session_timespan'].apply(lambda x: x // 30 if x < 210 else 7)\n",
    "#         df['session_timespan'] = df['session_timespan'].apply(lambda x: int(np.log2(x)) if x > 1 else 0)\n",
    "    if 'start_hour' in features:    \n",
    "        df['start_hour'] = df['time1'].apply(lambda x: x.hour)\n",
    "    if 'day_of_week' in features:    \n",
    "        df['day_of_week'] = df['time1'].apply(lambda x: x.weekday())\n",
    "    if 'time_of_day' in features:    \n",
    "        # время суток (06-12 - утро(1), 12-19 - день(2), 19-24 - вечер(3), 00-06 - ночь(4))\n",
    "        df['time_of_day'] = df['time1'].apply(lambda x:  1 if x.hour >  6 and x.hour <= 12 else \n",
    "                                                                     2 if x.hour > 12 and x.hour <= 19 else\n",
    "                                                                     3 if x.hour > 19 and x.hour <= 24 else \n",
    "                                                                     4)\n",
    "    if 'weekend' in features:\n",
    "        # индикатор уикэнда в день начала сессии\n",
    "        df['weekend'] = df['time1'].apply(lambda x: 1 if x.weekday() in [5, 6] else 0)\n",
    "        \n",
    "#     X_new_sparse = csr_matrix(df[features].values)\n",
    "    encoder = OneHotEncoder()\n",
    "    X_new_sparse = encoder.fit_transform(df[features].values)\n",
    "    \n",
    "    df_sites = df_sites.fillna(0).astype(int)\n",
    "#     train_sparse = csr_matrix((np.ones(df_sites.values.size), \n",
    "#                                 df_sites.values.ravel(), \n",
    "#                                 np.arange(df_sites.values.shape[0] + 1) * \n",
    "#                                 df_sites.values.shape[1]), dtype=int)[:, 1:]\n",
    "\n",
    "    # готовим блок с сайтами для кодирования посредством TfidfVectorizer()\n",
    "    df_sites = df_sites.astype(str)\n",
    "    sites_text = df_sites['site1']\n",
    "    for i in range(2, 11):\n",
    "        sites_text += ' ' + df_sites['site{}'.format(i)]\n",
    "    # кодируем подготовленный блок с сайтами\n",
    "    Tfidf_Vectorizer = TfidfVectorizer()    \n",
    "    train_sparse = Tfidf_Vectorizer.fit_transform(sites_text.values)\n",
    "   \n",
    "    X_sparse = sparse.hstack([train_sparse, X_new_sparse]).tocsr()\n",
    "    X_train_sparse = X_sparse[:train_df.shape[0]]\n",
    "    X_test_sparse = X_sparse[train_df.shape[0]:]\n",
    "    \n",
    "    y_train = df['target'].values[:train_df.shape[0]]\n",
    "    \n",
    "    return (X_train_sparse, y_train) if test_df is None else (X_train_sparse, y_train, X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.978230775915\n",
      "Wall time: 3min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_sparse_ohe_tfidf, y_train_ohe, X_test_sparse_ohe_tfidf = feature_engineering_df_OHE_Tfidf(train_df, test_df)\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "auc = CV_score(estimator, X_train_sparse_ohe_tfidf, y_train_ohe, cv=skf)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особого выигрыша не дало, но тем не менее попробуем на **Kaggle**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "test_pred_proba = estimator.fit(X_train_sparse_ohe_tfidf, y_train_ohe).predict_proba(X_test_sparse_ohe_tfidf)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_allfeatures_OHE_bal_tfidf.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.94773**   \n",
    "Урра!!! Позицию еще хоть и немного, но все же улучшили!   \n",
    ":)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем смешивание алгоритмов\n",
    "Будем делать это по схеме **блендинга**, которая представляет из себя разновидность **стекинга**, описаную в <a href='https://alexanderdyakonov.wordpress.com/2017/03/10/c%D1%82%D0%B5%D0%BA%D0%B8%D0%BD%D0%B3-stacking-%D0%B8-%D0%B1%D0%BB%D0%B5%D0%BD%D0%B4%D0%B8%D0%BD%D0%B3-blending/'>статье</a> на сайте **Александра Дьяконова**.   \n",
    "   \n",
    "А именно:  \n",
    "- делим обучающую выборку на две части\n",
    "- на первой настраиваем два алгоритма и делаем предсказания на второй\n",
    "- получаем два метапризнака\n",
    "- таким же образом получаем два метапризнака на тестовой выборке\n",
    "- далее, на полученных метапризнаках из второй части обучающей выборки настраиваем линейный классификатор и делаем предсказания на тестовых метапризнаках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_sparse_ohe_tfidf, y_train_ohe, X_test_sparse_ohe_tfidf = feature_engineering_df_OHE_Tfidf(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_sparse_ohe_tfidf, y_train_ohe, \n",
    "                                                      test_size=0.5, \n",
    "                                                      random_state=17, stratify=y_train_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 334 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "sgd_pred_proba = sgd_logit.fit(X_train, y_train).predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# инициализируем параметры\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 11,\n",
    "    'learning_rate': 0.5,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 300\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(n_jobs=-1, random_state=17, **params)\n",
    "xgb_pred_proba = xgb_classifier.fit(X_train, y_train).predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "делаем новые **метапризнаки** и обучаем на них **SGDClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_metafeature = np.hstack([sgd_pred_proba.reshape((-1, 1)), xgb_pred_proba.reshape((-1, 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_metafeature_test = np.hstack([sgd_logit.fit(X_train, y_train).predict_proba(X_test_sparse_ohe_tfidf)[:, 1].reshape((-1, 1)), \n",
    "                            xgb_classifier.fit(X_train, y_train).predict_proba(X_test_sparse_ohe_tfidf)[:, 1].reshape((-1, 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 961 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "meta_sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "test_pred_proba = meta_sgd_logit.fit(X_metafeature, y_valid).predict_proba(X_metafeature_test)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_allfeatures_OHE_bal_tfidf_meta.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.94406**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем **другую комбинацию** использования данных на аналогичном **блендинге**.    \n",
    "Для компактности соберем все в **одну ячейку** и организуем **измерение времени** отдельных этапов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for sgd_pred_proba: 0:00:00.358480\n",
      "Time for xgb_pred_proba: 0:01:10.550903\n",
      "Time for X_metafeature_test: 0:02:09.167243\n",
      "Time for write_to_submission_file: 0:00:00.879340\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "sgd_pred_proba = sgd_logit.fit(X_train, y_train).predict_proba(X_valid)[:, 1]\n",
    "\n",
    "print('Time for sgd_pred_proba:', datetime.datetime.now() - start_time) # время этапа\n",
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "# инициализируем параметры\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 11,\n",
    "    'learning_rate': 0.5,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 300\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(n_jobs=-1, random_state=17, **params)\n",
    "xgb_pred_proba = xgb_classifier.fit(X_train, y_train).predict_proba(X_valid)[:, 1]\n",
    "print('Time for xgb_pred_proba:', datetime.datetime.now() - start_time) # время этапа\n",
    "\n",
    "X_metafeature = np.hstack([sgd_pred_proba.reshape((-1, 1)), xgb_pred_proba.reshape((-1, 1))])\n",
    "\n",
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "# вычисляем метапризнаки на тестовой выборке\n",
    "X_metafeature_test = np.hstack([sgd_logit.fit(X_train_sparse_ohe_tfidf, y_train_ohe).predict_proba(X_test_sparse_ohe_tfidf)[:, 1].reshape((-1, 1)), \n",
    "                            xgb_classifier.fit(X_train_sparse_ohe_tfidf, y_train_ohe).predict_proba(X_test_sparse_ohe_tfidf)[:, 1].reshape((-1, 1))])\n",
    "\n",
    "print('Time for X_metafeature_test:', datetime.datetime.now() - start_time) # время этапа\n",
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "meta_sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "test_pred_proba = meta_sgd_logit.fit(X_metafeature, y_valid).predict_proba(X_metafeature_test)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_allfeatures_OHE_bal_tfidf_meta1.csv'))\n",
    "print('Time for write_to_submission_file:', datetime.datetime.now() - start_time) # время этапа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.94638**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for sgd_pred_proba: 0:00:00.360444\n",
      "Time for xgb_pred_proba: 0:00:29.772338\n",
      "Time for X_metafeature_test: 0:00:52.549302\n",
      "Time for write_to_submission_file: 0:00:00.779422\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "sgd_pred_proba = sgd_logit.fit(X_train, y_train).predict_proba(X_valid)[:, 1]\n",
    "\n",
    "print('Time for sgd_pred_proba:', datetime.datetime.now() - start_time) # время этапа\n",
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "# инициализируем параметры\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.5,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 170\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(n_jobs=-1, random_state=17, **params)\n",
    "xgb_pred_proba = xgb_classifier.fit(X_train, y_train).predict_proba(X_valid)[:, 1]\n",
    "print('Time for xgb_pred_proba:', datetime.datetime.now() - start_time) # время этапа\n",
    "\n",
    "X_metafeature = np.hstack([sgd_pred_proba.reshape((-1, 1)), xgb_pred_proba.reshape((-1, 1))])\n",
    "\n",
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "# вычисляем метапризнаки на тестовой выборке\n",
    "X_metafeature_test = np.hstack([sgd_logit.fit(X_train_sparse_ohe_tfidf, y_train_ohe).predict_proba(X_test_sparse_ohe_tfidf)[:, 1].reshape((-1, 1)), \n",
    "                            xgb_classifier.fit(X_train_sparse_ohe_tfidf, y_train_ohe).predict_proba(X_test_sparse_ohe_tfidf)[:, 1].reshape((-1, 1))])\n",
    "\n",
    "print('Time for X_metafeature_test:', datetime.datetime.now() - start_time) # время этапа\n",
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "meta_sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "test_pred_proba = meta_sgd_logit.fit(X_metafeature, y_valid).predict_proba(X_metafeature_test)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_allfeatures_OHE_bal_tfidf_meta1.csv'))\n",
    "print('Time for write_to_submission_file:', datetime.datetime.now() - start_time) # время этапа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.94642**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшим максимальную глубину деревьев в **xgboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for sgd_pred_proba: 0:00:00.354459\n",
      "Time for xgb_pred_proba: 0:00:24.615989\n",
      "Time for X_metafeature_test: 0:00:41.172088\n",
      "Time for write_to_submission_file: 0:00:00.767043\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "sgd_pred_proba = sgd_logit.fit(X_train, y_train).predict_proba(X_valid)[:, 1]\n",
    "\n",
    "print('Time for sgd_pred_proba:', datetime.datetime.now() - start_time) # время этапа\n",
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "# инициализируем параметры\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.5,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 170\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(n_jobs=-1, random_state=17, **params)\n",
    "xgb_pred_proba = xgb_classifier.fit(X_train, y_train).predict_proba(X_valid)[:, 1]\n",
    "print('Time for xgb_pred_proba:', datetime.datetime.now() - start_time) # время этапа\n",
    "\n",
    "X_metafeature = np.hstack([sgd_pred_proba.reshape((-1, 1)), xgb_pred_proba.reshape((-1, 1))])\n",
    "\n",
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "# вычисляем метапризнаки на тестовой выборке\n",
    "X_metafeature_test = np.hstack([sgd_logit.fit(X_train_sparse_ohe_tfidf, \n",
    "                                              y_train_ohe).predict_proba(X_test_sparse_ohe_tfidf)[:, 1].reshape((-1, 1)), \n",
    "                                xgb_classifier.fit(X_train_sparse_ohe_tfidf, \n",
    "                                                   y_train_ohe).predict_proba(X_test_sparse_ohe_tfidf)[:, 1].reshape((-1, 1))])\n",
    "\n",
    "print('Time for X_metafeature_test:', datetime.datetime.now() - start_time) # время этапа\n",
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "meta_sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "test_pred_proba = meta_sgd_logit.fit(X_metafeature, y_valid).predict_proba(X_metafeature_test)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, \n",
    "                                                       '[YDF & MIPT]_Coursera_allfeatures_OHE_bal_tfidf_meta2.csv'))\n",
    "print('Time for write_to_submission_file:', datetime.datetime.now() - start_time) # время этапа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.94739**    \n",
    "Неплохо, результат **вырос** при **уменьшении глубины** деревьев (вероятно, из-за уменьшения переобучения), но все же по-прежнему немного **уступает** самому лучшему результату, полученному выше на **SGDCclassifier**.   \n",
    "Тем не менее опыт интересный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**РЕЗЮМЕ:**   \n",
    "Итак, финальный результат этой недели - **результат на Kaggle: 0.94773**.   \n",
    "На сегодня это **63-я** позиция на **Public Leaderboard**, команда в соревновании - **[YDF & MIPT] Dmitry Shereshevskiy**   \n",
    "Бенчмарки этой недели -  **\"sgd_logit_benchmark.csv\"** и **\"Logit +3 features\"** - **побиты**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
