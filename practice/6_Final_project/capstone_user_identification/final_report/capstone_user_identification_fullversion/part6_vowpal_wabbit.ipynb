{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://habrastorage.org/web/677/8e1/337/6778e1337c3d4b159d7e99df94227cb2.jpg\"/>    \n",
    "<table>\n",
    "    <th><font size=5>Специализация \"Машинное обучение и анализ данных\" от:</font></th>\n",
    "    <th><img src=\"https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://d15cw65ipctsrr.cloudfront.net/11/ae0000b18911e5965623dd71776f15/mipt.png?auto=format%2Ccompress&dpr=1&w=200&h=100&fit=clip\"/> </th>\n",
    "    <th><img src=\"https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://d15cw65ipctsrr.cloudfront.net/fe/ab6f503b2b11e796ddbfacaf40b9e6/Yandex.jpg?auto=format%2Ccompress&dpr=1&w=200&h=100&fit=clip\"/></th>\n",
    "</table>\n",
    "\n",
    "# <center> ПРОЕКТ: Идентификация пользователей по посещенным веб-страницам\n",
    "# <center> АВТОР ПРОЕКТА: Дмитрий Шерешевский, *PhD*\n",
    "##    \n",
    "<center>Исходный материал: программист-исследователь Mail.ru Group, старший преподаватель Факультета Компьютерных Наук ВШЭ Юрий Кашницкий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://smartcity.eletsonline.com/wp-content/uploads/2017/04/digitalindia.png'>\n",
    "\n",
    "# <center>Часть 6 из 6.  Vowpal Wabbit\n",
    "\n",
    "В этой части мы применим популярную библиотеку **Vowpal Wabbit** и попробуем ее на данных соревнования. Перед этим поработаем с  [данными](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html) Scikit-learn по новостям, сначала в режиме бинарной классификации, затем – в многоклассовом режиме. Затем будем классифицировать рецензии к фильмам с сайта IMDB. Наконец, применим Vowpal Wabbit к данным по веб-сессиям. Vowpal Wabbit - отличная библиотека!\n",
    "\n",
    "**План 6 части:**\n",
    "- 6.1. Статья по Vowpal Wabbit\n",
    "- 6.2. Применение Vowpal Wabbit к данным по посещению сайтов\n",
    " - 2.1. Подготовка данных\n",
    " - 2.2. Валидация по отложенной выборке\n",
    " - 2.3. Валидация по тестовой выборке (Public Leaderboard)\n",
    "- 6.3. Vowpal Wabbit в соревновании\n",
    " - Введение\n",
    " - 3.1. Функция для подготовки признаков из \"сырых\" данных\n",
    " - 3.2. Валидация по отложенной выборке\n",
    " - 3.3. Подбор гиперпараметров\n",
    "\n",
    "**В этой части проекта Вам могут быть полезны видеозаписи следующих лекций курса \"Обучение на размеченных данных\":**\n",
    "   - [Стохатический градиентный спуск](https://www.coursera.org/learn/supervised-learning/lecture/xRY50/stokhastichieskii-ghradiientnyi-spusk)\n",
    "   - [Линейные модели. Sklearn.linear_model. Классификация](https://www.coursera.org/learn/supervised-learning/lecture/EBg9t/linieinyie-modieli-sklearn-linear-model-klassifikatsiia)\n",
    "   \n",
    "Также будет полезна [презентация](https://github.com/esokolov/ml-course-msu/blob/master/ML15/lecture-notes/Sem08_vw.pdf) лектора специализации Евгения Соколова. И, конечно же, [документация](https://github.com/JohnLangford/vowpal_wabbit/wiki) Vowpal Wabbit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>6.1. Статья про Vowpal Wabbit\n",
    "Краткая обзорная статья по Vowpal Wabbit на Хабре из серии открытого курса OpenDataScience по машинному обучению [здесь](https://habrahabr.ru/company/ods/blog/326418/). Можно скачать [ноутбук](https://github.com/Yorko/mlcourse_open/blob/master/jupyter_russian/topic08_sgd_hashing_vowpal_wabbit/topic8_sgd_hashing_vowpal_wabbit.ipynb), прилагаемый к статье, и попробовать на практике разобраться с Vowpal Wabbit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>6.2. Применение Vowpal Wabbit к данным по посещению сайтов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Далее посмотрим на Vowpal Wabbit в деле. Правда, в задаче соревнования при бинарной классификации веб-сессий мы разницы не заметим – как по качеству, так и по скорости работы, но уже в задаче классификации на 400 классов будет хорошо заметна скорость работы VW. Данные [здесь](https://inclass.kaggle.com/c/identify-me-if-you-can4/data) – файлы `train_sessions_400users.csv` и `test_sessions_400users.csv`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Путь к данным\n",
    "PATH_TO_DATA = ('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Загрузим обучающую и тестовую выборки. Можно заметить, что тестовые сессии здесь по времени четко отделены от сессий в обучающей выборке. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_400 = pd.read_csv(os.path.join(PATH_TO_DATA,'train_sessions_400users.csv'), \n",
    "                           index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df_400 = pd.read_csv(os.path.join(PATH_TO_DATA,'test_sessions_400users.csv'), \n",
    "                           index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23713</td>\n",
       "      <td>2014-03-24 15:22:40</td>\n",
       "      <td>23720.0</td>\n",
       "      <td>2014-03-24 15:22:48</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:22:48</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:22:54</td>\n",
       "      <td>23720.0</td>\n",
       "      <td>2014-03-24 15:22:54</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-24 15:22:55</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:01</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:03</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:04</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:05</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8726</td>\n",
       "      <td>2014-04-17 14:25:58</td>\n",
       "      <td>8725.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>665.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>8727.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-04-17 14:26:01</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2014-04-17 14:26:01</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:18</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:47</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:48</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>2014-03-21 10:12:24</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2014-03-21 10:12:36</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:12:54</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:01</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:24</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-21 10:13:36</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:54</td>\n",
       "      <td>309.0</td>\n",
       "      <td>2014-03-21 10:14:01</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:14:06</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:14:24</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1359</td>\n",
       "      <td>2013-12-13 09:52:28</td>\n",
       "      <td>925.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1346.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>2013-12-13 09:58:19</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>2013-12-13 09:58:19</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>2013-11-26 12:35:29</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:35:31</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2013-11-26 12:35:31</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:37:04</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1    site2                time2    site3  \\\n",
       "session_id                                                                      \n",
       "1           23713  2014-03-24 15:22:40  23720.0  2014-03-24 15:22:48  23713.0   \n",
       "2            8726  2014-04-17 14:25:58   8725.0  2014-04-17 14:25:59    665.0   \n",
       "3             303  2014-03-21 10:12:24     19.0  2014-03-21 10:12:36    303.0   \n",
       "4            1359  2013-12-13 09:52:28    925.0  2013-12-13 09:54:34   1240.0   \n",
       "5              11  2013-11-26 12:35:29     85.0  2013-11-26 12:35:31     52.0   \n",
       "\n",
       "                          time3    site4                time4    site5  \\\n",
       "session_id                                                               \n",
       "1           2014-03-24 15:22:48  23713.0  2014-03-24 15:22:54  23720.0   \n",
       "2           2014-04-17 14:25:59   8727.0  2014-04-17 14:25:59     45.0   \n",
       "3           2014-03-21 10:12:54    303.0  2014-03-21 10:13:01    303.0   \n",
       "4           2013-12-13 09:54:34   1360.0  2013-12-13 09:54:34   1344.0   \n",
       "5           2013-11-26 12:35:31     85.0  2013-11-26 12:35:32     11.0   \n",
       "\n",
       "                          time5   ...                  time6    site7  \\\n",
       "session_id                        ...                                   \n",
       "1           2014-03-24 15:22:54   ...    2014-03-24 15:22:55  23713.0   \n",
       "2           2014-04-17 14:25:59   ...    2014-04-17 14:26:01     45.0   \n",
       "3           2014-03-21 10:13:24   ...    2014-03-21 10:13:36    303.0   \n",
       "4           2013-12-13 09:54:34   ...    2013-12-13 09:54:34   1346.0   \n",
       "5           2013-11-26 12:35:32   ...    2013-11-26 12:35:32     11.0   \n",
       "\n",
       "                          time7    site8                time8    site9  \\\n",
       "session_id                                                               \n",
       "1           2014-03-24 15:23:01  23713.0  2014-03-24 15:23:03  23713.0   \n",
       "2           2014-04-17 14:26:01   5320.0  2014-04-17 14:26:18   5320.0   \n",
       "3           2014-03-21 10:13:54    309.0  2014-03-21 10:14:01    303.0   \n",
       "4           2013-12-13 09:54:34   1345.0  2013-12-13 09:54:34   1344.0   \n",
       "5           2013-11-26 12:37:03     85.0  2013-11-26 12:37:03     10.0   \n",
       "\n",
       "                          time9   site10               time10 user_id  \n",
       "session_id                                                             \n",
       "1           2014-03-24 15:23:04  23713.0  2014-03-24 15:23:05     653  \n",
       "2           2014-04-17 14:26:47   5320.0  2014-04-17 14:26:48     198  \n",
       "3           2014-03-21 10:14:06    303.0  2014-03-21 10:14:24      34  \n",
       "4           2013-12-13 09:58:19   1345.0  2013-12-13 09:58:19     601  \n",
       "5           2013-11-26 12:37:03     85.0  2013-11-26 12:37:04     273  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_400.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((182793, 21), (46473, 20), 400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_400.shape, test_df_400.shape, train_df_400['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Видим, что в обучающей выборке 182793 сессий, в тестовой – 46473, и сессии действительно принадлежат 400 различным пользователям.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vowpal Wabbit любит, чтоб метки классов были распределены от 1 до K, где K – число классов в задаче классификации (в нашем случае – 400). Поэтому придется применить `LabelEncoder` и затем еще +1 добавить (`LabelEncoder` переводит метки в диапозон от 0 до K-1). Потом надо будет применить обратное преобразование.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train_df_400['user_id']\n",
    "class_encoder = LabelEncoder()\n",
    "y_for_vw = class_encoder.fit_transform(y) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Далее будем сравнивать VW с SGDClassifier и с логистической регрессией. Всем моделям этим нужна предобработка входных данных. Подготовьте для sklearn-моделей разреженные матрицы, как мы это делали в 5 части:**\n",
    "- объединим обучающиую и тестовую выборки\n",
    "- выберем только сайты (признаки от 'site1' до 'site10')\n",
    "- заменим пропуски на нули (сайты у нас нумеровались с 0)\n",
    "- переведём в разреженный формат `csr_matrix`\n",
    "- разобьем обратно на обучающую и тестовую части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sites = ['site' + str(i) for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df_400, test_df_400])\n",
    "train_test_df_sites = train_test_df[sites].fillna(0).astype(int)\n",
    "train_test_sparse = csr_matrix((np.ones(train_test_df_sites.values.size), \n",
    "                                train_test_df_sites.values.ravel(), \n",
    "                                np.arange(train_test_df_sites.values.shape[0] + 1) * \n",
    "                                train_test_df_sites.values.shape[1]), dtype=int)[:, 1:]\n",
    "\n",
    "X_train_sparse = train_test_sparse[:train_df_400.shape[0]]\n",
    "X_test_sparse = train_test_sparse[train_df_400.shape[0]:]\n",
    "y = train_df_400['user_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Валидация по отложенной выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выделим обучающую (70%) и отложенную (30%) части исходной обучающей выборки. Данные не перемешиваем, учитываем, что сессии отсортированы по времени.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_share = int(.7 * train_df_400.shape[0])\n",
    "train_df_part = train_df_400[sites].iloc[:train_share, :]\n",
    "valid_df = train_df_400[sites].iloc[train_share:, :]\n",
    "X_train_part_sparse = X_train_sparse[:train_share, :]\n",
    "X_valid_sparse = X_train_sparse[train_share:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_part = y[:train_share]\n",
    "y_valid = y[train_share:]\n",
    "y_train_part_for_vw = y_for_vw[:train_share]\n",
    "y_valid_for_vw = y_for_vw[train_share:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Реализуем функцию, `arrays_to_vw`, переводящую обучающую выборку в формат Vowpal Wabbit.**\n",
    "\n",
    "Вход:\n",
    " - X – матрица `NumPy` (обучающая выборка)\n",
    " - y (необяз.) - вектор ответов (`NumPy`). Необязателен, поскольку тестовую матрицу будем обрабатывать этой же функцией\n",
    " - train – флаг, True в случае обучающей выборки, False – в случае тестовой выборки\n",
    " - out_file – путь к файлу .vw, в который будет произведена запись\n",
    " \n",
    "Детали:\n",
    "- надо пройтись по всем строкам матрицы `X` и записать через пробел все значения, предварительно добавив вперед нужную метку класса из вектора `y` и знак-разделитель `|`\n",
    "- в тестовой выборке на месте меток целевого класса можно писать произвольные, допустим, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arrays_to_vw(X, y=None, train=True, out_file=os.path.join(PATH_TO_DATA,'tmp.vw')):\n",
    "    if y is None:\n",
    "        y = [1] * X.shape[0]\n",
    "    with open(out_file, 'w') as vw_data:\n",
    "        for row, site_id in zip(X, y):\n",
    "            sample = str(site_id) +' | ' + ' '.join(row.astype(int).astype(str)) + '\\n'\n",
    "            vw_data.write(sample)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примените написанную функцию к части обучащей выборки `(train_df_part, y_train_part_for_vw)`, к отложенной выборке `(valid_df, y_valid_for_vw)`, ко всей обучающей выборке и ко всей тестовой выборке. Обратим внимание, что на вход наш метод принимает именно матрицы и вектора `NumPy`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# будет 4 вызова\n",
    "arrays_to_vw(train_df_part.fillna(0).values, \n",
    "             y_train_part_for_vw, \n",
    "             out_file=os.path.join(PATH_TO_DATA,'train_part.vw'))\n",
    "\n",
    "arrays_to_vw(valid_df.fillna(0).values, \n",
    "             y_valid_for_vw, \n",
    "             out_file=os.path.join(PATH_TO_DATA,'valid.vw'))\n",
    "\n",
    "arrays_to_vw(train_df_400[sites].fillna(0).values, \n",
    "             y_for_vw, \n",
    "             out_file=os.path.join(PATH_TO_DATA,'train.vw'))\n",
    "\n",
    "arrays_to_vw(test_df_400[sites].fillna(0).values, \n",
    "             out_file=os.path.join(PATH_TO_DATA,'test.vw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверяем:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 | 23713 23720 23713 23713 23720 23713 23713 23713 23713 23713\n",
      "82 | 8726 8725 665 8727 45 8725 45 5320 5320 5320\n",
      "16 | 303 19 303 303 303 303 303 309 303 303\n"
     ]
    }
   ],
   "source": [
    "!head -3 $PATH_TO_DATA/train_part.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучим модель Vowpal Wabbit на выборке `train_part.vw`. Укажем, что решается задача классификации с 400 классами (`--oaa`), сделаем 3 прохода по выборке (`--passes`). Зададим некоторый кэш-файл (флаг `-c`), так VW будет быстрее делать все следующие после первого проходы по выборке (прошлый кэш-файл удаляется с помощью аргумента `-k`). Также укажем значение параметра `b`=26. Это число бит, используемых для хэширования, в данном случае нужно больше, чем 18 по умолчанию. Наконец, укажем `random_seed`=17. Остальные параметры пока оставим по умолчанию.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_part_vw = os.path.join(PATH_TO_DATA, 'train_part.vw')\n",
    "valid_vw = os.path.join(PATH_TO_DATA, 'valid.vw')\n",
    "train_vw = os.path.join(PATH_TO_DATA, 'train.vw')\n",
    "test_vw = os.path.join(PATH_TO_DATA, 'test.vw')\n",
    "model = os.path.join(PATH_TO_DATA, 'vw_model.vw')\n",
    "pred = os.path.join(PATH_TO_DATA, 'vw_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "final_regressor = data/vw_model.vw\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = data/train_part.vw.cache\n",
      "Reading datafile = data/train_part.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0      262        1       11\n",
      "1.000000 1.000000            2            2.0       82      262       11\n",
      "1.000000 1.000000            4            4.0      241      262       11\n",
      "1.000000 1.000000            8            8.0      352      262       11\n",
      "1.000000 1.000000           16           16.0      135       16       11\n",
      "1.000000 1.000000           32           32.0       71      112       11\n",
      "0.968750 0.937500           64           64.0      358      231       11\n",
      "0.976563 0.984375          128          128.0      348      346       11\n",
      "0.941406 0.906250          256          256.0      202      202       11\n",
      "0.947266 0.953125          512          512.0       30        1       11\n",
      "0.925781 0.904297         1024         1024.0       36      290       11\n",
      "0.908203 0.890625         2048         2048.0       21      128       11\n",
      "0.880127 0.852051         4096         4096.0       80      229       11\n",
      "0.856323 0.832520         8192         8192.0      307      356       11\n",
      "0.828003 0.799683        16384        16384.0       59      193       11\n",
      "0.795441 0.762878        32768        32768.0      262       30       11\n",
      "0.760468 0.725494        65536        65536.0      171      238       11\n",
      "0.724008 0.724008       131072       131072.0        6        6       11 h\n",
      "0.697339 0.670672       262144       262144.0       12       12       11 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 115160\n",
      "passes used = 3\n",
      "weighted example sum = 345480.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.661352 h\n",
      "total feature number = 3800280\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw --oaa 400 -d $train_part_vw --passes 3 -c -k \\\n",
    "-f $model -b 26 --random_seed 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишем прогнозы на выборке *valid.vw* в *vw_valid_pred.csv*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.91 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = data/vw_valid_pred.csv\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        4      188       11\n",
      "1.000000 1.000000            2            2.0      160      220       11\n",
      "0.750000 0.500000            4            4.0      143      143       11\n",
      "0.750000 0.750000            8            8.0      247      247       11\n",
      "0.687500 0.625000           16           16.0      341       30       11\n",
      "0.593750 0.500000           32           32.0      237      237       11\n",
      "0.609375 0.625000           64           64.0      178      178       11\n",
      "0.640625 0.671875          128          128.0      132      228       11\n",
      "0.656250 0.671875          256          256.0       14       14       11\n",
      "0.646484 0.636719          512          512.0      370      370       11\n",
      "0.663086 0.679688         1024         1024.0      189      189       11\n",
      "0.655762 0.648438         2048         2048.0      311      311       11\n",
      "0.657227 0.658691         4096         4096.0      195      318       11\n",
      "0.660156 0.663086         8192         8192.0      171      195       11\n",
      "0.657654 0.655151        16384        16384.0      362       51       11\n",
      "0.655121 0.652588        32768        32768.0      248      248       11\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 54838\n",
      "passes used = 1\n",
      "weighted example sum = 54838.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.654583\n",
      "total feature number = 603218\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -t -i $model \\\n",
    "-d $valid_vw \\\n",
    "-p $PATH_TO_DATA/vw_valid_pred.csv --random_seed 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считаем прогнозы *\"../data/vw_valid_pred.csv\"*  из файла и посмотрим на долю правильных ответов на отложенной части.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34541741128414605"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "vw_valid_pred = np.loadtxt(os.path.join(PATH_TO_DATA, 'vw_valid_pred.csv'))\n",
    "\n",
    "accuracy_score(y_valid_for_vw, vw_valid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Теперь обучим `SGDClassifier` (3 прохода по выборке, логистическая функция потерь) и `LogisticRegression` на 70% разреженной обучающей выборки – `(X_train_part_sparse, y_train_part)`, сделаем прогноз для отложенной выборки `(X_valid_sparse, y_valid)` и посчитаем доли верных ответов. Логистическая регрессия обучается долго – это нормально. Укажем везде `random_state`=17, `n_jobs`=-1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(random_state=17, n_jobs=-1)\n",
    "sgd_logit = SGDClassifier(loss='log', max_iter=3, random_state=17, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=17, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logit.fit(X_train_part_sparse, y_train_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=3, n_iter=None,\n",
       "       n_jobs=-1, penalty='l2', power_t=0.5, random_state=17, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit.fit(X_train_part_sparse, y_train_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Какими получаются доли правильных ответов на отложенной выборке для Vowpal Wabbit, SGD и логистической регрессии? Запишите 3 числа в файл *answer6_1.txt* (в указанном порядке, сначала VW, потом SGD и logit) через пробел, округлив до 3 знаков после запятой.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vw_valid_acc = accuracy_score(y_valid_for_vw, vw_valid_pred)\n",
    "sgd_valid_acc = accuracy_score(y_valid, sgd_logit.predict(X_valid_sparse))\n",
    "logit_valid_acc = accuracy_score(y_valid, logit.predict(X_valid_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"vw_accuracy = {}, sgd_accuracy = {}, logit_accuracy = {}\"\n",
    "      .format(round(vw_valid_acc, 3), round(sgd_valid_acc, 3), round(logit_valid_acc, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vw_accuracy = 0.345, sgd_accuracy = 0.291, logit_accuracy = 0.363"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Валидация по тестовой выборке (Public Leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучим модель VW с теми же параметрами на всей обучающей выборке – *train.vw*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "final_regressor = data//vw_model_400_train.vw\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = data/train.vw.cache\n",
      "Reading datafile = data/train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0      262        1       11\n",
      "1.000000 1.000000            2            2.0       82      262       11\n",
      "1.000000 1.000000            4            4.0      241      262       11\n",
      "1.000000 1.000000            8            8.0      352      262       11\n",
      "1.000000 1.000000           16           16.0      135       16       11\n",
      "1.000000 1.000000           32           32.0       71      112       11\n",
      "0.968750 0.937500           64           64.0      358      231       11\n",
      "0.976563 0.984375          128          128.0      348      346       11\n",
      "0.941406 0.906250          256          256.0      202      202       11\n",
      "0.947266 0.953125          512          512.0       30        1       11\n",
      "0.925781 0.904297         1024         1024.0       36      290       11\n",
      "0.908203 0.890625         2048         2048.0       21      128       11\n",
      "0.880127 0.852051         4096         4096.0       80      229       11\n",
      "0.856323 0.832520         8192         8192.0      307      356       11\n",
      "0.828003 0.799683        16384        16384.0       59      193       11\n",
      "0.795441 0.762878        32768        32768.0      262       30       11\n",
      "0.760468 0.725494        65536        65536.0      171      238       11\n",
      "0.725319 0.690170       131072       131072.0      180      159       11\n",
      "0.692989 0.692989       262144       262144.0       88      221       11 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 164514\n",
      "passes used = 3\n",
      "weighted example sum = 493542.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.642595 h\n",
      "total feature number = 5428962\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw --oaa 400 -d $train_vw --passes 3 -c -k \\\n",
    "-f $PATH_TO_DATA/vw_model_400_train.vw -b 26 --random_seed 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделаем прогноз для тестовой выборки.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.76 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = data//vw_400_test_pred.csv\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        1       90       11\n",
      "1.000000 1.000000            2            2.0        1       21       11\n",
      "1.000000 1.000000            4            4.0        1      265       11\n",
      "1.000000 1.000000            8            8.0        1      137       11\n",
      "1.000000 1.000000           16           16.0        1      273       11\n",
      "1.000000 1.000000           32           32.0        1      384       11\n",
      "1.000000 1.000000           64           64.0        1      139       11\n",
      "1.000000 1.000000          128          128.0        1       85       11\n",
      "1.000000 1.000000          256          256.0        1       25       11\n",
      "0.994141 0.988281          512          512.0        1      364       11\n",
      "0.990234 0.986328         1024         1024.0        1      202       11\n",
      "0.992188 0.994141         2048         2048.0        1      181       11\n",
      "0.993652 0.995117         4096         4096.0        1       21       11\n",
      "0.994629 0.995605         8192         8192.0        1      137       11\n",
      "0.995300 0.995972        16384        16384.0        1      326       11\n",
      "0.994568 0.993835        32768        32768.0        1       10       11\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 46473\n",
      "passes used = 1\n",
      "weighted example sum = 46473.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.994642\n",
      "total feature number = 511203\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -t -i $PATH_TO_DATA/vw_model_400_train.vw \\\n",
    "-d $test_vw \\\n",
    "-p $PATH_TO_DATA/vw_400_test_pred.csv --random_seed 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишем прогноз в файл, применим обратное преобразование меток (был LabelEncoder и потом +1 в меткам) и отправим решение на Kaggle.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vw_pred = class_encoder.inverse_transform(np.loadtxt(os.path.join(PATH_TO_DATA, 'vw_400_test_pred.csv'), dtype=int) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(vw_pred, os.path.join(PATH_TO_DATA, 'vw_400_users1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделаем то же самое для SGD и логистической регрессии. Здесь времени потребуется гораздо больше.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(random_state=17)\n",
    "sgd_logit = SGDClassifier(loss='log', max_iter=3, random_state=17, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 47min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=17, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logit.fit(X_train_sparse, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=3, n_iter=None,\n",
       "       n_jobs=-1, penalty='l2', power_t=0.5, random_state=17, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit.fit(X_train_sparse, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_test_pred = logit.predict(X_test_sparse)\n",
    "sgd_logit_test_pred = sgd_logit.predict(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(logit_test_pred, \n",
    "                         os.path.join(PATH_TO_DATA, 'logit_400_users1.csv'))\n",
    "write_to_submission_file(sgd_logit_test_pred, \n",
    "                         os.path.join(PATH_TO_DATA, 'sgd_400_users1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим следующие доли правильных ответов  **Vowpal Wabbit, SGD и логистической регрессии** на публичной части (public leaderboard) тестовой выборки [этого](https://inclass.kaggle.com/c/identify-me-if-you-can4): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "0.188   \n",
    "0.175   \n",
    "0.199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫВОД по соотношению качества классификации и скорости обучения VW, SGD и logit:**\n",
    "- видим, что **Vowpal Wabbit** работает **значительно быстрее**, чем SGD и особенно логистическая регрессия, в то время как качество классификации - немного хуже логистической регрессии, но лучше SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пути улучшения\n",
    "\n",
    "Что еще можно попробовать в [соревновании](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2):\n",
    " - Использовать ранее построенные признаки для улучшения модели\n",
    " - Настроить параметры Vowpal Wabbit с hyperopt\n",
    " - Попробовать TF-IDF и n-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Часть 3. Vowpal Wabbit в соревновании"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим полученный опыт работы с **Vowpal Wabbit** для получения результата в [соревновании](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считаем данные [соревнования](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) в DataFrame train_df и test_df (обучающая и тестовая выборки).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Путь к данным\n",
    "PATH_TO_DATA = ('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n",
    "                       index_col='session_id')\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n",
    "                      index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>2014-02-20 10:02:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>942.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>2014-02-22 11:20:15</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>2014-02-22 11:20:16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>2013-12-16 16:40:17</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013-12-16 16:40:18</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14769.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:20</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:21</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:22</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:52:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:52:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:54:12</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-28 10:54:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>2014-02-28 10:53:05</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:57:06</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:57:11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1  site2                time2    site3  \\\n",
       "session_id                                                                    \n",
       "1             718  2014-02-20 10:02:45    NaN                  NaN      NaN   \n",
       "2             890  2014-02-22 11:19:50  941.0  2014-02-22 11:19:50   3847.0   \n",
       "3           14769  2013-12-16 16:40:17   39.0  2013-12-16 16:40:18  14768.0   \n",
       "4             782  2014-03-28 10:52:12  782.0  2014-03-28 10:52:42    782.0   \n",
       "5              22  2014-02-28 10:53:05  177.0  2014-02-28 10:55:22    175.0   \n",
       "\n",
       "                          time3    site4                time4  site5  \\\n",
       "session_id                                                             \n",
       "1                           NaN      NaN                  NaN    NaN   \n",
       "2           2014-02-22 11:19:51    941.0  2014-02-22 11:19:51  942.0   \n",
       "3           2013-12-16 16:40:19  14769.0  2013-12-16 16:40:19   37.0   \n",
       "4           2014-03-28 10:53:12    782.0  2014-03-28 10:53:42  782.0   \n",
       "5           2014-02-28 10:55:22    178.0  2014-02-28 10:55:23  177.0   \n",
       "\n",
       "                          time5  ...                  time6    site7  \\\n",
       "session_id                       ...                                   \n",
       "1                           NaN  ...                    NaN      NaN   \n",
       "2           2014-02-22 11:19:51  ...    2014-02-22 11:19:51   3847.0   \n",
       "3           2013-12-16 16:40:19  ...    2013-12-16 16:40:19  14768.0   \n",
       "4           2014-03-28 10:54:12  ...    2014-03-28 10:54:42    782.0   \n",
       "5           2014-02-28 10:55:23  ...    2014-02-28 10:55:59    175.0   \n",
       "\n",
       "                          time7    site8                time8    site9  \\\n",
       "session_id                                                               \n",
       "1                           NaN      NaN                  NaN      NaN   \n",
       "2           2014-02-22 11:19:52   3846.0  2014-02-22 11:19:52   1516.0   \n",
       "3           2013-12-16 16:40:20  14768.0  2013-12-16 16:40:21  14768.0   \n",
       "4           2014-03-28 10:55:12    782.0  2014-03-28 10:55:42    782.0   \n",
       "5           2014-02-28 10:55:59    177.0  2014-02-28 10:55:59    177.0   \n",
       "\n",
       "                          time9   site10               time10 target  \n",
       "session_id                                                            \n",
       "1                           NaN      NaN                  NaN      0  \n",
       "2           2014-02-22 11:20:15   1518.0  2014-02-22 11:20:16      0  \n",
       "3           2013-12-16 16:40:22  14768.0  2013-12-16 16:40:24      0  \n",
       "4           2014-03-28 10:56:12    782.0  2014-03-28 10:56:42      0  \n",
       "5           2014-02-28 10:57:06    178.0  2014-02-28 10:57:11      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отметим, что для работы с Vowpal Wabbit благодаря **хешированию** объединять обучающую и тестовую выборки нам нет необходимости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающей выборке видим следующие признаки:\n",
    "    - site1 – индекс первого посещенного сайта в сессии\n",
    "    - time1 – время посещения первого сайта в сессии\n",
    "    - ...\n",
    "    - site10 – индекс 10-го посещенного сайта в сессии\n",
    "    - time10 – время посещения 10-го сайта в сессии\n",
    "    - user_id – ID пользователя\n",
    "    \n",
    "Сессии пользователей выделены таким образом, что они не могут быть длинее получаса или 10 сайтов. То есть сессия считается оконченной либо когда пользователь посетил 10 сайтов подряд, либо когда сессия заняла по времени более 30 минут. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на статистику признаков.**\n",
    "\n",
    "Пропуски возникают там, где сессии короткие (менее 10 сайтов). Скажем, если человек 1 января 2015 года посетил *vk.com* в 20:01, потом *yandex.ru* в 20:29, затем *google.com* в 20:33, то первая его сессия будет состоять только из двух сайтов (site1 – ID сайта *vk.com*, time1 – 2015-01-01 20:01:00, site2 – ID сайта  *yandex.ru*, time2 – 2015-01-01 20:29:00, остальные признаки – NaN), а начиная с *google.com* пойдет новая сессия, потому что уже прошло более 30 минут с момента посещения *vk.com*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающей выборке – 2297 сессий одного пользователя (Alice) и 251264 сессий – других пользователей, не Элис. Дисбаланс классов очень сильный, и смотреть на долю верных ответов (accuracy) непоказательно. Поэтому для соревнования выбрана метрика **roc_auc**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    251264\n",
       "1      2297\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Функция для подготовки признаков из \"сырых\" данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем функцию, формирующую из загруженных выше \"сырых\" данных признаки, которые мы впоследствии преобразуем к формату для Vowpal Wabbit.    \n",
    "\n",
    "Будем это делать на основе опыта, полученного на 5 недели, когда мы участвовали в соревновании с использованием линейного  классификатора **SGDClassifier** и классификатора на основе градиентного бустинга из библиотеки **XGBoost**. Учтем все **инсайты**, к которым мы пришли. Будем придерживаться следующего:\n",
    "- сформируем функцию для признаков из исходноых данных на базе итоговой функции из 5 недели\n",
    "- учтем, что все дополнительные признаки - по сути **категориальные**\n",
    "- сформируем признаки, которые хорошо себя зарекомендовали на предыдущем этапе\n",
    "- не будем пока добавлять новых признаков, чтобы иметь возможность сравнить алгоритмы.\n",
    "- адаптируем функцию к последующему преобразованию в формат Vowpal Wabbit.\n",
    "\n",
    "**Итак, сформируем функцию для признаков. На вход она должна принимать \"сырой\" датафрейм с данными, а на выходе должен быть датафрейм с признаками и целевой вектор `y`, адапрированный по Vopal Wabbit, то есть принимающий зачения [-1, 1]. Пропуски заменим на 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_df_vw(train_df, test=False, features=None):\n",
    "    '''\n",
    "    features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    '''\n",
    "    if features is None:\n",
    "        features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    \n",
    "    df = train_df\n",
    "    if test:\n",
    "        df['target'] = 1\n",
    "    else:\n",
    "        df['target'] = df['target'].map({1:1, 0:-1})\n",
    "    \n",
    "    \n",
    "    # подготовим временнЫе метки\n",
    "    df[['time%d' % i for i in range(1, 11)]] = df[['time%d' % i for i in range(1, 11)]].applymap(pd.to_datetime)\n",
    "    timestamp_df = df[['time%d' % i for i in range(1, 11)]].applymap(lambda x: pd.datetime.timestamp(x) \n",
    "                                                                  if type(x) == pd._libs.tslib.Timestamp else np.nan)\n",
    "    # добавляем признаки\n",
    "    if '#unique_sites' in features:\n",
    "        df['#unique_sites'] = df[['site' + str(i) for i in range(1, 11)]].nunique(axis=1, dropna=True)\n",
    "    if 'session_timespan' in features:\n",
    "        df['session_timespan'] = timestamp_df.max(axis=1, skipna=True) - timestamp_df.min(axis=1, skipna=True)\n",
    "        df['session_timespan'] = df['session_timespan'].apply(lambda x: int(x // 30) if x < 210 else 7)\n",
    "    if 'start_hour' in features:    \n",
    "        df['start_hour'] = df['time1'].apply(lambda x: x.hour)\n",
    "    if 'day_of_week' in features:    \n",
    "        df['day_of_week'] = df['time1'].apply(lambda x: x.weekday())\n",
    "    if 'time_of_day' in features:    \n",
    "        # время суток (06-12 - утро(1), 12-19 - день(2), 19-24 - вечер(3), 00-06 - ночь(4))\n",
    "        df['time_of_day'] = df['time1'].apply(lambda x:  1 if x.hour >  6 and x.hour <= 12 else \n",
    "                                                                     2 if x.hour > 12 and x.hour <= 19 else\n",
    "                                                                     3 if x.hour > 19 and x.hour <= 24 else \n",
    "                                                                     4)\n",
    "    if 'weekend' in features:\n",
    "        # индикатор уикэнда в день начала сессии\n",
    "        df['weekend'] = df['time1'].apply(lambda x: 1 if x.weekday() in [5, 6] else 0)\n",
    "        \n",
    "    col = ['site%d' % i for i in range(1, 11)] + features\n",
    "    \n",
    "    return (df[col].fillna(0).astype(int), df['target'].values.astype(int)) if not test else df[col].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_for_vw_testing, y = feature_engineering_df_vw(train_df.head().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "      <th>#unique_sites</th>\n",
       "      <th>session_timespan</th>\n",
       "      <th>start_hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>weekend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>941</td>\n",
       "      <td>3847</td>\n",
       "      <td>941</td>\n",
       "      <td>942</td>\n",
       "      <td>3846</td>\n",
       "      <td>3847</td>\n",
       "      <td>3846</td>\n",
       "      <td>1516</td>\n",
       "      <td>1518</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>39</td>\n",
       "      <td>14768</td>\n",
       "      <td>14769</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>177</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>175</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "1             718      0      0      0      0      0      0      0      0   \n",
       "2             890    941   3847    941    942   3846   3847   3846   1516   \n",
       "3           14769     39  14768  14769     37     39  14768  14768  14768   \n",
       "4             782    782    782    782    782    782    782    782    782   \n",
       "5              22    177    175    178    177    178    175    177    177   \n",
       "\n",
       "            site10  #unique_sites  session_timespan  start_hour  day_of_week  \\\n",
       "session_id                                                                     \n",
       "1                0              1                 0          10            3   \n",
       "2             1518              7                 0          11            5   \n",
       "3            14768              4                 0          16            0   \n",
       "4              782              1                 7          10            4   \n",
       "5              178              4                 7          10            4   \n",
       "\n",
       "            time_of_day  weekend  \n",
       "session_id                        \n",
       "1                     1        0  \n",
       "2                     1        1  \n",
       "3                     2        0  \n",
       "4                     1        0  \n",
       "5                     1        0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_for_vw_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим функцию к обучающей и тестовой выборкам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_for_vw, y_for_vw = feature_engineering_df_vw(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_for_vw = feature_engineering_df_vw(test_df, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "      <th>#unique_sites</th>\n",
       "      <th>session_timespan</th>\n",
       "      <th>start_hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>weekend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>941</td>\n",
       "      <td>3847</td>\n",
       "      <td>941</td>\n",
       "      <td>942</td>\n",
       "      <td>3846</td>\n",
       "      <td>3847</td>\n",
       "      <td>3846</td>\n",
       "      <td>1516</td>\n",
       "      <td>1518</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>39</td>\n",
       "      <td>14768</td>\n",
       "      <td>14769</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>177</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>175</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "1             718      0      0      0      0      0      0      0      0   \n",
       "2             890    941   3847    941    942   3846   3847   3846   1516   \n",
       "3           14769     39  14768  14769     37     39  14768  14768  14768   \n",
       "4             782    782    782    782    782    782    782    782    782   \n",
       "5              22    177    175    178    177    178    175    177    177   \n",
       "\n",
       "            site10  #unique_sites  session_timespan  start_hour  day_of_week  \\\n",
       "session_id                                                                     \n",
       "1                0              1                 0          10            3   \n",
       "2             1518              7                 0          11            5   \n",
       "3            14768              4                 0          16            0   \n",
       "4              782              1                 7          10            4   \n",
       "5              178              4                 7          10            4   \n",
       "\n",
       "            time_of_day  weekend  \n",
       "session_id                        \n",
       "1                     1        0  \n",
       "2                     1        1  \n",
       "3                     2        0  \n",
       "4                     1        0  \n",
       "5                     1        0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_for_vw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "      <th>#unique_sites</th>\n",
       "      <th>session_timespan</th>\n",
       "      <th>start_hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>weekend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>35</td>\n",
       "      <td>22</td>\n",
       "      <td>321</td>\n",
       "      <td>23</td>\n",
       "      <td>2211</td>\n",
       "      <td>6730</td>\n",
       "      <td>21</td>\n",
       "      <td>44582</td>\n",
       "      <td>15336</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>1445</td>\n",
       "      <td>1445</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1023</td>\n",
       "      <td>1022</td>\n",
       "      <td>50</td>\n",
       "      <td>222</td>\n",
       "      <td>202</td>\n",
       "      <td>3374</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>3374</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>301</td>\n",
       "      <td>301</td>\n",
       "      <td>301</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>68</td>\n",
       "      <td>71</td>\n",
       "      <td>167</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "1              29     35     22    321     23   2211   6730     21  44582   \n",
       "2             782    782    782    782    782    782    782    782    782   \n",
       "3              55     55     55     55     55     55     55     55   1445   \n",
       "4            1023   1022     50    222    202   3374     50     48     48   \n",
       "5             301    301    301     66     67     69     70     68     71   \n",
       "\n",
       "            site10  #unique_sites  session_timespan  start_hour  day_of_week  \\\n",
       "session_id                                                                     \n",
       "1            15336             10                 0          11            5   \n",
       "2              782              1                 2          11            3   \n",
       "3             1445              2                 2          15            4   \n",
       "4             3374              7                 0          10            1   \n",
       "5              167              8                 0          15            4   \n",
       "\n",
       "            time_of_day  weekend  \n",
       "session_id                        \n",
       "1                     1        1  \n",
       "2                     1        0  \n",
       "3                     2        0  \n",
       "4                     1        0  \n",
       "5                     2        0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_for_vw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Реализуем функцию, `dataframe_to_vw`, переводящую обучающую или тестовую выборку в формат Vowpal Wabbit.**\n",
    "\n",
    "Вход:\n",
    " - df – датафрейм `DataFrame` (обучающая выборка)\n",
    " - y (необяз.) - вектор ответов (`NumPy`). Необязателен, поскольку тестовую матрицу будем обрабатывать этой же функцией\n",
    " - train – флаг, True в случае обучающей выборки, False – в случае тестовой выборки\n",
    " - out_file – путь к файлу .vw, в который будет произведена запись\n",
    " \n",
    "Детали:\n",
    "- надо пройтись по всем строкам датафрейма `df` и записать через пробел все значения, предварительно добавив вперед нужную метку класса из вектора `y` и знак-разделитель `|`\n",
    "- в тестовой выборке на месте меток целевого класса можно писать произвольные, допустим, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataframe_to_vw(df, y=None, out_file=os.path.join(PATH_TO_DATA,'tmp.vw')):\n",
    "    df = df.astype(str)\n",
    "    sites = ['site%d' % i for i in range(1, 11)]\n",
    "    if y is None:\n",
    "        y = ['1'] * df.shape[0]\n",
    "    with open(out_file, 'w') as vw_data:\n",
    "        for (_, row), target in zip(df.iterrows(), y):\n",
    "            sample = str(target) + \\\n",
    "                    ' |a_sites ' + ' '.join([row[site] for site in sites if row[site] != '0']) + \\\n",
    "                    ''.join([' |' + col + ' ' + row[col] for col in row.index if col not in sites]) + \\\n",
    "                    '\\n'\n",
    "            vw_data.write(sample)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe_to_vw(train_for_vw_testing, y, out_file=os.path.join(PATH_TO_DATA,'tmp.vw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 |a_sites 718 |#unique_sites 1 |session_timespan 0 |start_hour 10 |day_of_week 3 |time_of_day 1 |weekend 0\n",
      "-1 |a_sites 890 941 3847 941 942 3846 3847 3846 1516 1518 |#unique_sites 7 |session_timespan 0 |start_hour 11 |day_of_week 5 |time_of_day 1 |weekend 1\n",
      "-1 |a_sites 14769 39 14768 14769 37 39 14768 14768 14768 14768 |#unique_sites 4 |session_timespan 0 |start_hour 16 |day_of_week 0 |time_of_day 2 |weekend 0\n"
     ]
    }
   ],
   "source": [
    "!head -3 $PATH_TO_DATA/tmp.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё корректно.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Валидация по отложенной выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим обучающую (70%) и отложенную (30%) части исходной обучающей выборки. Данные не перемешиваем, учитываем, что сессии отсортированы по времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_share = int(.7 * train_for_vw.shape[0])\n",
    "train_for_vw_part = train_for_vw.iloc[:train_share, :]\n",
    "valid_for_vw = train_for_vw.iloc[train_share:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_part_for_vw = y_for_vw[:train_share]\n",
    "y_valid_for_vw = y_for_vw[train_share:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним вектора целевых функций, это может потребоваться в дальнейшем при **валидации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(PATH_TO_DATA,'vw_y_for_vw.npy'), y_for_vw)\n",
    "np.save(os.path.join(PATH_TO_DATA,'vw_y_train_part_for_vw.npy'), y_train_part_for_vw)\n",
    "np.save(os.path.join(PATH_TO_DATA,'vw_y_valid_for_vw.npy'), y_valid_for_vw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Применим написанную функцию к части обучащей выборки (train_df_part, y_train_part_for_vw), к отложенной выборке (valid_df, y_valid_for_vw), ко всей обучающей выборке и ко всей тестовой выборке - всего 4 вызова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataframe_to_vw(train_for_vw_part, y_train_part_for_vw, out_file=os.path.join(PATH_TO_DATA,'vw_train_part.vw'))\n",
    "dataframe_to_vw(valid_for_vw, y_valid_for_vw, out_file=os.path.join(PATH_TO_DATA,'vw_valid_part.vw'))\n",
    "dataframe_to_vw(train_for_vw, y_for_vw, out_file=os.path.join(PATH_TO_DATA,'vw_train.vw'))\n",
    "dataframe_to_vw(test_for_vw, out_file=os.path.join(PATH_TO_DATA,'vw_test.vw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 |a_sites 3000 32 33 35 3000 5302 23 21 5302 21 |#unique_sites 7 |session_timespan 3 |start_hour 13 |day_of_week 2 |time_of_day 2 |weekend 0\n",
      "-1 |a_sites 21 39 676 812 23 679 570 814 21 39 |#unique_sites 8 |session_timespan 1 |start_hour 11 |day_of_week 1 |time_of_day 1 |weekend 0\n",
      "-1 |a_sites 786 786 782 782 782 786 786 782 782 782 |#unique_sites 2 |session_timespan 1 |start_hour 8 |day_of_week 3 |time_of_day 1 |weekend 0\n"
     ]
    }
   ],
   "source": [
    "!head -3 $PATH_TO_DATA/vw_valid_part.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучим модель Vowpal Wabbit на выборке `vw_train_part.vw`. Укажем, что решается задача бинарной классификации (`--binary`), сделаем 3 прохода по выборке (`--passes`). Поскольку для вычисления `roc_auc` нам потребуются вероятности, то функцию потерь (параметр `--loss_function`) возьмем `logistic`. Зададим некоторый кэш-файл (флаг `-c`), так VW будет быстрее делать все следующие после первого проходы по выборке (прошлый кэш-файл удалим с помощью аргумента `-k`). Также укажите значение параметра `b`=26. Это число бит, используемых для хэширования, попробуем больше, чем 18 по умолчанию. Укажите `random_seed`=17. Остальные параметры пока оставим по умолчанию, далее, возможно, имеет смысл попробовать другие функции потерь.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.29 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "final_regressor = data/vw_model_bin1.vw\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = data/vw_train_part.vw.cache\n",
      "Reading datafile = data/vw_train_part.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  -1.0000  -1.0000        8\n",
      "0.000000 0.000000            2            2.0  -1.0000  -1.0000       17\n",
      "0.000000 0.000000            4            4.0  -1.0000  -1.0000       17\n",
      "0.000000 0.000000            8            8.0  -1.0000  -1.0000       17\n",
      "0.000000 0.000000           16           16.0  -1.0000  -1.0000       17\n",
      "0.000000 0.000000           32           32.0  -1.0000  -1.0000       17\n",
      "0.000000 0.000000           64           64.0  -1.0000  -1.0000       17\n",
      "0.000000 0.000000          128          128.0  -1.0000  -1.0000       17\n",
      "0.003906 0.007813          256          256.0  -1.0000  -1.0000       17\n",
      "0.003906 0.003906          512          512.0  -1.0000  -1.0000       17\n",
      "0.006836 0.009766         1024         1024.0  -1.0000  -1.0000       17\n",
      "0.005859 0.004883         2048         2048.0  -1.0000  -1.0000       17\n",
      "0.007080 0.008301         4096         4096.0  -1.0000  -1.0000       17\n",
      "0.008667 0.010254         8192         8192.0  -1.0000  -1.0000       17\n",
      "0.008484 0.008301        16384        16384.0  -1.0000  -1.0000       17\n",
      "0.009216 0.009949        32768        32768.0  -1.0000  -1.0000       17\n",
      "0.008286 0.007355        65536        65536.0  -1.0000  -1.0000       17\n",
      "0.007729 0.007172       131072       131072.0  -1.0000  -1.0000       17\n",
      "0.007347 0.007347       262144       262144.0  -1.0000  -1.0000       17 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 159743\n",
      "passes used = 3\n",
      "weighted example sum = 479229.000000\n",
      "weighted label sum = -470517.000000\n",
      "average loss = 0.006874 h\n",
      "best constant = -4.691493\n",
      "best constant's loss = 0.051775\n",
      "total feature number = 7914627\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw --binary -d $PATH_TO_DATA/vw_train_part.vw --passes 3 -c -k --loss_function logistic \\\n",
    "-f $PATH_TO_DATA/vw_model_bin1.vw -b 26 --random_seed 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем предсказание и запишем прогнозы в **`vw_bin_train_part1.vw`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 764 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = data/vw_valid_bin_pred1.csv\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/vw_valid_part.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "9.543870 9.543870            1            1.0  -1.0000  -4.0893       17\n",
      "125.824303 242.104736            2            2.0  -1.0000 -16.5597       17\n",
      "168.263796 210.703289            4            4.0  -1.0000 -12.0331       17\n",
      "159.823450 151.383103            8            8.0  -1.0000 -11.2454       17\n",
      "130.448373 101.073297           16           16.0  -1.0000 -11.2036       17\n",
      "140.540938 150.633502           32           32.0  -1.0000  -7.7768       17\n",
      "140.709853 140.878769           64           64.0  -1.0000 -10.0703       12\n",
      "141.077516 141.445178          128          128.0  -1.0000 -11.6037       17\n",
      "160.197752 179.317989          256          256.0  -1.0000 -20.5312       17\n",
      "171.764806 183.331860          512          512.0  -1.0000 -17.7864       17\n",
      "169.899550 168.034294         1024         1024.0  -1.0000 -22.6130       17\n",
      "169.889586 169.879622         2048         2048.0  -1.0000 -21.2035       17\n",
      "169.891023 169.892461         4096         4096.0  -1.0000  -6.1336       17\n",
      "170.296363 170.701702         8192         8192.0  -1.0000 -12.0736       17\n",
      "170.964194 171.632025        16384        16384.0  -1.0000 -18.1700       17\n",
      "168.732449 166.500705        32768        32768.0  -1.0000 -12.9648       17\n",
      "168.568497 168.404545        65536        65536.0  -1.0000 -12.9090       17\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 76069\n",
      "passes used = 1\n",
      "weighted example sum = 76069.000000\n",
      "weighted label sum = -74685.000000\n",
      "average loss = 169.053846\n",
      "best constant = -0.981806\n",
      "best constant's loss = 0.036057\n",
      "total feature number = 1256496\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -t -i $PATH_TO_DATA/vw_model_bin1.vw \\\n",
    "-d $PATH_TO_DATA/vw_valid_part.vw \\\n",
    "-p $PATH_TO_DATA/vw_valid_bin_pred1.csv --random_seed 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем предсказания и посчитаем **roc_auc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vw_valid_bin_pred1 = np.loadtxt(os.path.join(PATH_TO_DATA,'vw_valid_bin_pred1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98484476413398214"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_valid_for_vw, vw_valid_bin_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем посылку на **Kaggle** по результатам работы **Vowpal Wabbit**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for Kaggle submission\n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ПОСЫЛКА НА Kaggle\n",
    "\n",
    "# обучем на всей обучающей выборке\n",
    "!vw --binary -d $PATH_TO_DATA/vw_train.vw --passes 3 -c -k --loss_function logistic \\\n",
    "-f $PATH_TO_DATA/vw_model_bin_train_all1.vw -b 26 --random_seed 17 --quiet\n",
    "\n",
    "# предсказываем на тесте\n",
    "!vw -t -i $PATH_TO_DATA/vw_model_bin_train_all1.vw \\\n",
    "-d $PATH_TO_DATA/vw_test.vw \\\n",
    "-p $PATH_TO_DATA/vw_test_bin_pred1.csv --random_seed 17 --quiet\n",
    "\n",
    "# считываем предсказания\n",
    "vw_test_bin_pred1 = np.loadtxt(os.path.join(PATH_TO_DATA,'vw_test_bin_pred1.csv'))\n",
    "\n",
    "# формируем файл посылки на Kaggle\n",
    "write_to_submission_file(vw_test_bin_pred1, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_vw_logistic1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.94275**.    \n",
    "В общем то для начальной посылки неплохо, если учесть, что мы еще не подбирали параметры. А если учесть** небольшое** время работы алгоритма - то плюсы очевидны!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 этап: Делаем подбор некоторых параметров вручную"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства скомпануем код **более компактно**, сформировав некий **блок валидации** для **тестирования** некоторых параметров, вывод текущей информации тоже уберем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# при необходимости считаем вектор ответов для валидации\n",
    "y_valid_for_vw = np.load(os.path.join(PATH_TO_DATA,'vw_y_valid_for_vw.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc: 0.987711989314\n",
      "Wall time: 9.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# БЛОК ВАЛИДАЦИИ\n",
    "\n",
    "# обучение на train_part\n",
    "!vw --binary -d $PATH_TO_DATA/vw_train_part.vw --passes 9 -c -k --loss_function logistic \\\n",
    "-f $PATH_TO_DATA/vw_model_bin_tmp.vw -b 26 --random_seed 17 --quiet -q as\n",
    "\n",
    "# предсказание на valid_part\n",
    "!vw -t -i $PATH_TO_DATA/vw_model_bin_tmp.vw \\\n",
    "-d $PATH_TO_DATA/vw_valid_part.vw \\\n",
    "-p $PATH_TO_DATA/vw_valid_bin_pred_tmp.csv --random_seed 17 --quiet\n",
    "\n",
    "# считаем предсказания и вычислим roc_auc\n",
    "vw_valid_bin_pred_tmp = np.loadtxt(os.path.join(PATH_TO_DATA,'vw_valid_bin_pred_tmp.csv'))\n",
    "print('roc_auc:', roc_auc_score(y_valid_for_vw, vw_valid_bin_pred_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем вручную параметры:\n",
    "- **--passes**: улучшение есть (при увеличении), но повышать больше **9** смысла уже не имеет **[+]**\n",
    "- **--loss_function**: **hinge** дает результат не очень хороший, оставляем **logistic**\n",
    "- **-b** 28 сильно **перегружает** ресурсы и особенно память, а результат не увеличивает. Оставляем **26**\n",
    "- **--ngram**: улучшения нет\n",
    "- **-q**: as - немного улучшило\n",
    "\n",
    "На этом ручной подбор в некотором роде исчерпан, далее будем подбирать с помощью библиотеки **hyperopt**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 этап: подбор гиперпараметров с помощью модуля `hyperopt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем нужные элементы библиотеки **hyperopt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def hyperopt_objective(params):\n",
    "    # обучение на train_part\n",
    "    !vw --binary -d $PATH_TO_DATA/vw_train_part.vw --passes 9 -c -k --loss_function logistic \\\n",
    "    -f $PATH_TO_DATA/vw_model_bin_tmp.vw -b 26 --random_seed 17 --quiet -q as \\\n",
    "    --learning_rate {params['learning_rate']} \\\n",
    "    --power_t {params['power_t']} \\\n",
    "    --initial_t {params['initial_t']} \\\n",
    "    --l1 {params['l1']} \\\n",
    "    --l2 {params['l2']}\n",
    "    \n",
    "    # предсказание на valid_part\n",
    "    !vw -t -i $PATH_TO_DATA/vw_model_bin_tmp.vw \\\n",
    "    -d $PATH_TO_DATA/vw_valid_part.vw \\\n",
    "    -p $PATH_TO_DATA/vw_valid_bin_pred_tmp.csv --random_seed 17 --quiet\n",
    "      \n",
    "    # scoring roc_auc\n",
    "    vw_valid_bin_pred_tmp = np.loadtxt(os.path.join(PATH_TO_DATA,'vw_valid_bin_pred_tmp.csv'))\n",
    "    roc_auc_scoring = roc_auc_score(y_valid_for_vw, vw_valid_bin_pred_tmp)\n",
    "    \n",
    "    return 1 - roc_auc_scoring\n",
    "\n",
    "# задаем диапазоны подбора параметров\n",
    "params_space = {\n",
    "    'learning_rate': hp.uniform('learning_rate', 0., 2.0), # default = 0.5\n",
    "    'power_t': hp.uniform('power_t', 0., 2.0), # default = 0.5\n",
    "    'initial_t': hp.loguniform('initial_t', -100, 1), # default = 0\n",
    "    'l1': hp.loguniform('l1', -100, -10), # default = 0\n",
    "    'l2': hp.loguniform('l2', -100, -10) # default = 0\n",
    "                }\n",
    "\n",
    "# запускаем подбор\n",
    "trials = Trials()\n",
    "\n",
    "best_params = fmin(\n",
    "    hyperopt_objective,\n",
    "    space=params_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'initial_t': 1.1505114803794776e-29,\n",
       " 'l1': 1.8335143745961346e-31,\n",
       " 'l2': 8.016379503987476e-29,\n",
       " 'learning_rate': 0.42659194007671675,\n",
       " 'power_t': 0.3974167436688515}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим подобранные параметры на отложенной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc: 0.988443332364\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# БЛОК ВАЛИДАЦИИ\n",
    "\n",
    "# обучение на train_part\n",
    "!vw --binary -d $PATH_TO_DATA/vw_train_part.vw --passes 9 -c -k --loss_function logistic \\\n",
    "-f $PATH_TO_DATA/vw_model_bin_tmp.vw -b 26 --random_seed 17 --quiet -q as \\\n",
    "--learning_rate {best_params['learning_rate']} \\\n",
    "--power_t {best_params['power_t']} \\\n",
    "--initial_t {best_params['initial_t']} \\\n",
    "--l1 {best_params['l1']} \\\n",
    "--l2 {best_params['l2']}\n",
    "\n",
    "# предсказание на valid_part\n",
    "!vw -t -i $PATH_TO_DATA/vw_model_bin_tmp.vw \\\n",
    "-d $PATH_TO_DATA/vw_valid_part.vw \\\n",
    "-p $PATH_TO_DATA/vw_valid_bin_pred_tmp.csv --quiet\n",
    "\n",
    "# считаем предсказания и вычислим roc_auc\n",
    "vw_valid_bin_pred_tmp = np.loadtxt(os.path.join(PATH_TO_DATA,'vw_valid_bin_pred_tmp.csv'))\n",
    "print('roc_auc:', roc_auc_score(y_valid_for_vw, vw_valid_bin_pred_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидалось, подбор параметров дал свой результат и метрика **roc_auc** на **валидации** заметно выросла.    \n",
    "\n",
    "Сделаем обучение/предсказание на **всей** обучающей выборке и сделаем посылку на **Kaggle**. Практика показывает, что улучшение метрики на валидации не всегда приводит к улучшению метрики на **Kaggle**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ПОСЫЛКА НА Kaggle\n",
    "\n",
    "# обучем на всей обучающей выборке\n",
    "!vw --binary -d $PATH_TO_DATA/vw_train.vw --passes 9 -c -k --loss_function logistic \\\n",
    "-f $PATH_TO_DATA/vw_model_bin_train_param.vw -b 26 --random_seed 17 --quiet -q as \\\n",
    "--learning_rate {best_params['learning_rate']} \\\n",
    "--power_t {best_params['power_t']} \\\n",
    "--initial_t {best_params['initial_t']} \\\n",
    "--l1 {best_params['l1']} \\\n",
    "--l2 {best_params['l2']}\n",
    "\n",
    "# предсказываем на тесте\n",
    "!vw -t -i $PATH_TO_DATA/vw_model_bin_train_param.vw \\\n",
    "-d $PATH_TO_DATA/vw_test.vw \\\n",
    "-p $PATH_TO_DATA/vw_model_bin_train_param_pred.csv --quiet\n",
    "\n",
    "# считываем предсказания\n",
    "vw_model_bin_train_param_pred = np.loadtxt(os.path.join(PATH_TO_DATA,'vw_model_bin_train_param_pred.csv'))\n",
    "\n",
    "# формируем файл посылки на Kaggle\n",
    "write_to_submission_file(vw_model_bin_train_param_pred, os.path.join(PATH_TO_DATA, \n",
    "                                                                     '[YDF & MIPT]_Coursera_vw_logistic_hyperopt2.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle:** 0.93547"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы увидели, результат на **Kaggle** даже немного уменьшился.    \n",
    "\n",
    "**РЕЗЮМЕ:**   \n",
    "- **Vopal Wabbit** - достаточно эффективная и быстрая библиотека\n",
    "- **hyperopt** - удобный модуль для подбора гиперпараметров"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
