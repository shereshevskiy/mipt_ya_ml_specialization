{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://habrastorage.org/web/677/8e1/337/6778e1337c3d4b159d7e99df94227cb2.jpg\"/>    \n",
    "<table>\n",
    "    <th><font size=5>Специализация \"Машинное обучение и анализ данных\" от:</font></th>\n",
    "    <th><img src=\"https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://d15cw65ipctsrr.cloudfront.net/11/ae0000b18911e5965623dd71776f15/mipt.png?auto=format%2Ccompress&dpr=1&w=200&h=100&fit=clip\"/> </th>\n",
    "    <th><img src=\"https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://d15cw65ipctsrr.cloudfront.net/fe/ab6f503b2b11e796ddbfacaf40b9e6/Yandex.jpg?auto=format%2Ccompress&dpr=1&w=200&h=100&fit=clip\"/></th>\n",
    "</table>\n",
    "\n",
    "# <center> ПРОЕКТ: Идентификация пользователей по посещенным веб-страницам\n",
    "# <center> АВТОР ПРОЕКТА: Дмитрий Шерешевский, *PhD*\n",
    "##    \n",
    "<center>Исходный материал: программист-исследователь Mail.ru Group, старший преподаватель Факультета Компьютерных Наук ВШЭ Юрий Кашницкий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://smartcity.eletsonline.com/wp-content/uploads/2017/04/digitalindia.png'>\n",
    "\n",
    "# <center>Часть 5 из 6.  \n",
    "# <center>Соревнование Kaggle \"Catch Me If You Can\"\n",
    "## <center>Введение\n",
    "\n",
    "В этой части мы вспомним про концепцию стохастического градиентного спуска и опробуем классификатор Scikit-learn SGDClassifier, который работает намного быстрее на больших выборках, чем алгоритмы, которые мы тестировали в 4 части. Также мы познакомимся с данными [соревнования](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) Kaggle по идентификации пользователей и сделаем в нем свои посылки. Попробуем добиться хорошего результата.\n",
    "\n",
    "**Используемые инструменты:**\n",
    "   - [Стохатический градиентный спуск](https://www.coursera.org/learn/supervised-learning/lecture/xRY50/stokhastichieskii-ghradiientnyi-spusk)\n",
    "   - [Линейные модели. Sklearn.linear_model. Классификация](https://www.coursera.org/learn/supervised-learning/lecture/EBg9t/linieinyie-modieli-sklearn-linear-model-klassifikatsiia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 5.1. Получение результата и подготовка первой посылки на Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считаем данные [соревнования](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) в DataFrame train_df и test_df (обучающая и тестовая выборки).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Путь к данным\n",
    "PATH_TO_DATA = ('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n",
    "                       index_col='session_id')\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n",
    "                      index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>2014-02-20 10:02:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>942.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>2014-02-22 11:20:15</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>2014-02-22 11:20:16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>2013-12-16 16:40:17</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013-12-16 16:40:18</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14769.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:20</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:21</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:22</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:52:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:52:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:54:12</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-28 10:54:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>2014-02-28 10:53:05</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:57:06</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:57:11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1  site2                time2    site3  \\\n",
       "session_id                                                                    \n",
       "1             718  2014-02-20 10:02:45    NaN                  NaN      NaN   \n",
       "2             890  2014-02-22 11:19:50  941.0  2014-02-22 11:19:50   3847.0   \n",
       "3           14769  2013-12-16 16:40:17   39.0  2013-12-16 16:40:18  14768.0   \n",
       "4             782  2014-03-28 10:52:12  782.0  2014-03-28 10:52:42    782.0   \n",
       "5              22  2014-02-28 10:53:05  177.0  2014-02-28 10:55:22    175.0   \n",
       "\n",
       "                          time3    site4                time4  site5  \\\n",
       "session_id                                                             \n",
       "1                           NaN      NaN                  NaN    NaN   \n",
       "2           2014-02-22 11:19:51    941.0  2014-02-22 11:19:51  942.0   \n",
       "3           2013-12-16 16:40:19  14769.0  2013-12-16 16:40:19   37.0   \n",
       "4           2014-03-28 10:53:12    782.0  2014-03-28 10:53:42  782.0   \n",
       "5           2014-02-28 10:55:22    178.0  2014-02-28 10:55:23  177.0   \n",
       "\n",
       "                          time5  ...                  time6    site7  \\\n",
       "session_id                       ...                                   \n",
       "1                           NaN  ...                    NaN      NaN   \n",
       "2           2014-02-22 11:19:51  ...    2014-02-22 11:19:51   3847.0   \n",
       "3           2013-12-16 16:40:19  ...    2013-12-16 16:40:19  14768.0   \n",
       "4           2014-03-28 10:54:12  ...    2014-03-28 10:54:42    782.0   \n",
       "5           2014-02-28 10:55:23  ...    2014-02-28 10:55:59    175.0   \n",
       "\n",
       "                          time7    site8                time8    site9  \\\n",
       "session_id                                                               \n",
       "1                           NaN      NaN                  NaN      NaN   \n",
       "2           2014-02-22 11:19:52   3846.0  2014-02-22 11:19:52   1516.0   \n",
       "3           2013-12-16 16:40:20  14768.0  2013-12-16 16:40:21  14768.0   \n",
       "4           2014-03-28 10:55:12    782.0  2014-03-28 10:55:42    782.0   \n",
       "5           2014-02-28 10:55:59    177.0  2014-02-28 10:55:59    177.0   \n",
       "\n",
       "                          time9   site10               time10 target  \n",
       "session_id                                                            \n",
       "1                           NaN      NaN                  NaN      0  \n",
       "2           2014-02-22 11:20:15   1518.0  2014-02-22 11:20:16      0  \n",
       "3           2013-12-16 16:40:22  14768.0  2013-12-16 16:40:24      0  \n",
       "4           2014-03-28 10:56:12    782.0  2014-03-28 10:56:42      0  \n",
       "5           2014-02-28 10:57:06    178.0  2014-02-28 10:57:11      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Объединим обучающую и тестовую выборки – это понадобится, чтоб вместе потом привести их к разреженному формату.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающей выборке видим следующие признаки:\n",
    "    - site1 – индекс первого посещенного сайта в сессии\n",
    "    - time1 – время посещения первого сайта в сессии\n",
    "    - ...\n",
    "    - site10 – индекс 10-го посещенного сайта в сессии\n",
    "    - time10 – время посещения 10-го сайта в сессии\n",
    "    - user_id – ID пользователя\n",
    "    \n",
    "Сессии пользователей выделены таким образом, что они не могут быть длинее получаса или 10 сайтов. То есть сессия считается оконченной либо когда пользователь посетил 10 сайтов подряд, либо когда сессия заняла по времени более 30 минут. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на статистику признаков.**\n",
    "\n",
    "Пропуски возникают там, где сессии короткие (менее 10 сайтов). Скажем, если человек 1 января 2015 года посетил *vk.com* в 20:01, потом *yandex.ru* в 20:29, затем *google.com* в 20:33, то первая его сессия будет состоять только из двух сайтов (site1 – ID сайта *vk.com*, time1 – 2015-01-01 20:01:00, site2 – ID сайта  *yandex.ru*, time2 – 2015-01-01 20:29:00, остальные признаки – NaN), а начиная с *google.com* пойдет новая сессия, потому что уже прошло более 30 минут с момента посещения *vk.com*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В обучающей выборке – 2297 сессий одного пользователя (Alice) и 251264 сессий – других пользователей, не Элис. Дисбаланс классов очень сильный, и смотреть на долю верных ответов (accuracy) непоказательно.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    251264\n",
       "1      2297\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пока для прогноза будем использовать только индексы посещенных сайтов. Индексы нумеровались с 1, так что заменим пропуски на нули.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df_sites = train_test_df[['site%d' % i for i in range(1, 11)]].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>941</td>\n",
       "      <td>3847</td>\n",
       "      <td>941</td>\n",
       "      <td>942</td>\n",
       "      <td>3846</td>\n",
       "      <td>3847</td>\n",
       "      <td>3846</td>\n",
       "      <td>1516</td>\n",
       "      <td>1518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>39</td>\n",
       "      <td>14768</td>\n",
       "      <td>14769</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>177</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>175</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>570</td>\n",
       "      <td>21</td>\n",
       "      <td>570</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>803</td>\n",
       "      <td>23</td>\n",
       "      <td>5956</td>\n",
       "      <td>17513</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>803</td>\n",
       "      <td>17514</td>\n",
       "      <td>17514</td>\n",
       "      <td>17514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>5041</td>\n",
       "      <td>14422</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>5041</td>\n",
       "      <td>14421</td>\n",
       "      <td>14421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>668</td>\n",
       "      <td>940</td>\n",
       "      <td>942</td>\n",
       "      <td>941</td>\n",
       "      <td>941</td>\n",
       "      <td>942</td>\n",
       "      <td>940</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3700</td>\n",
       "      <td>229</td>\n",
       "      <td>570</td>\n",
       "      <td>21</td>\n",
       "      <td>229</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>2336</td>\n",
       "      <td>2044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "1             718      0      0      0      0      0      0      0      0   \n",
       "2             890    941   3847    941    942   3846   3847   3846   1516   \n",
       "3           14769     39  14768  14769     37     39  14768  14768  14768   \n",
       "4             782    782    782    782    782    782    782    782    782   \n",
       "5              22    177    175    178    177    178    175    177    177   \n",
       "6             570     21    570     21     21      0      0      0      0   \n",
       "7             803     23   5956  17513     37     21    803  17514  17514   \n",
       "8              22     21     29   5041  14422     23     21   5041  14421   \n",
       "9             668    940    942    941    941    942    940     23     21   \n",
       "10           3700    229    570     21    229     21     21     21   2336   \n",
       "\n",
       "            site10  \n",
       "session_id          \n",
       "1                0  \n",
       "2             1518  \n",
       "3            14768  \n",
       "4              782  \n",
       "5              178  \n",
       "6                0  \n",
       "7            17514  \n",
       "8            14421  \n",
       "9               22  \n",
       "10            2044  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df_sites.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создадим разреженные матрицы *X_train_sparse* и *X_test_sparse* аналогично тому, как мы это делали ранее. Используем объединенную матрицу *train_test_df_sites*, потом разделим обратно на обучающую и тестовую части.**\n",
    "\n",
    "Обратим внимание на то, что в  сессиях меньше 10 сайтов  у нас остались нули, так что первый признак (сколько раз попался 0) по смыслу отличен от остальных (сколько раз попался сайт с индексом $i$). Поэтому первый столбец разреженной матрицы надо будет удалить.\n",
    "\n",
    "**Выделим в отдельный вектор *y* ответы на обучающей выборке.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_sparse = csr_matrix((np.ones(train_test_df_sites.values.size), \n",
    "                                train_test_df_sites.values.ravel(), \n",
    "                                np.arange(train_test_df_sites.values.shape[0] + 1) * \n",
    "                                train_test_df_sites.values.shape[1]), dtype=int)[:, 1:]\n",
    "\n",
    "X_train_sparse = train_test_sparse[:train_df.shape[0]]\n",
    "X_test_sparse = train_test_sparse[train_df.shape[0]:]\n",
    "y = train_df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Для контроля выведем размерности матриц *X_train_sparse* и *X_test_sparse*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 48371), (82797, 48371))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохраним в pickle-файлы объекты *X_train_sparse*, *X_test_sparse* и *y* (последний – в файл *train_target.pkl*).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'X_train_sparse.pkl'), 'wb') as X_train_sparse_pkl:\n",
    "    pickle.dump(X_train_sparse, X_train_sparse_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'X_test_sparse.pkl'), 'wb') as X_test_sparse_pkl:\n",
    "    pickle.dump(X_test_sparse, X_test_sparse_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'train_target.pkl'), 'wb') as train_target_pkl:\n",
    "    pickle.dump(y, train_target_pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разобьем обучающую выборку на 2 части в пропорции 7/3, причем не перемешивая. Исходные данные упорядочены по времени, тестовая выборка по времени четко отделена от обучающей, это же соблюдем и здесь.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_share = int(.7 * X_train_sparse.shape[0])\n",
    "X_train, y_train = X_train_sparse[:train_share, :], y[:train_share]\n",
    "X_valid, y_valid  = X_train_sparse[train_share:, :], y[train_share:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Создадим объект `sklearn.linear_model.SGDClassifier` с логистической функцией потерь и параметром *random_state*=17. Остальные параметры оставим по умолчанию, разве что *n_jobs*=-1 никогда не помешает. Обучим  модель на выборке `(X_train, y_train)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=None, n_iter=None,\n",
       "       n_jobs=-1, penalty='l2', power_t=0.5, random_state=17, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "sgd_logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделаем прогноз в виде предсказанных вероятностей того, что это сессия Элис, на отложенной выборке *(X_valid, y_valid)*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Напечатаем ROC AUC логистической регрессии, обученной с помощью стохастического градиентного спуска, на отложенной выборке, округлив до 3 знаков после разделителя.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC = 0.934\n"
     ]
    }
   ],
   "source": [
    "print('ROC AUC =', round(roc_auc_score(y_valid, logit_valid_pred_proba[:, 1]), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделаем прогноз в виде предсказанных вероятностей отнесения к классу 1 для тестовой выборки с помощью той же *sgd_logit*, обученной уже на всей обучающей выборке (а не на 70%).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 676 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "logit_test_pred_proba = sgd_logit.fit(X_train_sparse, y).predict_proba(X_test_sparse)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишем ответы в файл и сделайте посылку на Kaggle. Дадим своей команде (из одного человека) на Kaggle название –**    \n",
    "### [YDF & MIPT] Dmitry Shereshevskiy   \n",
    "**чтобы можно было легко идентифицировать посылки на [лидерборде](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/leaderboard/public).**\n",
    "\n",
    "**Результат, который мы только что получили, соответствует бейзлайну \"SGDCLassifer\" на лидерборде. Поставим себе задачу - максимально улучшить его.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(logit_test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_logit.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Пути улучшения\n",
    "Что можно попробовать:\n",
    " - Использовать ранее построенные признаки для улучшения модели (проверить их можно на меньшей выборке по 150 пользователям, отделив одного из пользователей от остальных – это быстрее)\n",
    " - Настроить параметры моделей (например, коэффициенты регуляризации)\n",
    " - Если позволят мощности (или хватит терпения), можно попробовать смешивание (блендинг) ответов бустинга и линейной модели. [Вот](http://mlwave.com/kaggle-ensembling-guide/) один из самых известных тьюториалов по смешиванию ответов алгоритмов, также хороша [статья](https://alexanderdyakonov.wordpress.com/2017/03/10/cтекинг-stacking-и-блендинг-blending) Александра Дьяконова\n",
    " - Обратим внимание, что в соревновании также даны исходные данные о посещенных веб-страницах Элис и остальными 1557 пользователями (*train.zip*). По этим данным можно сформировать свою обучающую выборку. \n",
    "\n",
    "В 6 части мы разберем пакет **Vowpal Wabbit** и попробуем его в деле, на данных соревнования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>5.2. Работа по улучшению результата на Kaggle. Первые успехи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем новые **дополнительные признаки** из исходных даннных.\n",
    "\n",
    "Для этого сделаем функцию, которая принимает на вход **train_df**, а выдает **X_train_sparse** с признаками-**сайтами** в виде \"мешка слов\" и **дополнительные признаки**, а также целевой признак **y**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_df(train_df, features, train=True):\n",
    "    '''\n",
    "    features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    '''\n",
    "    # повторим, что было выше (частично)\n",
    "    train_df_sites = train_df[['site%d' % i for i in range(1, 11)]] # не забыть заполнить нули\n",
    "    \n",
    "    # подготовим временнЫе метки\n",
    "    train_df[['time%d' % i for i in range(1, 11)]] = train_df[['time%d' % i for i in range(1, 11)]].applymap(pd.to_datetime)\n",
    "    timestamp_df = train_df[['time%d' % i for i in range(1, 11)]].applymap(lambda x: pd.datetime.timestamp(x) \n",
    "                                                                  if type(x) == pd._libs.tslib.Timestamp else np.nan)\n",
    "    # добавляем признаки\n",
    "    if '#unique_sites' in features:\n",
    "        train_df['#unique_sites'] = train_df[['site' + str(i) for i in range(1, 11)]].nunique(axis=1, dropna=True)\n",
    "    if 'session_timespan' in features:\n",
    "        train_df['session_timespan'] = timestamp_df.max(axis=1, skipna=True) - timestamp_df.min(axis=1, skipna=True)\n",
    "    if 'start_hour' in features:    \n",
    "        train_df['start_hour'] = train_df['time1'].apply(lambda x: x.hour)\n",
    "    if 'day_of_week' in features:    \n",
    "        train_df['day_of_week'] = train_df['time1'].apply(lambda x: x.weekday())\n",
    "    if 'time_of_day' in features:    \n",
    "        # время суток (06-12 - утро(1), 12-19 - день(2), 19-24 - вечер(3), 00-06 - ночь(4))\n",
    "        train_df['time_of_day'] = train_df['time1'].apply(lambda x:  1 if x.hour >  6 and x.hour <= 12 else \n",
    "                                                                     2 if x.hour > 12 and x.hour <= 19 else\n",
    "                                                                     3 if x.hour > 19 and x.hour <= 24 else \n",
    "                                                                     4)\n",
    "    if 'weekend' in features:\n",
    "        # индикатор уикэнда в день начала сессии\n",
    "        train_df['weekend'] = train_df['time1'].apply(lambda x: 1 if x.weekday() in [5, 6] else 0)\n",
    "   \n",
    "    train_df_sites = train_df_sites.fillna(0).astype('int')\n",
    "    train_sparse = csr_matrix((np.ones(train_df_sites.values.size), \n",
    "                                train_df_sites.values.ravel(), \n",
    "                                np.arange(train_df_sites.values.shape[0] + 1) * \n",
    "                                train_df_sites.values.shape[1]), dtype=int)[:, 1:]\n",
    "    if train:\n",
    "        y = train_df['target'].values\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    return train_sparse, train_df[features], y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для автоматизации исследования признаков также **создадим функцию**, которая принимает на вход размеченные данные в виде **матрицы признаков** и вектора **целевой функции**, а на выходе дает метрику **roc_auc** по отложенной выборке размером **30%** от исходных данных.   \n",
    "Вообще-то это не совсем корректно и правильнее использовать для кнтроля метрики **кросс-валидацию**, но на первом этапе для \"прикидок\" этого, скорее всего, будет достаточно, а аналогичную функцию с **кросс-валидацией** сделаем позже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logit_roc_auc_score(X_train_sparse, y):\n",
    "    '''\n",
    "    обучение и расчет метрики на отложенной выборке\n",
    "    '''\n",
    "\n",
    "    train_share = int(.7 * X_train_sparse.shape[0])\n",
    "    X_train, y_train = X_train_sparse[:train_share, :], y[:train_share]\n",
    "    X_valid, y_valid  = X_train_sparse[train_share:, :], y[train_share:]\n",
    "\n",
    "    sgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "    sgd_logit.fit(X_train, y_train)\n",
    "\n",
    "    logit_valid_pred_proba = sgd_logit.predict_proba(X_valid)\n",
    "\n",
    "    return roc_auc_score(y_valid, logit_valid_pred_proba[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим **все** комбинации добавления признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 лучших метрик с фичами\n",
      "roc_auc_score: 0.934381940306 features: ['weekend']\n",
      "roc_auc_score: 0.933847286791 features: []\n",
      "roc_auc_score: 0.924980978083 features: ['day_of_week', 'weekend']\n",
      "roc_auc_score: 0.924717063844 features: ['day_of_week']\n",
      "roc_auc_score: 0.923196690838 features: ['#unique_sites', 'weekend']\n",
      "\n",
      "5 худших метрик с фичами\n",
      "roc_auc_score: 0.674627312681 features: ['session_timespan', 'day_of_week', 'weekend']\n",
      "roc_auc_score: 0.668984875716 features: ['session_timespan', 'weekend']\n",
      "roc_auc_score: 0.66837137001 features: ['session_timespan', 'time_of_day', 'weekend']\n",
      "roc_auc_score: 0.666589603811 features: ['session_timespan', 'time_of_day']\n",
      "roc_auc_score: 0.665770081657 features: ['session_timespan']\n"
     ]
    }
   ],
   "source": [
    "features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "train_sites_sparse, features_df, y_values = feature_engineering_df(train_df, features, train=True)\n",
    "\n",
    "feature_scores = []\n",
    "for r in range(len(features) + 1):\n",
    "    for fe in itertools.combinations(features, r):\n",
    "        fe_ = list(fe)\n",
    "        features_sparse = csr_matrix(features_df[fe_].values)\n",
    "  \n",
    "        X_train_sparse = sparse.hstack([train_sites_sparse , features_sparse]).tocsr()\n",
    "        \n",
    "        logit_roc_auc = logit_roc_auc_score(X_train_sparse, y_values)\n",
    "        feature_scores.append((logit_roc_auc, fe_))\n",
    "\n",
    "sorted_features = sorted(feature_scores, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print('5 лучших метрик с фичами')\n",
    "for pair in sorted_features[:5]:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])\n",
    "\n",
    "print('\\n5 худших метрик с фичами')\n",
    "for pair in sorted_features[-5:]:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫВОД:**    \n",
    "Как показал этот анализ, только добавление **одного** из всех признаков - а именно **'weekend'** - дало **увеличение** метрики **ROC AUC**.\n",
    "Остальные признаки только лишь **уменьшили** метрику **ROC AUC**.   \n",
    "Причем особенно **негативно** влияет присутствие признака **'session_timespan'**.  \n",
    "\n",
    "На первый взгляд результат может **показаться страным** - ведь интуитивно кажется, что **добавление новых признаков** должно результат **улучшать**.   \n",
    "Но если посмотреть на признаки повнимательнее, то можно увидеть, что дополнительные признаки, являясь по форме **числовыми и количественными**, по сути же, если разобраться в их природе, являются **категориальными**. Поэтому обращение с ними как с **числовыми признаками**, имеющими отношение **порядка**, и привело к ухудшению результата. \n",
    "\n",
    "Для корректного их использования каждый из них нужно **бинаризовать**. Как это правильно сделать мы поговорим ниже. Здесь же отметим, что именно признак **weekend** по своей природе уже и так **является бинарным**, поскольку принимает только 2 значения - *0* и *1*, и именно поэтому его использование в таком виде **уже** является корректным и приводит к **улучшению** результата.    \n",
    "\n",
    "Использование же других дополнительных признаков стало **демонстрацией некорректного применения** признаков. Эту ситуацию мы исправим ниже.\n",
    "#### ИТАК, на этом этапе добавляем только признак **'weekend'**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим  **'weekend'**, подготовим данные и сформируем посылку на **Kaggle**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_sites_sparse, features_df, y_train = feature_engineering_df(train_test_df, features=features, train=True)\n",
    "features_sparse = csr_matrix(features_df[['weekend']].values)\n",
    "X_train_test_sparse_newfe = sparse.hstack([train_sites_sparse , features_sparse]).tocsr()\n",
    "\n",
    "# делим на трейн и тест\n",
    "X_train_sparse_newfe = X_train_test_sparse_newfe[:train_df.shape[0]]\n",
    "X_test_sparse_newfe = X_train_test_sparse_newfe[train_df.shape[0]:]\n",
    "y_train = y_train[:train_df.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "logit_test_pred_proba = sgd_logit.fit(X_train_sparse_newfe, y_train).predict_proba(X_test_sparse_newfe)[:, 1]\n",
    "    \n",
    "write_to_submission_file(logit_test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_logit_newfe.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем соответствующую **посылку на Kaggle**.   \n",
    "Её результат:   \n",
    "- **+46** позиций вверх    \n",
    "- Score Public Leaderboard = **0.92600**   \n",
    "- Бенчмарк \"sgd_logit_benchmark.csv\" на публичной части рейтинга в соревновании Kaggle - **побит**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 5.3. Настраиваем параметры и пробуем упрощенную предобработку признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном соревновании **длина сессии** и **ширина окна** заданы жестко как **(10, 10)**, поэтому подобирать эти параметры не получается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В связи с этим будем работать с настройками SGDClassifier:\n",
    "- альфа\n",
    "- балансировка классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации временн**ы**х затрат поработаем с выборкой на 150 пользователей.\n",
    "\n",
    "Загрузим сохраненные ранее данные и сформируем выборку для работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'selected_features_150users.pkl'), 'rb') as selected_features_150users_pkl:\n",
    "    selected_features_150users = pickle.load(selected_features_150users_pkl)\n",
    "    \n",
    "with open(os.path.join(PATH_TO_DATA, 'new_features_150users.pkl'), 'rb') as new_features_150users_pkl:\n",
    "    new_features_150users = pickle.load(new_features_150users_pkl)\n",
    "    \n",
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_150users.pkl'), 'rb') as X_sparse_150users_pkl:\n",
    "    X_sparse_150users = pickle.load(X_sparse_150users_pkl)\n",
    "    \n",
    "with open(os.path.join(PATH_TO_DATA, 'y_150users.pkl'), 'rb') as y_150users_pkl:\n",
    "    y_150users = pickle.load(y_150users_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_year</th>\n",
       "      <th>start_month</th>\n",
       "      <th>start_day</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>freq_top30_0_1</th>\n",
       "      <th>freq_top30_1_3</th>\n",
       "      <th>freq_top30_2_41</th>\n",
       "      <th>freq_top30_3_2</th>\n",
       "      <th>freq_top30_4_181</th>\n",
       "      <th>freq_top30_5_214</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_top30_21_1201</th>\n",
       "      <th>freq_top30_22_6</th>\n",
       "      <th>freq_top30_23_2165</th>\n",
       "      <th>freq_top30_24_244</th>\n",
       "      <th>freq_top30_25_180</th>\n",
       "      <th>freq_top30_26_1463</th>\n",
       "      <th>freq_top30_27_106</th>\n",
       "      <th>freq_top30_28_253</th>\n",
       "      <th>freq_top30_29_2707</th>\n",
       "      <th>weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_year  start_month  start_day  time_of_day  freq_top30_0_1  \\\n",
       "0        2013           11         15            1               4   \n",
       "1        2013           11         15            1               0   \n",
       "2        2013           11         15            1               5   \n",
       "3        2013           11         15            1               4   \n",
       "4        2013           11         15            1               3   \n",
       "\n",
       "   freq_top30_1_3  freq_top30_2_41  freq_top30_3_2  freq_top30_4_181  \\\n",
       "0               2                0               2                 0   \n",
       "1               1                0               0                 0   \n",
       "2               1                0               0                 0   \n",
       "3               0                0               0                 0   \n",
       "4               0                0               0                 0   \n",
       "\n",
       "   freq_top30_5_214   ...     freq_top30_21_1201  freq_top30_22_6  \\\n",
       "0                 0   ...                      0                0   \n",
       "1                 0   ...                      0                1   \n",
       "2                 0   ...                      0                0   \n",
       "3                 0   ...                      0                0   \n",
       "4                 0   ...                      0                0   \n",
       "\n",
       "   freq_top30_23_2165  freq_top30_24_244  freq_top30_25_180  \\\n",
       "0                   0                  0                  0   \n",
       "1                   0                  0                  0   \n",
       "2                   0                  0                  0   \n",
       "3                   0                  0                  0   \n",
       "4                   0                  0                  0   \n",
       "\n",
       "   freq_top30_26_1463  freq_top30_27_106  freq_top30_28_253  \\\n",
       "0                   0                  0                  0   \n",
       "1                   0                  0                  0   \n",
       "2                   0                  0                  0   \n",
       "3                   0                  0                  0   \n",
       "4                   0                  0                  0   \n",
       "\n",
       "   freq_top30_29_2707  weekend  \n",
       "0                   0        0  \n",
       "1                   0        0  \n",
       "2                   0        0  \n",
       "3                   0        0  \n",
       "4                   0        0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_150users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((137019, 35), (137019,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_150users.shape, y_150users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<137019x27797 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 1369510 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sparse_150users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем отобранный выше признак **'weekend'** и добавим к сформированному ранее **\"мешку слов\"** с сайтами. Скорректируем также целевой признак под **бинарную** классификацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_150_newfe_sparse = csr_matrix(selected_features_150users[['weekend']].values)\n",
    "  \n",
    "X_150_train_newfe_sparse = sparse.hstack([X_sparse_150users, X_150_newfe_sparse]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_150_train = (y_150users == 128).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подбора параметров воспльзуемся функцией **GridSearchCV** из библиотеки `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "\n",
    "sgd_params = {'alpha': np.logspace(-8, 1, 40)}\n",
    "\n",
    "sgd_grid_searcher = GridSearchCV(sgd_logit, param_grid=sgd_params, scoring='roc_auc', n_jobs=-1, cv=skf)\n",
    "sgd_grid_searcher.fit(X_150_train_newfe_sparse, y_150_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 4.9238826317067415e-05} 0.971957399925\n"
     ]
    }
   ],
   "source": [
    "print(sgd_grid_searcher.best_params_, sgd_grid_searcher.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем тот же набор параметров, но **добавим балансировку классов** в **SGDClassifier()**, что логично, поскольку, как мы видели в 4 части, в исследуемой выборке есть большой дисбаланс классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "\n",
    "sgd_params = {'alpha': np.logspace(-8, 1, 40)}\n",
    "\n",
    "sgd_grid_searcher = GridSearchCV(sgd_logit, param_grid=sgd_params, scoring='roc_auc', n_jobs=-1, cv=skf)\n",
    "sgd_grid_searcher.fit(X_150_train_newfe_sparse, y_150_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.00041246263829013564} 0.977036274013\n"
     ]
    }
   ],
   "source": [
    "print(sgd_grid_searcher.best_params_, sgd_grid_searcher.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫВОД: **   \n",
    "результат, как и ожидалось, **улучшился**.    \n",
    "Обучим на этих параметрах классификатор и сформируем посылку на **Kaggle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train_sites_sparse, features_df, y_train = feature_engineering_df(train_test_df, features=features, train=True)\n",
    "# features_sparse = csr_matrix(features_df[['weekend']].values)\n",
    "# X_train_test_sparse_newfe = sparse.hstack([train_sites_sparse , features_sparse]).tocsr()\n",
    "\n",
    "# делим на трейн и тест\n",
    "X_train_sparse_newfe = X_train_test_sparse_newfe[:train_df.shape[0]]\n",
    "X_test_sparse_newfe = X_train_test_sparse_newfe[train_df.shape[0]:]\n",
    "y_train = y_train[:train_df.shape[0]]\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, alpha=sgd_grid_searcher.best_params_['alpha'], \n",
    "                          class_weight='balanced', random_state=17)\n",
    "logit_test_pred_proba = sgd_logit.fit(X_train_sparse_newfe, y_train).predict_proba(X_test_sparse_newfe)[:, 1]\n",
    "    \n",
    "write_to_submission_file(logit_test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_logit_newfe_alpha_bal.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Против ожиданий, при таких параметрах **auc_roc** на **Kaggle** больше не стал, наоборот - **уменьшился**: **0.92275** (было 0.92600).    \n",
    "Будем подбирать дальше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ПРОМЕЖУТОЧНЫЙ ВЫВОД:**    \n",
    "Пока подбор параметра **alpha** по выборке на 150 пользователях положительного результата не дал.    \n",
    "Возможно, это связано с небольшим отличием правила формирования тренировочной выборки, на которой проводился подбор, от правила для выборки, которая участвует в соревновании (исходные данные одинаковы)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Поработаем еще с признаками.\n",
    "Для этого сформируем (как планировали выше) дополнительную функцию, которая оценивает оптимизируемую метрику уже **не по отложенной выборке**, а с использованием процедуры **кросс-валидации**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CV_score(estimator, X_train_sparse, y, cv=None, random_state=17, test_size=0.3):\n",
    "    '''\n",
    "    обучение и расчет метрики на отложенной выборке\n",
    "    '''\n",
    "    # Разобьем выборку на 2 части. Учтем, что разбиение выборки с train_test_split должно быть стратифицированным\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_sparse, y, \n",
    "                                                          test_size=test_size, \n",
    "                                                          random_state=random_state, stratify=y)\n",
    "    mean_CV_score = cross_val_score(estimator, X_train, y_train, cv=cv, scoring='roc_auc', n_jobs=-1).mean()\n",
    " \n",
    "    return mean_CV_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Далее, теперь будем учитывать обстоятельство, что большинство дополнительных фич у нас по сути КАТЕГОРИАЛЬНЫЕ, и для них нужно делать определенную предобработку**.\n",
    "Мы уже обсуждали это ранее. Как мы отмечали, для корректности категориальные признаки надо **бинаризовать**. Более правильно это сделать с применением техники **'one hot coding'** или  **Tf-idf**-кодирования. Сделаем это ниже, а пока попробуем **упрощенный вариант** - просто поделим пространства значений признаков на 2 части и примем значение признака равно **1**, если он принадлежит к одной из половин, и **0**, если он к ней не принадлежит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В соответствии с этими соображениями модифицируем функцию для подготовки признаков для такой **упрощенной** бинаризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_df_bin(train_df, features=None, train=True):\n",
    "    '''\n",
    "    features = ['#unique_sites_bin', 'session_timespan_bin', 'time_of_day', 'weekend']\n",
    "    '''\n",
    "    \n",
    "    if features == None:\n",
    "        features_ = ['#unique_sites_bin', 'session_timespan_bin', 'time_of_day', 'weekend']\n",
    "    else:\n",
    "        features_ = features\n",
    "    \n",
    "    # готовим сайты\n",
    "    train_df_sites = train_df[['site%d' % i for i in range(1, 11)]] # наны заполним нулями потом\n",
    "    \n",
    "    # подготовим временнЫе метки\n",
    "    train_df[['time%d' % i for i in range(1, 11)]] = train_df[['time%d' % i for i in range(1, 11)]].applymap(pd.to_datetime)\n",
    "    timestamp_df = train_df[['time%d' % i for i in range(1, 11)]].applymap(lambda x: pd.datetime.timestamp(x) \n",
    "                                                                  if type(x) == pd._libs.tslib.Timestamp else np.nan)\n",
    "    # добавляем признаки\n",
    "    if '#unique_sites_bin' in features_:\n",
    "        train_df['#unique_sites_bin'] = (train_df[['site' + str(i) for i in range(1, 11)]].nunique(axis=1, dropna=True)). \\\n",
    "                                         apply(lambda x: 1 if x < 6 else 0)\n",
    "    if 'session_timespan_bin' in features_:\n",
    "        train_df['session_timespan_bin'] = (timestamp_df.max(axis=1, skipna=True) - timestamp_df.min(axis=1, skipna=True)). \\\n",
    "                                            apply(lambda x: 1 if x < 50 else 0)\n",
    "#     if 'start_hour' in features:    \n",
    "#         train_df['start_hour'] = train_df['time1'].apply(lambda x: x.hour)\n",
    "#     if 'day_of_week' in features:    \n",
    "#         train_df['day_of_week'] = train_df['time1'].apply(lambda x: x.weekday())\n",
    "    if 'time_of_day' in features_:    \n",
    "        # до обеда - после обеда\n",
    "        train_df['time_of_day'] = train_df['time1'].apply(lambda x:  1 if x.hour <  14 else 0)\n",
    "    if 'weekend' in features_:\n",
    "        # индикатор уикэнда в день начала сессии\n",
    "        train_df['weekend'] = train_df['time1'].apply(lambda x: 1 if x.weekday() in [5, 6] else 0)\n",
    " \n",
    "    train_df_sites = train_df_sites.fillna(0).astype('int')\n",
    "    train_sparse = csr_matrix((np.ones(train_df_sites.values.size), \n",
    "                                train_df_sites.values.ravel(), \n",
    "                                np.arange(train_df_sites.values.shape[0] + 1) * \n",
    "                                train_df_sites.values.shape[1]), dtype=int)[:, 1:]\n",
    "    if train:\n",
    "        y = train_df['target'].values\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    return train_sparse, train_df[features_], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_sites_sparse, selected_features_df, y_values = feature_engineering_df_bin(train_df, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Без баланса\n",
      "\n",
      "5 лучших метрик с фичами\n",
      "roc_auc_score: 0.934218622552 features: ['weekend']\n",
      "roc_auc_score: 0.933635284594 features: ['time_of_day', 'weekend']\n",
      "roc_auc_score: 0.932279637877 features: []\n",
      "roc_auc_score: 0.930773100919 features: ['time_of_day']\n",
      "roc_auc_score: 0.929718057263 features: ['#unique_sites_bin', 'time_of_day', 'weekend']\n",
      "\n",
      "5 худших метрик с фичами\n",
      "roc_auc_score: 0.923128645975 features: ['session_timespan_bin', 'weekend']\n",
      "roc_auc_score: 0.920592024569 features: ['#unique_sites_bin', 'session_timespan_bin', 'time_of_day']\n",
      "roc_auc_score: 0.920483235092 features: ['session_timespan_bin']\n",
      "roc_auc_score: 0.915035943593 features: ['#unique_sites_bin', 'session_timespan_bin', 'weekend']\n",
      "roc_auc_score: 0.910999149669 features: ['#unique_sites_bin', 'session_timespan_bin']\n",
      "Wall time: 5min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "features = selected_features_df.columns\n",
    "feature_scores = []\n",
    "for r in range(len(features) + 1):\n",
    "    for fe in itertools.combinations(features, r):\n",
    "        fe_ = list(fe)\n",
    "        selected_features_sparse = csr_matrix(selected_features_df[fe_].values)\n",
    "  \n",
    "        X_train_new_sparse = sparse.hstack([train_sites_sparse , selected_features_sparse]).tocsr()\n",
    "        \n",
    "        estimator = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "        roc_auc_score = CV_score(estimator, X_train_new_sparse, y_values, cv=skf, \n",
    "                                 random_state=17, test_size=0.3)\n",
    "        feature_scores.append((roc_auc_score, fe_))\n",
    "\n",
    "sorted_features = sorted(feature_scores, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print('Без баланса')\n",
    "print('\\n5 лучших метрик с фичами')\n",
    "for pair in sorted_features[:5]:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])\n",
    "\n",
    "print('\\n5 худших метрик с фичами')\n",
    "for pair in sorted_features[-5:]:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проделаем то же самое, делая баланс в обучающей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "С балансом\n",
      "\n",
      "5 лучших метрик с фичами\n",
      "roc_auc_score: 0.938555892249 features: ['#unique_sites_bin', 'session_timespan_bin', 'time_of_day', 'weekend']\n",
      "roc_auc_score: 0.936702678996 features: ['#unique_sites_bin', 'session_timespan_bin', 'time_of_day']\n",
      "roc_auc_score: 0.936023752468 features: ['#unique_sites_bin', 'session_timespan_bin', 'weekend']\n",
      "roc_auc_score: 0.935668706936 features: ['session_timespan_bin', 'time_of_day', 'weekend']\n",
      "roc_auc_score: 0.934530145929 features: ['#unique_sites_bin', 'session_timespan_bin']\n",
      "\n",
      "5 худших метрик с фичами\n",
      "roc_auc_score: 0.930224205183 features: ['weekend']\n",
      "roc_auc_score: 0.930118730518 features: ['session_timespan_bin']\n",
      "roc_auc_score: 0.929995398298 features: ['#unique_sites_bin']\n",
      "roc_auc_score: 0.929944599468 features: ['time_of_day']\n",
      "roc_auc_score: 0.926259578693 features: []\n",
      "Wall time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "features = selected_features_df.columns\n",
    "feature_scores = []\n",
    "for r in range(len(features) + 1):\n",
    "    for fe in itertools.combinations(features, r):\n",
    "        fe_ = list(fe)\n",
    "        selected_features_sparse = csr_matrix(selected_features_df[fe_].values)\n",
    "  \n",
    "        X_train_new_sparse = sparse.hstack([train_sites_sparse , selected_features_sparse]).tocsr()\n",
    "        \n",
    "        estimator = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "        roc_auc_score = CV_score(estimator, X_train_new_sparse, y_values, cv=skf, \n",
    "                                 random_state=17, test_size=0.3)\n",
    "        feature_scores.append((roc_auc_score, fe_))\n",
    "\n",
    "sorted_features = sorted(feature_scores, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print('С балансом')\n",
    "print('\\n5 лучших метрик с фичами')\n",
    "for pair in sorted_features[:5]:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])\n",
    "\n",
    "print('\\n5 худших метрик с фичами')\n",
    "for pair in sorted_features[-5:]:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что **с балансом** результат улучшился.    \n",
    "Возьмем комбинацию лучших фич и на ее основе сформируем посылку на **Kaggle**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#unique_sites_bin', 'session_timespan_bin', 'time_of_day', 'weekend']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_features[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = ['#unique_sites_bin', 'session_timespan_bin', 'time_of_day', 'weekend']\n",
    "train_sites_sparse, selected_features_df, y_values\n",
    "X_train_test_sites_sparse, selected_train_test_features_df, y_train_none = \\\n",
    "                                                        feature_engineering_df_bin(train_test_df, features=features, train=True)\n",
    "# делим на трейн и тест\n",
    "selected_train_test_features_sparse = csr_matrix(selected_train_test_features_df[features].values)\n",
    "  \n",
    "X_train_test_newfe_sparse = sparse.hstack([X_train_test_sites_sparse , selected_train_test_features_sparse]).tocsr()\n",
    "X_train_newfe_sparse = X_train_test_newfe_sparse[:train_df.shape[0]]\n",
    "X_test_newfe_sparse = X_train_test_newfe_sparse[train_df.shape[0]:]\n",
    "y_train = y_train_none[:train_df.shape[0]]\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "logit_test_pred_proba = sgd_logit.fit(X_train_newfe_sparse, y_train).predict_proba(X_test_newfe_sparse)[:, 1]\n",
    "    \n",
    "write_to_submission_file(logit_test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_logit_4features_bal.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили на **Kaggle: 0.90482**.    \n",
    "Видим, что такая **упрощенная** предобработка категориальных признаков хорошего результата не дала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подведем промежуточный итог\n",
    "- правильно предобработанные категориальные признаки улучшают результат предсказания\n",
    "- балансировка выборки по целевому признаку результат улучшает\n",
    "- упрощенная предобработка категориальных признаков не дает хорошего результата\n",
    "- добавление некорректно предобработанных категориальных признаков результат **ухудшает**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 5.4. Пробуем БУСТИНГ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рассмотрим возможность применить CatBoostClassifier\n",
    "```python\n",
    "%%time\n",
    "# пробуем CatBoostClassifier\n",
    "import catboost\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "features = ['time_of_day', 'weekend']\n",
    "feature_scores = []\n",
    "for r in range(len(features) + 1):\n",
    "    for fe in itertools.combinations(features, r):\n",
    "        fe_ = list(fe)\n",
    "        selected_features_sparse = csr_matrix(selected_features_df[fe_].values)\n",
    "  \n",
    "        X_train_new_sparse = sparse.hstack([train_sites_sparse , selected_features_sparse]).tocsr()\n",
    "        \n",
    "        estimator = catboost.CatBoostClassifier()\n",
    "        roc_auc_score = CV_score(estimator, X_train_new_sparse.toarray(), y_values, cv=skf, \n",
    "                                 random_state=17, test_size=0.3)\n",
    "        feature_scores.append((roc_auc_score, fe_))\n",
    "\n",
    "sorted_features = sorted(feature_scores, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "for pair in sorted_features:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])\n",
    "    \n",
    "```\n",
    "**РЕЗЮМЕ:**    \n",
    "Catboost **не поддерживае**т работу с **sparse**-форматом, поэтому пока не подходит для этой задачи (а перевод в **np.array** приводит к **ошибке памяти**, скорее всего - памяти для этого просто не хватает)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пробуем XGBOOST, поскольку он умеет работать со sparse-форматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.904199430799 features: ['time_of_day', 'weekend']\n",
      "roc_auc_score: 0.899112212739 features: ['time_of_day']\n",
      "roc_auc_score: 0.890882452164 features: ['weekend']\n",
      "roc_auc_score: 0.884568244521 features: []\n",
      "Wall time: 5min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# пробуем xgboost\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "features = ['time_of_day', 'weekend']\n",
    "feature_scores = []\n",
    "for r in range(len(features) + 1):\n",
    "    for fe in itertools.combinations(features, r):\n",
    "        fe_ = list(fe)\n",
    "        selected_features_sparse = csr_matrix(selected_features_df[fe_].values)\n",
    "  \n",
    "        X_train_new_sparse = sparse.hstack([train_sites_sparse , selected_features_sparse]).tocsr()\n",
    "        \n",
    "        estimator = xgb.XGBClassifier(n_jobs=-1, random_state=17)\n",
    "        roc_auc_score = CV_score(estimator, X_train_new_sparse, y_values, cv=skf, \n",
    "                                 random_state=17, test_size=0.3)\n",
    "        feature_scores.append((roc_auc_score, fe_))\n",
    "\n",
    "sorted_features = sorted(feature_scores, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "for pair in sorted_features:\n",
    "    print('roc_auc_score:', pair[0], 'features:', pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат пока **не очень хороший**.   \n",
    "Модифицируем **feature_engineering_df**, изменив **session_timespan** таким образом, чтобы он принимал только **7** значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_df_mod(train_df, test_df=None, features=None):\n",
    "    '''\n",
    "    features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    '''\n",
    "    if features is None:\n",
    "        features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    \n",
    "    if test_df is None:\n",
    "        df = train_df\n",
    "    else:\n",
    "        df = pd.concat([train_df, test_df])\n",
    "    \n",
    "    # выделим отдельно сайты\n",
    "    df_sites = df[['site%d' % i for i in range(1, 11)]] # не забыть заполнить нули\n",
    "    \n",
    "    # подготовим временнЫе метки\n",
    "    df[['time%d' % i for i in range(1, 11)]] = df[['time%d' % i for i in range(1, 11)]].applymap(pd.to_datetime)\n",
    "    timestamp_df = df[['time%d' % i for i in range(1, 11)]].applymap(lambda x: pd.datetime.timestamp(x) \n",
    "                                                                  if type(x) == pd._libs.tslib.Timestamp else np.nan)\n",
    "    # добавляем признаки\n",
    "    if '#unique_sites' in features:\n",
    "        df['#unique_sites'] = df[['site' + str(i) for i in range(1, 11)]].nunique(axis=1, dropna=True)\n",
    "    if 'session_timespan' in features:\n",
    "        df['session_timespan'] = timestamp_df.max(axis=1, skipna=True) - timestamp_df.min(axis=1, skipna=True)\n",
    "        df['session_timespan'] = df['session_timespan'].apply(lambda x: x // 30 if x < 210 else 7)\n",
    "    if 'start_hour' in features:    \n",
    "        df['start_hour'] = df['time1'].apply(lambda x: x.hour)\n",
    "    if 'day_of_week' in features:    \n",
    "        df['day_of_week'] = df['time1'].apply(lambda x: x.weekday())\n",
    "    if 'time_of_day' in features:    \n",
    "        # время суток (06-12 - утро(1), 12-19 - день(2), 19-24 - вечер(3), 00-06 - ночь(4))\n",
    "        df['time_of_day'] = df['time1'].apply(lambda x:  1 if x.hour >  6 and x.hour <= 12 else \n",
    "                                                                     2 if x.hour > 12 and x.hour <= 19 else\n",
    "                                                                     3 if x.hour > 19 and x.hour <= 24 else \n",
    "                                                                     4)\n",
    "    if 'weekend' in features:\n",
    "        # индикатор уикэнда в день начала сессии\n",
    "        df['weekend'] = df['time1'].apply(lambda x: 1 if x.weekday() in [5, 6] else 0)\n",
    "    \n",
    "    X_new_sparse = csr_matrix(df[features].values)\n",
    "    \n",
    "    df_sites = df_sites.fillna(0).astype('int')\n",
    "    train_sparse = csr_matrix((np.ones(df_sites.values.size), \n",
    "                                df_sites.values.ravel(), \n",
    "                                np.arange(df_sites.values.shape[0] + 1) * \n",
    "                                df_sites.values.shape[1]), dtype=int)[:, 1:]\n",
    "    \n",
    "    \n",
    "    y_train = df['target'].values[:train_df.shape[0]]\n",
    "        \n",
    "    X_sparse = sparse.hstack([train_sparse, X_new_sparse]).tocsr()\n",
    "    X_train_sparse = X_sparse[:train_df.shape[0]]\n",
    "    X_test_sparse = X_sparse[train_df.shape[0]:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (X_train_sparse, y_train) if test_df is None else (X_train_sparse, y_train, X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# готовим данные\n",
    "X_train_new_sparse, y_train = feature_engineering_df_mod(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# инициализируем параметры\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.5,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 170\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98638590813925198"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = xgb.XGBClassifier(n_jobs=-1, random_state=17, **params)\n",
    "roc_auc_score = CV_score(estimator, X_train_new_sparse, y_train, \n",
    "                         random_state=17, test_size=0.3)\n",
    "roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот теперь **roc_auc** получился довольно приличный!    \n",
    "Подготовим и сделаем на этих параметрах посылку на **Kaggle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_sparse, y_train, X_test_sparse = feature_engineering_df_mod(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.5,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 170\n",
    "}\n",
    "\n",
    "\n",
    "estimator = xgb.XGBClassifier(n_jobs=-1, random_state=17, **params)\n",
    "test_pred_proba = estimator.fit(X_train_sparse, y_train).predict_proba(X_test_sparse)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_xgb_allfeatures_1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.90088**     \n",
    "Против ожидания по итогам предварительной метрики, результат **на Kaggle не высокий**.    \n",
    "С другой стороны, это **согласуется** с отмеченным ранее обстоятельством, что для **подобного рода задач** лучше подходят именно **линейные** методы. Так что все в пределах разумного."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточное РЕЗЮМЕ:**   \n",
    "Видим, что **xgboost с логисической регрессией**, несмотря на **высокие результаты на кроссвалидации**, на **Kaggle** хорошего результата **не дает**.    \n",
    "\n",
    "Будем иметь ввиду бустинг в дальнейшем для **блендинга** - смешивания различных алгоритмов. А пока снова вернемся к линейным методам. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 5.5. Предобработка категориальных признаков\n",
    "**Вернемся к SGDClassifier**.    \n",
    "Сделаем предобработку категориальных признаков по алгоритму **One Hot Encoding (OHE)** и применим **SGDClassifier**, с которым в предыдущих разделах получился **достаточно высокий AUC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модифицируем подготовку признаков, добавив **OHE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_df_OHE(train_df, test_df=None, features=None):\n",
    "    '''\n",
    "    features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    '''\n",
    "    if features is None:\n",
    "        features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    \n",
    "    if test_df is None:\n",
    "        df = train_df\n",
    "    else:\n",
    "        df = pd.concat([train_df, test_df])\n",
    "    \n",
    "    # выделим отдельно сайты\n",
    "    df_sites = df[['site%d' % i for i in range(1, 11)]] # не забыть заполнить нули\n",
    "    \n",
    "    # подготовим временнЫе метки\n",
    "    df[['time%d' % i for i in range(1, 11)]] = df[['time%d' % i for i in range(1, 11)]].applymap(pd.to_datetime)\n",
    "    timestamp_df = df[['time%d' % i for i in range(1, 11)]].applymap(lambda x: pd.datetime.timestamp(x) \n",
    "                                                                  if type(x) == pd._libs.tslib.Timestamp else np.nan)\n",
    "    # добавляем признаки\n",
    "    if '#unique_sites' in features:\n",
    "        df['#unique_sites'] = df[['site' + str(i) for i in range(1, 11)]].nunique(axis=1, dropna=True)\n",
    "    if 'session_timespan' in features:\n",
    "        df['session_timespan'] = timestamp_df.max(axis=1, skipna=True) - timestamp_df.min(axis=1, skipna=True)\n",
    "        df['session_timespan'] = df['session_timespan'].apply(lambda x: x // 30 if x < 210 else 7)\n",
    "    if 'start_hour' in features:    \n",
    "        df['start_hour'] = df['time1'].apply(lambda x: x.hour)\n",
    "    if 'day_of_week' in features:    \n",
    "        df['day_of_week'] = df['time1'].apply(lambda x: x.weekday())\n",
    "    if 'time_of_day' in features:    \n",
    "        # время суток (06-12 - утро(1), 12-19 - день(2), 19-24 - вечер(3), 00-06 - ночь(4))\n",
    "        df['time_of_day'] = df['time1'].apply(lambda x:  1 if x.hour >  6 and x.hour <= 12 else \n",
    "                                                                     2 if x.hour > 12 and x.hour <= 19 else\n",
    "                                                                     3 if x.hour > 19 and x.hour <= 24 else \n",
    "                                                                     4)\n",
    "    if 'weekend' in features:\n",
    "        # индикатор уикэнда в день начала сессии\n",
    "        df['weekend'] = df['time1'].apply(lambda x: 1 if x.weekday() in [5, 6] else 0)\n",
    "    \n",
    "    encoder = OneHotEncoder()\n",
    "    #X_new_sparse = csr_matrix(df[features].values)\n",
    "    X_new_sparse = encoder.fit_transform(df[features].values)\n",
    "    \n",
    "    df_sites = df_sites.fillna(0).astype('int')\n",
    "    train_sparse = csr_matrix((np.ones(df_sites.values.size), \n",
    "                                df_sites.values.ravel(), \n",
    "                                np.arange(df_sites.values.shape[0] + 1) * \n",
    "                                df_sites.values.shape[1]), dtype=int)[:, 1:]\n",
    "    \n",
    "    \n",
    "    y_train = df['target'].values[:train_df.shape[0]]\n",
    "        \n",
    "    X_sparse = sparse.hstack([train_sparse, X_new_sparse]).tocsr()\n",
    "    X_train_sparse = X_sparse[:train_df.shape[0]]\n",
    "    X_test_sparse = X_sparse[train_df.shape[0]:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (X_train_sparse, y_train) if test_df is None else (X_train_sparse, y_train, X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_sparse_ohe, y_train_ohe, X_test_sparse_ohe = feature_engineering_df_OHE(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973683288799\n",
      "Wall time: 37.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "auc = CV_score(estimator, X_train_sparse_ohe, y_train_ohe, cv=skf)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат впечатляет, попробуем на **Kaggle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\n",
    "test_pred_proba = estimator.fit(X_train_sparse_ohe, y_train_ohe).predict_proba(X_test_sparse_ohe)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_allfeatures_OHE.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.93742**    \n",
    "Наконец то!    \n",
    ":))   \n",
    " Результат достаточно высокий. Пробуем его еще улучшить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем с **балансировкой** классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.977325731478\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "auc = CV_score(estimator, X_train_sparse_ohe, y_train_ohe, cv=skf)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат улучшился. Сделаем посылку на Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "test_pred_proba = estimator.fit(X_train_sparse_ohe, y_train_ohe).predict_proba(X_test_sparse_ohe)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_allfeatures_OHE_bal.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.94607** - результат **еще более улучшился!**   \n",
    ":)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем подбор **alpha** в SGDClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.00041246263829013564} 0.981989687948\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "\n",
    "sgd_params = {'alpha': np.logspace(-8, 1, 40)}\n",
    "\n",
    "sgd_grid_searcher = GridSearchCV(sgd_logit, param_grid=sgd_params, scoring='roc_auc', n_jobs=-1, cv=skf)\n",
    "sgd_grid_searcher.fit(X_train_sparse_ohe, y_train_ohe)\n",
    "\n",
    "print(sgd_grid_searcher.best_params_, sgd_grid_searcher.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, alpha=sgd_grid_searcher.best_params_['alpha'], \n",
    "                          class_weight='balanced', random_state=17)\n",
    "test_pred_proba = estimator.fit(X_train_sparse_ohe, y_train_ohe).predict_proba(X_test_sparse_ohe)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_allfeatures_OHE_bal_alpha.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.94478 - немного уменьшился**.   \n",
    "\n",
    "Видим, что подбор **alpha** пока результата на **Kaggle** не дал: несмотря на **увеличение метрики на кроссвалидации**, на Leader Board **результат ухудшился**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем для предобработки категориальных признаков алгоритм **TfidfVectorizer()**, который более гибко учитывает частоту появления тех или иных значений признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_df_OHE_Tfidf(train_df, test_df=None, features=None):\n",
    "    '''\n",
    "    features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    '''\n",
    "    if features is None:\n",
    "        features = ['#unique_sites', 'session_timespan', 'start_hour', 'day_of_week', 'time_of_day', 'weekend']\n",
    "    \n",
    "    if test_df is None:\n",
    "        df = train_df\n",
    "    else:\n",
    "        df = pd.concat([train_df, test_df])\n",
    "    \n",
    "    # выделим отдельно сайты\n",
    "    df_sites = df[['site%d' % i for i in range(1, 11)]] # не забыть заполнить нули\n",
    "    \n",
    "    # подготовим временнЫе метки\n",
    "    df[['time%d' % i for i in range(1, 11)]] = df[['time%d' % i for i in range(1, 11)]].applymap(pd.to_datetime)\n",
    "    timestamp_df = df[['time%d' % i for i in range(1, 11)]].applymap(lambda x: pd.datetime.timestamp(x) \n",
    "                                                                  if type(x) == pd._libs.tslib.Timestamp else np.nan)\n",
    "    # добавляем признаки\n",
    "    if '#unique_sites' in features:\n",
    "        df['#unique_sites'] = df[['site' + str(i) for i in range(1, 11)]].nunique(axis=1, dropna=True)\n",
    "    if 'session_timespan' in features:\n",
    "        df['session_timespan'] = timestamp_df.max(axis=1, skipna=True) - timestamp_df.min(axis=1, skipna=True)\n",
    "        df['session_timespan'] = df['session_timespan'].apply(lambda x: x // 30 if x < 210 else 7)\n",
    "#         df['session_timespan'] = df['session_timespan'].apply(lambda x: int(np.log2(x)) if x > 1 else 0)\n",
    "    if 'start_hour' in features:    \n",
    "        df['start_hour'] = df['time1'].apply(lambda x: x.hour)\n",
    "    if 'day_of_week' in features:    \n",
    "        df['day_of_week'] = df['time1'].apply(lambda x: x.weekday())\n",
    "    if 'time_of_day' in features:    \n",
    "        # время суток (06-12 - утро(1), 12-19 - день(2), 19-24 - вечер(3), 00-06 - ночь(4))\n",
    "        df['time_of_day'] = df['time1'].apply(lambda x:  1 if x.hour >  6 and x.hour <= 12 else \n",
    "                                                                     2 if x.hour > 12 and x.hour <= 19 else\n",
    "                                                                     3 if x.hour > 19 and x.hour <= 24 else \n",
    "                                                                     4)\n",
    "    if 'weekend' in features:\n",
    "        # индикатор уикэнда в день начала сессии\n",
    "        df['weekend'] = df['time1'].apply(lambda x: 1 if x.weekday() in [5, 6] else 0)\n",
    "        \n",
    "#     X_new_sparse = csr_matrix(df[features].values)\n",
    "    encoder = OneHotEncoder()\n",
    "    X_new_sparse = encoder.fit_transform(df[features].values)\n",
    "    \n",
    "    df_sites = df_sites.fillna(0).astype(int)\n",
    "#     train_sparse = csr_matrix((np.ones(df_sites.values.size), \n",
    "#                                 df_sites.values.ravel(), \n",
    "#                                 np.arange(df_sites.values.shape[0] + 1) * \n",
    "#                                 df_sites.values.shape[1]), dtype=int)[:, 1:]\n",
    "\n",
    "    # готовим блок с сайтами для кодирования посредством TfidfVectorizer()\n",
    "    df_sites = df_sites.astype(str)\n",
    "    sites_text = df_sites['site1']\n",
    "    for i in range(2, 11):\n",
    "        sites_text += ' ' + df_sites['site{}'.format(i)]\n",
    "    # кодируем подготовленный блок с сайтами\n",
    "    Tfidf_Vectorizer = TfidfVectorizer()    \n",
    "    train_sparse = Tfidf_Vectorizer.fit_transform(sites_text.values)\n",
    "   \n",
    "    X_sparse = sparse.hstack([train_sparse, X_new_sparse]).tocsr()\n",
    "    X_train_sparse = X_sparse[:train_df.shape[0]]\n",
    "    X_test_sparse = X_sparse[train_df.shape[0]:]\n",
    "    \n",
    "    y_train = df['target'].values[:train_df.shape[0]]\n",
    "    \n",
    "    return (X_train_sparse, y_train) if test_df is None else (X_train_sparse, y_train, X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97748485306\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_sparse_ohe_tfidf, y_train_ohe, X_test_sparse_ohe_tfidf = feature_engineering_df_OHE_Tfidf(train_df, test_df)\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "auc = CV_score(estimator, X_train_sparse_ohe_tfidf, y_train_ohe, cv=skf)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особого выигрыша не дало, но тем не менее попробуем на **Kaggle**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "test_pred_proba = estimator.fit(X_train_sparse_ohe_tfidf, y_train_ohe).predict_proba(X_test_sparse_ohe_tfidf)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_sgd_allfeatures_OHE_bal_tfidf.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.94773**   \n",
    "Отлично! Результат и позицию еще хоть и немного, но все же **улучшили!  ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 5.6. Cмешивание алгоритмов\n",
    "Будем делать это по схеме **блендинга**, которая представляет из себя разновидность **стекинга**, описаную в <a href='https://alexanderdyakonov.wordpress.com/2017/03/10/c%D1%82%D0%B5%D0%BA%D0%B8%D0%BD%D0%B3-stacking-%D0%B8-%D0%B1%D0%BB%D0%B5%D0%BD%D0%B4%D0%B8%D0%BD%D0%B3-blending/'>статье</a> на сайте **Александра Дьяконова**.   \n",
    "   \n",
    "А именно:  \n",
    "- делим обучающую выборку на две части\n",
    "- на первой настраиваем два алгоритма и делаем предсказания на второй\n",
    "- получаем два метапризнака\n",
    "- таким же образом получаем два метапризнака на тестовой выборке\n",
    "- далее, на полученных метапризнаках из второй части обучающей выборки настраиваем линейный классификатор и делаем предсказания на тестовых метапризнаках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_sparse_ohe_tfidf, y_train_ohe, X_test_sparse_ohe_tfidf = feature_engineering_df_OHE_Tfidf(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_sparse_ohe_tfidf, y_train_ohe, \n",
    "                                                      test_size=0.5, \n",
    "                                                      random_state=17, stratify=y_train_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 377 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "sgd_pred_proba = sgd_logit.fit(X_train, y_train).predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу попробуем **большую** глубину деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# инициализируем параметры\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 11,\n",
    "    'learning_rate': 0.5,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 300\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(n_jobs=-1, random_state=17, **params)\n",
    "xgb_pred_proba = xgb_classifier.fit(X_train, y_train).predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем новые **метапризнаки** и обучаем на них **SGDClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_metafeature = np.hstack([sgd_pred_proba.reshape((-1, 1)), xgb_pred_proba.reshape((-1, 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_metafeature_test = np.hstack([sgd_logit.fit(X_train, y_train).predict_proba(X_test_sparse_ohe_tfidf)[:, 1].reshape((-1, 1)), \n",
    "                            xgb_classifier.fit(X_train, y_train).predict_proba(X_test_sparse_ohe_tfidf)[:, 1].reshape((-1, 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 901 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "meta_sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "test_pred_proba = meta_sgd_logit.fit(X_metafeature, y_valid).predict_proba(X_metafeature_test)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, '[YDF & MIPT]_Coursera_allfeatures_OHE_bal_tfidf_meta.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.94406**    \n",
    "Результат достаточно высокий, но ниже максимального, полученного ранее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем **другую комбинацию** использования данных на аналогичном **блендинге**.    \n",
    "Для компактности соберем все в **одну ячейку** и организуем **измерение времени** отдельных этапов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку для градиентного бустинга свойственно **переобучение**, попробуем **уменьшить максимальную глубину** деревьев в **xgboost**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for sgd_pred_proba: 0:00:00.343441\n",
      "Time for xgb_pred_proba: 0:00:25.080256\n",
      "Time for X_metafeature_test: 0:00:43.124767\n",
      "Time for write_to_submission_file: 0:00:00.929278\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "sgd_pred_proba = sgd_logit.fit(X_train, y_train).predict_proba(X_valid)[:, 1]\n",
    "\n",
    "print('Time for sgd_pred_proba:', datetime.datetime.now() - start_time) # время этапа\n",
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "# инициализируем параметры\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.5,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 170\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(n_jobs=-1, random_state=17, **params)\n",
    "xgb_pred_proba = xgb_classifier.fit(X_train, y_train).predict_proba(X_valid)[:, 1]\n",
    "print('Time for xgb_pred_proba:', datetime.datetime.now() - start_time) # время этапа\n",
    "\n",
    "X_metafeature = np.hstack([sgd_pred_proba.reshape((-1, 1)), xgb_pred_proba.reshape((-1, 1))])\n",
    "\n",
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "# вычисляем метапризнаки на тестовой выборке\n",
    "X_metafeature_test = np.hstack([sgd_logit.fit(X_train_sparse_ohe_tfidf, \n",
    "                                              y_train_ohe).predict_proba(X_test_sparse_ohe_tfidf)[:, 1].reshape((-1, 1)), \n",
    "                                xgb_classifier.fit(X_train_sparse_ohe_tfidf, \n",
    "                                                   y_train_ohe).predict_proba(X_test_sparse_ohe_tfidf)[:, 1].reshape((-1, 1))])\n",
    "\n",
    "print('Time for X_metafeature_test:', datetime.datetime.now() - start_time) # время этапа\n",
    "start_time = datetime.datetime.now() # фиксируем время для измерения времени этапа\n",
    "\n",
    "meta_sgd_logit = SGDClassifier(loss='log', n_jobs=-1, class_weight='balanced', random_state=17)\n",
    "test_pred_proba = meta_sgd_logit.fit(X_metafeature, y_valid).predict_proba(X_metafeature_test)[:, 1]\n",
    "    \n",
    "write_to_submission_file(test_pred_proba, os.path.join(PATH_TO_DATA, \n",
    "                                                       '[YDF & MIPT]_Coursera_allfeatures_OHE_bal_tfidf_meta2.csv'))\n",
    "print('Time for write_to_submission_file:', datetime.datetime.now() - start_time) # время этапа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle: 0.94739**    \n",
    "Неплохо, как мы и ожидали, результат **вырос** при **уменьшении глубины** деревьев, но все же по-прежнему немного **уступает** самому лучшему результату, полученному выше на **линейном** классификаторе **SGDCclassifier**.   \n",
    "Тем не менее опыт интересный.    \n",
    "Как известно, **xgboost** любит хорошую настройку. Возможно, при более тщательном подборе гиперпараметров в **xgboost**, например, с использованием библиотеки **hyperopt**, мы смогли бы добиться более высокого результата."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**РЕЗЮМЕ:**   \n",
    "Итак, финальный результат этой этой части проекта - **результат в [соревновании на Kaggle](https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/leaderboard): 0.94773**.   \n",
    "На момент его получени это - **63-я** позиция на **Public Leaderboard**, команда в соревновании - **[YDF & MIPT] Dmitry Shereshevskiy**.   \n",
    "\n",
    "По результатам работы видно, что:\n",
    "- корректная **предобработка категориальных признаков** дает существенный прирост результата предсказания, и чем таких признаков **больше** - тем лучше\n",
    "- балансировка классов при их дисбалансе - **важна**\n",
    "- в этой задаче **линейные методы** показали результат лучше, чем деревья\n",
    "- можно **смешивать** разные алгоритмы и это тоже дает хороший результат"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
