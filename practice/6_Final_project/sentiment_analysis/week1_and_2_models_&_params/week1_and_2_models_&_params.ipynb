{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Анализ тональности отзывов на фильмы\n",
    "## <center>Часть 1: Строим простые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные\n",
    "Будем использовать стандартный датасет из **nltk**. Импортируем необходимый модуль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "получим id-шники негативных и позитивных отзывов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "получим список негативных отзывов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negfeats = [movie_reviews.words(fileids=[f]) for f in negids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "получим список позитивных отзывов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posfeats = [movie_reviews.words(fileids=[f]) for f in posids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим список из текстов всех имеющихся отзывов, а также список с классами, которые будет использовать наш классификатор:    \n",
    "    \n",
    "    0 - для негативных отзывов и   \n",
    "    1 - для позитивных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allfeats = negfeats + posfeats # список из текстов всех имеющихся отзывов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_list = [0] * len(negfeats) + [1] * len(posfeats) # список с классами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# функция для записи ответов\n",
    "def write_answer(answer, filename, path='answers'):\n",
    "    name = path + '/' + filename\n",
    "    with open(name, 'w') as file:\n",
    "        file.write(str(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_answer(len(allfeats), 'answer1_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportion_1 = (np.array(class_list) == 1).sum() / len(class_list)\n",
    "proportion_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_answer(proportion_1, 'answer1_2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем **CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CountVectorizer** принимает на вход список из текстов, а не список из списков отдельных токенов, поэтому преобразуем списки токенов в \"как-бы текст\", **соединив их через пробел**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allfeats_join = [' '.join(row) for row in allfeats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x39659 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 666842 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(allfeats_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39659"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_answer(len(vectorizer.vocabulary_), 'answer1_3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем **pipeline** из CountVectorizer и LogisticRegression c настройками по-умолчанию и с помощью cross_val_score (также со стандартными настройками) оценим получаемое \"из коробки\" качество по **accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.836021650393\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = CountVectorizer()\n",
    "clf = LogisticRegression()\n",
    "vectorizer_clf = Pipeline([('vectorizer', vectorizer), ('clf', clf)])\n",
    "acc = cross_val_score(vectorizer_clf, allfeats_join, class_list, scoring='accuracy').mean()\n",
    "print('accuracy =', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_answer(acc, 'answer1_4.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично accuracy, оценим качество по **ROC AUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc = 0.910776493783\n",
      "Wall time: 7.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = CountVectorizer()\n",
    "clf = LogisticRegression()\n",
    "vectorizer_clf = Pipeline([('vectorizer', vectorizer), ('clf', clf)])\n",
    "auc = cross_val_score(vectorizer_clf, allfeats_join, class_list, scoring='roc_auc').mean()\n",
    "print('roc_auc =', auc)\n",
    "write_answer(auc, 'answer1_5.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим **логистическую регрессию** на всей доступной вам выборке и выведим **5 наиболее важных** для модели признака. Будем использовать метод `get_feature_names()` или поле `vocabulary_` у класса `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = vectorizer.fit_transform(allfeats_join)\n",
    "logit = LogisticRegression()\n",
    "logit.fit(features, class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38417, 14159, 39195, 37056,  2954], dtype=int64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_5_ind = abs(logit.coef_[0, :]).argsort()[-5:]\n",
    "important_5_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['waste', 'fun', 'worst', 'unfortunately', 'bad'],\n",
       "      dtype='<U58')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_5_words = np.array(vectorizer.get_feature_names())[important_5_ind]\n",
    "important_5_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# запишем 2 самых важных признака\n",
    "write_answer(' '.join(important_5_words[-2:]), 'answer1_6.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Часть 2: Настройка параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь и далее оценка качества будет выполняться с помощью **cross_val_score с cv=5** и остальными параметрами по умолчанию.\n",
    "\n",
    "Оценим среднее качество ( .mean() ) и стандартное отклонение ( .std() ) по fold'ам для:   \n",
    "\n",
    "    а) pipeline из CountVectorizer() и LogisticRegression(),   \n",
    "    б) pipeline из TfidfVectorizer() и LogisticRegression(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVl_acc_mean = 0.841\n",
      "CVl_acc_std = 0.0167779617356\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CountVect = CountVectorizer()\n",
    "logit = LogisticRegression()\n",
    "CountVect_logit = Pipeline([('CountVect', CountVect), ('logit', logit)])\n",
    "CountVect_logit_acc = cross_val_score(CountVect_logit, allfeats_join, class_list, cv=5)\n",
    "CVl_acc_mean = CountVect_logit_acc.mean()\n",
    "CVl_acc_std = CountVect_logit_acc.std()\n",
    "print('CVl_acc_mean =', CVl_acc_mean)\n",
    "print('CVl_acc_std =', CVl_acc_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVl_acc_mean = 0.821\n",
      "TVl_acc_std = 0.00406201920232\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TfidfVect = TfidfVectorizer()\n",
    "logit = LogisticRegression()\n",
    "TfidfVect_logit = Pipeline([('TfidfVect', TfidfVect), ('logit', logit)])\n",
    "TfidfVect_logit_acc = cross_val_score(TfidfVect_logit, allfeats_join, class_list, cv=5)\n",
    "TVl_acc_mean = TfidfVect_logit_acc.mean()\n",
    "TVl_acc_std = TfidfVect_logit_acc.std()\n",
    "print('TVl_acc_mean =', TVl_acc_mean)\n",
    "print('TVl_acc_std =', TVl_acc_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_answer(' '.join(np.array([CVl_acc_mean, CVl_acc_std, TVl_acc_mean, TVl_acc_std]).astype(str)), 'answer2_1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем разные **min_df** у **CountVectorizer**. \n",
    "\n",
    "Оценим качество классикатора с **min_df=10** и с **min_df=50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df= 10 CVl_acc_mean = 0.839\n",
      "min_df= 50 CVl_acc_mean = 0.813\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CountVect = CountVectorizer()\n",
    "logit = LogisticRegression()\n",
    "CountVect_logit = Pipeline([('CountVect', CountVect), ('logit', logit)])\n",
    "CVl_acc_means = []\n",
    "for n in [10, 50]:\n",
    "    CountVect_logit_acc = cross_val_score(CountVect_logit.set_params(CountVect__min_df=n), allfeats_join, class_list, cv=5)\n",
    "    CVl_acc_mean = CountVect_logit_acc.mean()\n",
    "    CVl_acc_means.append(CVl_acc_mean)\n",
    "    print('min_df=', n, 'CVl_acc_mean =', CVl_acc_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫВОД:**   \n",
    "с увеличением min_df качество **падает**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_answer(' '.join(np.array(CVl_acc_means).astype(str)), 'answer2_2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем разные **классификаторы**.   \n",
    "Сравним результаты для **LogisticRegression, LinearSVC и SGDClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) \n",
      " CV_clf_acc_mean = 0.841\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0) \n",
      " CV_clf_acc_mean = 0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False) \n",
      " CV_clf_acc_mean = 0.784\n",
      "Wall time: 40.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CountVect = CountVectorizer()\n",
    "CV_clf_acc_means = []\n",
    "for estimator in [LogisticRegression, LinearSVC, SGDClassifier]:\n",
    "    clf = estimator()\n",
    "    CountVect_clf = Pipeline([('CountVect', CountVect), ('clf', clf)])\n",
    "    CV_clf_acc_mean = cross_val_score(CountVect_clf, allfeats_join, class_list, cv=5).mean()\n",
    "    \n",
    "    CV_clf_acc_means.append(CV_clf_acc_mean)\n",
    "    print(str(clf),'\\n', 'CV_clf_acc_mean =', CV_clf_acc_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_answer(np.array(CV_clf_acc_means).min().astype(str), 'answer2_3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим список **стоп-слов** с помощью **nltk.corpus.stopwords.words('english')**, посмотрим на его элементы, и передадим его в **соответствующий параметр** CountVectorizer. \n",
    "\n",
    "Оценим качество классификатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words - from nltk CVl_acc_StopWordsNltk = 0.8415\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CountVect = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "logit = LogisticRegression()\n",
    "CountVect_logit = Pipeline([('CountVect', CountVect), ('logit', logit)])\n",
    "CountVect_logit_acc = cross_val_score(CountVect_logit, allfeats_join, class_list, cv=5)\n",
    "CVl_acc_StopWordsNltk = CountVect_logit_acc.mean()\n",
    "print('stop_words - from nltk', 'CVl_acc_StopWordsNltk =', CVl_acc_StopWordsNltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В **sklearn** также предусмотрен **свой список** английских стоп-слов - для этого нужно задать соответствующий параметр равным строке 'english'. \n",
    "\n",
    "Оценим качество классификатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words - from sklearn CVl_acc_StopWordsSklearn = 0.839\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CountVect = CountVectorizer(stop_words='english')\n",
    "logit = LogisticRegression()\n",
    "CountVect_logit = Pipeline([('CountVect', CountVect), ('logit', logit)])\n",
    "CountVect_logit_acc = cross_val_score(CountVect_logit, allfeats_join, class_list, cv=5)\n",
    "CVl_acc_StopWordsSklearn = CountVect_logit_acc.mean()\n",
    "print('stop_words - from sklearn', 'CVl_acc_StopWordsSklearn =', CVl_acc_StopWordsSklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_answer(' '.join(np.array([CVl_acc_StopWordsNltk, CVl_acc_StopWordsSklearn]).astype(str)), 'answer2_4.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В **CountVectorizer** добавим к словам биграммы и измерим качество модели.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVl_acc_analyzerWord = 0.8525\n",
      "Wall time: 50.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CountVect = CountVectorizer(ngram_range=(1, 2))\n",
    "logit = LogisticRegression()\n",
    "CountVect_logit = Pipeline([('CountVect', CountVect), ('logit', logit)])\n",
    "CVl_acc_analyzerWord = cross_val_score(CountVect_logit, allfeats_join, class_list, cv=5).mean()\n",
    "print('CVl_acc_analyzerWord =', CVl_acc_analyzerWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим модель на частотах буквенных n-грамм c n от 3 до 5, указав соответствующее значение параметра **ngram_range** и параметр **analyzer='char_wb'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVl_acc_analyzerChar_wb = 0.82\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CountVect = CountVectorizer(ngram_range=(3, 5), analyzer='char_wb')\n",
    "logit = LogisticRegression()\n",
    "CountVect_logit = Pipeline([('CountVect', CountVect), ('logit', logit)])\n",
    "CVl_acc_analyzerChar_wb = cross_val_score(CountVect_logit, allfeats_join, class_list, cv=5).mean()\n",
    "print('CVl_acc_analyzerChar_wb =', CVl_acc_analyzerChar_wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_answer(' '.join(np.array([CVl_acc_analyzerWord, CVl_acc_analyzerChar_wb]).astype(str)), 'answer2_5.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>ИТОГИ\n",
    "\n",
    "**Подведем итоги по настройке параметров**\n",
    "\n",
    "1. **vectorizer**: CountVectorizer() / TfidfVectorizer(): 0.841 / 0.821 --> **CountVectorizer()**\n",
    "2. **min_df** у CountVectorizer: 1 (default) / 10 / 50: 0.841/0.839/0.813 --> **1 (default)**\n",
    "3. **классификатор**: LogisticRegression / LinearSVC / SGDClassifier: 0.841 / 0.833 / 0.784 --> **LogisticRegression**\n",
    "4. **стоп-слова**: nltk.corpus.stopwords.words('english') / 'english': 0.8415 / 0,839 --> **nltk.corpus.stopwords.words('english')**\n",
    "5. **ngramm**: (1, 2), analyzer='word' (default) / (3, 5), analyzer='char_wb': 0.853 / 0.82 --> **(1, 2), analyzer='word'** (default)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
