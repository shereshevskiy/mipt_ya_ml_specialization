{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения задания был написан \n",
    "а) парсер для сбора данных, похожих на данный тестовой выборки (отзывы на мобильные телефоны),\n",
    "б) подобрана модель, показавшее наилучшее качество на тестовых данных.\n",
    "\n",
    "Для парсинга использовалась библиотека Scrapy и сайт отзывов на товары mail.ru (Яндекс.Маркет упорно сопротивлялся парсингу, в результате удалось набрать только 1700 отзывов, причем с сильным перевесом в пользу положительных). С сайта \n",
    "https://torg.mail.ru было собрано почти 20 000 отзывов, из них около 4 000 негативных. Чтобы классы были сбалансированы, использовалось поровну положительных и отрицательных отзывов, общий размер выборки около 8 000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приводится класс парсера для https://torg.mail.ru. В качестве положительных отзывов брался текст блоков \"Комментарий\" и \"Достоинства\" отзывов, чей рейтинг выше или равен 4.5. В качестве отрицательных - текст блоков \"Комментарий\" и \"Недостатки\" отзывов, чей рейтинг ниже или равен 3 (остальные отзывы пропускались)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReviewsSpider(scrapy.Spider):\n",
    "    name = \"reviews\"\n",
    "    POS = 'positive'\n",
    "    NEG = 'negative'\n",
    "    start_urls = [\n",
    "        'https://torg.mail.ru/review/goods/mobilephones/?page=723',\n",
    "    ]\n",
    "    def __init__(self):\n",
    "        self.counters = {self.POS: 0,\n",
    "                         self.NEG: 0\n",
    "                        };\n",
    "        self.max_reviews = 8000;\n",
    "\n",
    "\n",
    "    def parse(self, response):\n",
    "        logging.info('response:%s\\n', response)\n",
    "\n",
    "        links = response.css('div.card__responses__good_information__name a::attr(\"href\")')\n",
    "        for link in links:\n",
    "            logging.info('Link:%s\\n', link.extract())\n",
    "            yield scrapy.Request(link.extract(), self.unwrap_reviews)\n",
    "\n",
    "        # follow pagination links\n",
    "        pager = response.css('div.pager')\n",
    "        if len(pager) > 0:\n",
    "            fwd = pager.xpath('.//a[@title=\"%s\"]/@href' % u'Следующая страница').extract_first()\n",
    "            logging.info(self.counters)\n",
    "            if fwd is not None and any(value < self.max_reviews for value in self.counters.values()):\n",
    "                logging.info('Next product page:%s\\n', fwd)\n",
    "                yield response.follow(fwd, self.parse)\n",
    "\n",
    "    def unwrap_reviews(self, response):\n",
    "        # unwrap all reviews\n",
    "        unwrap_link = response.xpath('.//div[@class=\"content_grid__center_column__box\"]/div[contains(@class, \"controls-row\")]//a/@href').extract_first()\n",
    "        logging.info('Unwrap Link:%s\\n', unwrap_link)\n",
    "        if unwrap_link is not None:\n",
    "            logging.info('Request parse_page')\n",
    "            yield scrapy.Request(unwrap_link, self.parse_page)\n",
    "        else:\n",
    "            yield self.parse_page(response)\n",
    "\n",
    "    def parse_page(self, response):\n",
    "        logging.info('parse_page %s' % response)\n",
    "        logging.info(self.counters)\n",
    "        reviews = response.css('div.review-item')\n",
    "        n_reviews = len(reviews.extract())\n",
    "        logging.info('Retrieved %i reviews', n_reviews)\n",
    "        if n_reviews ==0:\n",
    "            logging.info(response)\n",
    "\n",
    "        for r in reviews:\n",
    "            id_link = r.xpath('.//div[@class=\"review-item__publication-info\"]/a/@href').extract_first()\n",
    "            id_re = re.search(r'review_id=(\\d+)', id_link)\n",
    "            id = int(id_re.group(1))\n",
    "            rating = float(r.xpath('.//span[@class=\"review-item__rating-counter\"]/text()').extract_first().replace(',', '.'))\n",
    "            logging.info('ID=%i\\n', id)\n",
    "            logging.info('rating=%i\\n', rating)\n",
    "            item = None\n",
    "            if rating >= 4.5:\n",
    "                text = self.get_text(r, self.POS)\n",
    "                if text is None:\n",
    "                    continue\n",
    "\n",
    "                self.counters[self.POS] += 1;\n",
    "                logging.info('positive counter=%i\\n', self.counters[self.POS])\n",
    "                item = {'y': 1,\n",
    "                        'id': id,\n",
    "                        'text': text.replace('\\n', ' ')}\n",
    "                yield item\n",
    "            elif rating <= 3:\n",
    "                text = self.get_text(r, self.NEG)\n",
    "                if text is None:\n",
    "                    continue\n",
    "\n",
    "                self.counters[self.NEG] += 1;\n",
    "                logging.info('negative counter=%i\\n', self.counters[self.NEG])\n",
    "                item = {'y': 0,\n",
    "                        'id': id,\n",
    "                        'text': text.replace('\\n', ' ')}\n",
    "                yield item\n",
    "\n",
    "        # follow pagination links\n",
    "        pager = response.css('div.pager')\n",
    "        if len(pager) > 0:\n",
    "            fwd = pager.xpath('.//a[@title=\"%s\"]/@href' % u'Следующая страница').extract_first()\n",
    "            logging.info(self.counters)\n",
    "            if fwd is not None and any(value < self.max_reviews for value in self.counters.values()):\n",
    "                logging.info('Next review page:%s\\n', fwd)\n",
    "                yield response.follow(fwd, self.parse_page)\n",
    "\n",
    "    def get_text(self, review, sentiment):\n",
    "        comment = review.xpath('.//div[@class=\"review-item__content\"]/p[1]//span/text()').extract_first()\n",
    "        comment_text = comment if comment is not None else ''\n",
    "        if sentiment == self.POS:\n",
    "            pos_comment = review.xpath('.//div[@class=\"review-item__content\"]/div[text()=\"%s\"]/following-sibling::p[1]//span/text()' % u'Достоинства').extract_first()\n",
    "            pos_comment_text = pos_comment if pos_comment is not None else ''\n",
    "            text = comment_text.strip() + \\\n",
    "                   pos_comment_text.strip()\n",
    "        elif sentiment == self.NEG:\n",
    "            neg_comment = review.xpath('.//div[@class=\"review-item__content\"]/div[text()=\"%s\"]/following-sibling::p[1]//span/text()' % u'Недостатки').extract_first()\n",
    "            neg_comment_text = neg_comment if neg_comment is not None else ''\n",
    "            text = comment_text.strip() + \\\n",
    "                   neg_comment_text.strip()\n",
    "        else:\n",
    "            raise Exception('Unknown sentiment %s', sentiment)\n",
    "        if text is None or len(text) < 1:\n",
    "            return None\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На обучающей выборке наилучшее качество показали классификаторы SGDClassifier, LogisticRegression и MLPClassifier. Для векторизации использовался TfidfVectorizer. \n",
    "Также была попытка применить предварительную лемматизацию, замену слов синонимами для уменьшения словаря и добавление маркеров эмоциональной окраски (Positive/Negative).\n",
    "На обучающей выборке это дало более высокое качество (87%), на тестовой - такое же, как и TfidfVectorizer + MLPClassifier без предобрботки (98%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from xml.dom import minidom\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Загрузка данных для обучения\n",
    "def load_reviews(path, sep=',', shuffle=True, balance = True):\n",
    "    df = pd.read_csv(path, sep=sep, encoding ='utf8')\n",
    "    df = df.dropna()\n",
    "    df = df.drop(df[df['text'].map(len) < 10].index)\n",
    "\n",
    "    if balance:\n",
    "        g = df.groupby('y')\n",
    "        df = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n",
    "\n",
    "    df.ix[:]['text'].apply(lambda x: x.replace('\\n', ' '))\n",
    "    X_train = df.ix[:]['text'].values\n",
    "    y_train = df.ix[:]['y'].values\n",
    "    Xy = zip(X_train, y_train)\n",
    "    random.shuffle(Xy, lambda: random.random() if shuffle else 0.42)\n",
    "    return [x[0] for x in Xy], [x[1] for x in Xy]\n",
    "\n",
    "# Загрузка тестовых данных\n",
    "def load_reviews_xml(path):\n",
    "    path = os.path.join(os.path.dirname(os.path.abspath(__file__)), path)\n",
    "    xmldoc = minidom.parse(path)\n",
    "    itemlist = xmldoc.getElementsByTagName('review')\n",
    "    reviews = [item.firstChild.nodeValue.encode('utf-8').decode('utf-8') for item in itemlist]\n",
    "    df = pd.DataFrame(data={'text': reviews, 'Id': range(0, len(reviews))})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_pipeline_results(pipeline, accuracy):\n",
    "    print('{}\\nAccuracy={:f}\\n'.format(pipeline.named_steps.keys(), accuracy))\n",
    "    \n",
    "def write_predictions(fname, df):\n",
    "    df.to_csv(fname, columns=['Id', 'y'], index=False)\n",
    "    \n",
    "def get_pipeline_predictions(pipeline, X_train, y_train, X_test):\n",
    "    labels = LabelEncoder()\n",
    "    y_train = labels.fit_transform(y_train)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def evaluate(clf, X, y, cv=3):\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    return scores.mean()\n",
    "\n",
    "# Подбор параметров по сетке\n",
    "def grid_search(clf, params_grid, X, y):\n",
    "    gs = GridSearchCV(clf, params_grid)\n",
    "    gs.fit(X, y)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = load_reviews('data/reviews_train.csv', balance=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_pipelines_params2(X, y):\n",
    "    pipe = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(min_df=0, max_df=0.9, norm='l2',ngram_range=(1,2), stop_words='english', max_features=None)),\n",
    "        ('clf', SGDClassifier(penalty='l2', loss='hinge', alpha=1e-5, n_iter=50)),\n",
    "    ])\n",
    "    #('Best params: ', {'tfidf__stop_words': 'english', 'clf__loss': 'hinge', 'tfidf__ngram_range': (1, 2), 'tfidf__max_df': 0.75, 'clf__penalty': 'l2',\n",
    "    # 'clf__alpha': 1e-05, 'clf__n_iter': 50}) Best score: 0.867101\n",
    "    params_grid = {#'tfidf__ngram_range': ((1,2), (1,3)),\n",
    "                   #'tfidf__min_df': (0, 1),\n",
    "                   #'tfidf__max_df': (0.75, 0.9, 1),\n",
    "                   #'tfidf__max_features': (1000, 5000, 10000, None),\n",
    "                   #'tfidf__stop_words' : ('english', None),\n",
    "                   #'tfidf__norm': ('l1', 'l2', None),\n",
    "                   'clf__alpha': (1e-4, 1e-5, 1e-6),\n",
    "                   'clf__penalty': ('l1','l2', 'elasticnet'),\n",
    "                   'clf__loss': ('hinge','log', 'perceptron'),\n",
    "                   #'clf__n_iter': (10, 50, 80),\n",
    "                   }\n",
    "    res = grid_search(pipe, params_grid, X, y)\n",
    "    print ('Best params: ', res.best_params_)\n",
    "    print 'Best score: %f' % res.best_score_\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def test_pipelines_params3(X, y):\n",
    "    pipe = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(min_df=0, max_df=0.75, norm='l2',ngram_range=(1,2), stop_words='english')),\n",
    "        ('clf', LogisticRegression(solver='newton-cg', C=150, tol=0.9)),\n",
    "    ])\n",
    "    #'Best params: ', {'tfidf__stop_words': 'english', 'tfidf__min_df': 0, 'tfidf__max_features': None, 'clf__tol': 0.9, \n",
    "    #'clf__C': 150, 'clf__solver': 'newton-cg'}) Best score: 0.868066\n",
    "    params_grid = {#'tfidf__ngram_range': ((1,2), (1,3)),\n",
    "                   #'tfidf__min_df': (0, 2, 10),\n",
    "                   #'tfidf__max_df': (0.75, 0.9, 1),\n",
    "                   #'tfidf__max_features': (5000, 10000, None),\n",
    "                   #'tfidf__stop_words' : ('english', None),\n",
    "                   #'tfidf__norm': ('l1', 'l2', None),\n",
    "                   'clf__C': (100, 150, 180),\n",
    "                   'clf__solver': ('newton-cg', 'liblinear', 'sag'),\n",
    "                   #'clf__penalty': ('l2'),\n",
    "                   'clf__tol': (1, 0.9, 0.75),\n",
    "                   #'clf__max_iter': (10, 50, 80),\n",
    "                   }\n",
    "    res = grid_search(pipe, params_grid, X, y)\n",
    "    print ('Best params: ', res.best_params_)\n",
    "    print 'Best score: %f' % res.best_score_\n",
    "    return pipe\n",
    "\n",
    "    \n",
    "def test_pipelines_params4(X, y):\n",
    "    pipe = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(min_df=0, max_df=0.9, norm='l2',ngram_range=(1,2), stop_words=None, max_features=None)),\n",
    "        ('clf', MLPClassifier(hidden_layer_sizes=(30), solver='sgd', activation='tanh', learning_rate_init=0.1, learning_rate='constant', momentum=0.8)),\n",
    "        #'Best params: ', {'tfidf__max_df': 0.9, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': None, \n",
    "        # 'tfidf__max_features': None, 'tfidf__norm': 'l2', 'clf__solver': 'sgd'\n",
    "        # 'clf__momentum': 0.8, 'clf__learning_rate': 'constant'}) Best score: 0.869513\n",
    "    ])\n",
    "    params_grid = {'tfidf__ngram_range': ((1,2), (1,3)),\n",
    "                   #'tfidf__min_df': (0, 2, 10),\n",
    "                   'tfidf__max_df': (0.75, 0.9, 1),\n",
    "                   'tfidf__max_features': (5000, 10000, None),\n",
    "                   #'tfidf__stop_words' : ('english', None),\n",
    "                   #'tfidf__norm': ('l1', 'l2', None),\n",
    "                   #'clf__activation': ('logistic', 'tanh', 'relu'),\n",
    "                   #'clf__solver': ('lbfgs', 'sgd', 'adam'),\n",
    "                   #'clf__alpha': (0.01, 0.001, 0.0001),\n",
    "                   #'clf__learning_rate': ('constant', 'adaptive'),\n",
    "                   #'clf__learning_rate_init': (0.5, 0.1, 0.01),\n",
    "                   #'clf__max_iter': (100, 200, 500),\n",
    "                   #'clf__momentum': (0.8, 0.9, 1)\n",
    "                   #'clf__hidden_layer_sizes': ((30), (100), (70, 30))\n",
    "                   }\n",
    "    res = grid_search(pipe, params_grid, X, y)\n",
    "    print ('Best params: ', res.best_params_)\n",
    "    print 'Best score: %f' % res.best_score_\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best params: ', {'clf__penalty': 'l2', 'clf__loss': 'log', 'clf__alpha': 1e-06})\n",
      "Best score: 0.867463\n",
      "('Best params: ', {'clf__tol': 0.75, 'clf__C': 100, 'clf__solver': 'newton-cg'})\n",
      "Best score: 0.868066\n"
     ]
    }
   ],
   "source": [
    "test_pipelines_params2(X_train, y_train)\n",
    "test_pipelines_params3(X_train, y_train)\n",
    "pipeline = test_pipelines_params4(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка тестовых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = load_reviews_xml('data/reviews_test.xml')\n",
    "X_test = df_test.ix[:]['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказание:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = get_pipeline_predictions(pipeline, X_train, y_train, X_test)\n",
    "df_test['y'] = ['neg' if y == 0 else 'pos' for y in y_test]\n",
    "write_predictions('data/phone_reviews_sentiment_prediction.csv', df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На тестовой выборке качество модели 2 - 96%, моделей 3 и 4 - 98%.\n",
    "\n",
    "Можно сделать вывод, что основную роль в данной задаче сыграл объем обучающей выборки, подбор классификатора и настройка параметров дали небольшое увеличение качества, обработка данных не дала заметного увеличения качества модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
