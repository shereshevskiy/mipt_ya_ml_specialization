{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам нужно воспользоваться опытом предыдущих недель, чтобы побить бейзлайн в соревновании по сентимент-анализу отзывов на товары на Kaggle Inclass:\n",
    "\n",
    "https://inclass.kaggle.com/c/product-reviews-sentiment-analysis-light "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве ответа в этом задании вам нужно загрузить ноутбук с решением и скриншот вашего результата на leaderboard.\n",
    "\n",
    "Убедитесь, что:\n",
    "\n",
    "1) ход вашего решения задокументирован достаточно подробно для того, чтобы ваши сокурсники поняли, что вы делали и почему,\n",
    "\n",
    "2) ваша команда в соревновании состоит только из вас и названа вашим логином на Сoursera, чтобы ваши сокурсники могли понять, что на скриншоте именно ваш результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import xgboost as xg\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import  LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import  pos_tag ,word_tokenize\n",
    "#from nltk.tag import pos_tag\n",
    "import re as regex\n",
    "import pandas as pd\n",
    "#import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nltk.data.path.append('/home/doniyor/Final_Project/nltk_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for writing to file\n",
    "\n",
    "def AnswerWrite(prediction, filename):\n",
    "  #np.savetxt('np.csv', a, fmt='%.2f', delimiter=',', header=\" #1,  #2,  #3,  #4\")\n",
    "  answer = np.vstack([[i for i in np.arange(0, len(prediction),1)], prediction])\n",
    "  np.savetxt(filename, answer.T, fmt='%.0f', delimiter=',', header=\"Id,y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_train = pd.read_csv(\"/home/doniyor/Final_Project/Part3/products_sentiment_train.tsv\", sep='\\t')\n",
    "data_test = pd.read_csv(\"/home/doniyor/Final_Project/Part3/products_sentiment_test.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 . take around 10,000 640x480 pictures .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the wrt54g plus the hga7t is a perfect solutio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont especially like how music files are uns...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was using the cheapie pail ... and it worked...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Category\n",
       "0          2 . take around 10,000 640x480 pictures .         1\n",
       "1  i downloaded a trial version of computer assoc...         1\n",
       "2  the wrt54g plus the hga7t is a perfect solutio...         1\n",
       "3  i dont especially like how music files are uns...         0\n",
       "4  i was using the cheapie pail ... and it worked...         1"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>'so , why the small digital elph , rather than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3/4 way through the first disk we played on it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>better for the zen micro is outlook compatibil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6 . play gameboy color games on it with goboy .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>'likewise , i ''ve heard norton 2004 professio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               Text\n",
       "0   0  'so , why the small digital elph , rather than...\n",
       "1   1  3/4 way through the first disk we played on it...\n",
       "2   2  better for the zen micro is outlook compatibil...\n",
       "3   3    6 . play gameboy color games on it with goboy .\n",
       "4   4  'likewise , i ''ve heard norton 2004 professio..."
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    " * #### From special symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_symbols(data,column='Text'):\n",
    "    for remove in map(lambda r: regex.compile(regex.escape(r)), [\",\", \" : \", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\",\n",
    "                                                                     \"@\", \"%\", \"^\", \"*\", \"{\", \"}\",\n",
    "                                                                     \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                                                                     \"!\", \"?\", \".\", \"'\",\n",
    "                                                                     \"--\", \"---\", \"#\",\n",
    "                                                                 \"(\", \")\",\n",
    "                                                                 ]):\n",
    "            data.loc[:, \"Text\"].replace(remove, \" \", inplace=True)    \n",
    "    return data\n",
    "                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_by_regex(data ,regexp):\n",
    "        data.loc[:, \"Text\"].replace(regexp, \" \", inplace=True)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_urls(data):\n",
    "     return remove_by_regex(data, regex.compile(r\"http.?://[^\\s]+[\\s]?\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_usernames(data):\n",
    "     return remove_by_regex(data, regex.compile(r\"@[^\\s]+[\\s]?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_numbers(data):\n",
    "     return remove_by_regex(data, regex.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_full(data):\n",
    "    data = clean_symbols(data)\n",
    "    data = remove_urls(data)\n",
    "    data = remove_usernames(data)\n",
    "    data = remove_numbers(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = clean_full(data_train) # cleande train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = clean_full(data_test) # cleaned test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Stemming (Used lemmatization instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stemming our data\n",
    "stemmer = nltk.PorterStemmer()\n",
    "analyzerStem = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(frame):\n",
    "    return (stemmer.stem(w) for w in analyzer(frame))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatiztion\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "analyzerLem = CountVectorizer().build_analyzer()\n",
    "\n",
    "# Function for part of speech detect\n",
    "def pos_prep(word):\n",
    "    pos = pos_tag(word_tokenize(word))[0][1]\n",
    "    if pos.startswith('J'):\n",
    "        return (word,'a')\n",
    "    elif pos.startswith('V'):\n",
    "        return (word,'v')\n",
    "    elif pos.startswith('N'):\n",
    "        return (word,'n')\n",
    "    elif pos.startswith('R'):\n",
    "        return (word,'r')\n",
    "    else:\n",
    "        return (word,'n')\n",
    "    \n",
    "    \n",
    "    \n",
    "#Lemmatize words\n",
    "def lemmatized_words(frame):\n",
    "        arr = []\n",
    "        for w in analyzerLem(frame):\n",
    "            prepare = pos_prep(w)\n",
    "            arr.append(lemmatizer.lemmatize(prepare[0],pos=prepare[1]))\n",
    "            \n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "vectorizerCountLem = CountVectorizer(analyzer=lemmatized_words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple vectorizer and TF-idF vectorizer\n",
    "vectorizerCount = CountVectorizer(analyzer=stemmed_words);\n",
    "vectorizerTFidf = TfidfVectorizer(analyzer=stemmed_words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepare train and test data\n",
    "train_text = data_train[\"Text\"]\n",
    "train_label = data_train[\"Category\"]\n",
    "test_text = data_test[\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Classificator\n",
    "clflog = LogisticRegression();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pipeline with vectorizer and classificator\n",
    "pipe = Pipeline([('vectorizer',vectorizerCountLem),('clf', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GridSearch parameters\n",
    "grid_params =     {\n",
    "     'vectorizer__min_df':[0,1,10,20,30],\n",
    "     'vectorizer__max_df':[0.9,0.80,0.7,1.0],\n",
    "     'vectorizer__analyzer':['word'],\n",
    "     'vectorizer__ngram_range':[(1,2)],\n",
    "     #'vectorizer__stop_words':[nltk.corpus.stopwords.words('english2')],\n",
    "    \n",
    "      'clf__C': [1.0,0.5,0.7,0.9,0.25,0.1],\n",
    "      'clf__penalty': ['l2','l1'],\n",
    "      'clf__class_weight': ['balanced',None]       \n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GridSearchCV\n",
    "grid = GridSearchCV(pipe, cv=3,scoring='accuracy', param_grid=grid_params, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer=<function lemmatized_words at 0x7f83412b3d08>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), prep...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'vectorizer__min_df': [0, 1, 10, 20, 30], 'vectorizer__max_df': [0.9, 0.8, 0.7, 1.0], 'vectorizer__analyzer': ['word'], 'vectorizer__ngram_range': [(1, 2)], 'clf__C': [1.0, 0.5, 0.7, 0.9, 0.25, 0.1], 'clf__penalty': ['l2', 'l1'], 'clf__class_weight': ['balanced', None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit train data to GridSearchCV\n",
    "grid.fit(train_text, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.774"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Score of best estimator\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AnswerWrite(grid.best_estimator_.predict(test_text),\"test10.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### SGD CLassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeSGD = Pipeline([('vectorizer',vectorizerCount),('clf',SGDClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_paramsSGD = {\n",
    "               #'vectorizer': [CountVectorizer(), TfidfVectorizer()],\n",
    "               'vectorizer__min_df':[0,1,10,20,30],\n",
    "               'vectorizer__max_df':[0.9,0.80,0.7,1.0],\n",
    "             #'vectorizer__stop_words':['english',stopwords.words('english')],\n",
    "\n",
    "              #'clf': [LogisticRegreion()],\n",
    "               'clf__loss': [ 'hinge' ],\n",
    "               'clf__penalty': ['l2','l1'],\n",
    "               'clf__alpha': [0.0001,0.001,0.01,0.1,1],  \n",
    "               'clf__learning_rate':['optimal','invscaling'],\n",
    "               'clf__class_weight': ['balanced',None],\n",
    "               'clf__eta0': [0.01]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gridSGD = GridSearchCV(pipeSGD, cv=3,scoring='accuracy', param_grid=grid_paramsSGD, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gridSGD.fit(train_text, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gridSGD.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pipeline for Random Forest\n",
    "pipeRF = Pipeline([('vectorizer',vectorizerCountLem),('clf',RandomForestClassifier())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prameters\n",
    "grid_paramsRF = {\n",
    "               #'vectorizer': [CountVectorizer(), TfidfVectorizer()],\n",
    "               'vectorizer__min_df':[0,1,10,20,30],\n",
    "               'vectorizer__max_df':[0.9,0.80,0.7,1.0],\n",
    "               'vectorizer__ngram_range':[(1,2)],\n",
    "               #'vectorizer__stop_words':['english',stopwords.words('english')],\n",
    "\n",
    "                'clf__n_estimators': [1000],\n",
    "                'clf__max_features': ['sqrt'],\n",
    "                #'clf__min_samples_split': [2,3],\n",
    "                #'clf__min_samples_leaf':[2,3],\n",
    "                'clf__class_weight':['balanced']\n",
    "               };\n",
    "              \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GridDearch\n",
    "gridRF = GridSearchCV(pipeRF, cv=3, scoring='accuracy', param_grid=grid_paramsRF, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer=<function lemmatized_words at 0x7f8310224bf8>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), prep...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'vectorizer__min_df': [0, 1, 10, 20, 30], 'vectorizer__max_df': [0.9, 0.8, 0.7, 1.0], 'vectorizer__ngram_range': [(1, 2)], 'clf__n_estimators': [1000], 'clf__max_features': ['sqrt'], 'clf__class_weight': ['balanced']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train Random Forest\n",
    "gridRF.fit(train_text, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.759"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Score\n",
    "gridRF.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer=<function stemmed_words at 0x7fc78bc6dd90>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=0.8,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 2), preproc...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Estimator\n",
    "gridRF.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AnswerWrite(gridRF.best_estimator_.predict(test_text),\"test8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
